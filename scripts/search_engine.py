#!/usr/bin/env python3
"""
Professor Search Engine (Speed Optimized Edition)
ä½¿ç”¨ BGE-Base æ¨¡å‹ + MPS åŠ é€Ÿï¼Œè§£å†³ M4 ä¸Šè¿è¡Œç¼“æ…¢çš„é—®é¢˜
"""

import os
import sys

# --- å…³é”®é…ç½®ï¼šè®¾ç½®å›½å†…é•œåƒåŠ é€Ÿ ---
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
# ç¦ç”¨ Tokenizers å¹¶è¡Œè­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import json
import csv
from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
from openai import OpenAI

# è·¯å¾„é…ç½®
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
RAW_DIR = DATA_DIR / "raw"
PROCESSED_DIR = DATA_DIR / "processed"
CRAWLED_DIR = PROCESSED_DIR / "crawled_homepages"
# å‘é‡ç¼“å­˜æ–‡ä»¶
VECTOR_CACHE_FILE = PROCESSED_DIR / "professor_vectors.npy"

# Qwen AI é…ç½®
QWEN_API_KEY = "sk-a6356e618255431a941a47afeb99e4b1"
QWEN_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"


class ProfessorSearchEngine:
    """æ•™æˆæœç´¢å¼•æ“ (æé€Ÿç‰ˆ)"""
    
    def __init__(self, enable_ai_summary: bool = True):
        """åˆå§‹åŒ–æœç´¢å¼•æ“"""
        print("ğŸš€ åˆå§‹åŒ–æœç´¢å¼•æ“ (æé€Ÿä¼˜åŒ–ç‰ˆ)...")
        
        # 1. åŠ è½½åŸºç¡€æ•°æ®
        self.countries = self._load_countries()
        self.professors = self._load_professors()
        self.homepage_contents = self._load_homepage_contents()
        
        # 2. åˆå§‹åŒ– Embedding æ¨¡å‹ (æ›´æ¢ä¸º Base æ¨¡å‹)
        self._init_embedding_model()
        
        # 3. æ„å»ºæˆ–åŠ è½½å‘é‡ç´¢å¼•
        self._build_vector_index()
        
        # 4. åˆå§‹åŒ– Qwen AI å®¢æˆ·ç«¯
        self.enable_ai_summary = enable_ai_summary
        if enable_ai_summary:
            try:
                self.qwen_client = OpenAI(
                    api_key=QWEN_API_KEY,
                    base_url=QWEN_BASE_URL
                )
                print(f"âœ… Qwen AI å·²å¯ç”¨")
            except Exception as e:
                print(f"âš ï¸  Qwen AI åˆå§‹åŒ–å¤±è´¥: {e}")
                self.enable_ai_summary = False
                self.qwen_client = None
        else:
            self.qwen_client = None
        
        print(f"âœ… æœç´¢å¼•æ“åˆå§‹åŒ–å®Œæˆï¼")
        print(f"   - å›½å®¶/åœ°åŒº: {len(self.countries)} ä¸ª")
        print(f"   - æ•™æˆæ€»æ•°: {len(self.professors)} ä½")
        print(f"   - æœ‰ä¸»é¡µå†…å®¹: {len(self.homepage_contents)} ä½")
        print()

    def _init_embedding_model(self):
        """åˆå§‹åŒ–æ¨¡å‹å¹¶é…ç½®ç¡¬ä»¶åŠ é€Ÿ"""
        # æ£€æµ‹ç¡¬ä»¶è®¾å¤‡
        if torch.backends.mps.is_available():
            self.device = "mps"
            print("ğŸš€ æ£€æµ‹åˆ° Apple Silicon (M4)ï¼Œå·²å¯ç”¨ MPS GPU åŠ é€Ÿï¼")
        elif torch.cuda.is_available():
            self.device = "cuda"
            print("ğŸš€ æ£€æµ‹åˆ° NVIDIA GPUï¼Œå·²å¯ç”¨ CUDA åŠ é€Ÿï¼")
        else:
            self.device = "cpu"
            print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPU è¿è¡Œ (é€Ÿåº¦è¾ƒæ…¢)")

        # ä½¿ç”¨ Base æ¨¡å‹ï¼Œé€Ÿåº¦æ¯” M3 å¿« 5-10 å€ï¼Œæ•ˆæœä¾ç„¶ä¼˜ç§€
        model_name = 'BAAI/bge-base-zh-v1.5'
        print(f"ğŸ“¥ æ­£åœ¨åŠ è½½è½»é‡çº§æ¨¡å‹ {model_name}...")
        
        try:
            self.embedding_model = SentenceTransformer(
                model_name, 
                device=self.device
            )
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e

    def _load_countries(self) -> Dict:
        """åŠ è½½å›½å®¶å’Œåœ°åŒºä¿¡æ¯"""
        countries_file = RAW_DIR / "countries.csv"
        countries = {}
            
        with open(countries_file, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                try:
                    alpha_2 = row['alpha_2'].lower()
                    countries[alpha_2] = {
                        'name': row['name'],
                        'region': row['region'],
                        'sub_region': row['sub_region'],
                        'alpha_2': alpha_2
                    }
                except KeyError as e:
                    print(f"âš ï¸  è·³è¿‡è¡Œï¼ˆç¼ºå°‘å­—æ®µ {e}ï¼‰: {row}")
                    continue
        return countries
    
    def _load_professors(self) -> List[Dict]:
        """åŠ è½½æ•™æˆåŸºæœ¬ä¿¡æ¯"""
        professors_file = PROCESSED_DIR / "professors.json"
        with open(professors_file, 'r', encoding='utf-8') as f:
            return json.load(f)
        
        return professors
    
    def _load_homepage_contents(self) -> Dict[str, Dict]:
        """åŠ è½½æ•™æˆä¸»é¡µå†…å®¹"""
        contents = {}
        if not CRAWLED_DIR.exists():
            print(f"âš ï¸  è­¦å‘Š: çˆ¬å–ç›®å½•ä¸å­˜åœ¨: {CRAWLED_DIR}")
            return contents
        
        for json_file in CRAWLED_DIR.glob("*.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if data.get('status') == 'success' and data.get('content'):
                        name = data.get('name', json_file.stem.replace('_', ' '))
                        contents[name] = {
                            'content': data['content'],
                            'content_length': data.get('content_length', len(data['content'])),
                            'homepage': data.get('homepage', ''),
                            'method': data.get('method', 'unknown')
                        }
            except Exception:
                print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {json_file.name}: {e}")
                continue
        return contents
    
    def _build_vector_index(self):
        """æ„å»ºè¯­ä¹‰å‘é‡ç´¢å¼•"""
        print("ğŸ“Š æ„å»ºè¯­ä¹‰ç´¢å¼•...")
        
        self.indexed_professors = []
        documents = []
        
        # 1. å‡†å¤‡æ–‡æ¡£æ•°æ®
        for i, prof in enumerate(self.professors):
            name = prof['name']
            if name in self.homepage_contents:
                prof_data = {
                    **prof,
                    'homepage_data': self.homepage_contents[name],
                    'vector_index': len(self.indexed_professors)
                }
                self.indexed_professors.append(prof_data)
                
                # ç»„åˆæ–‡æœ¬ï¼šName + Institution + Content
                # è¿™é‡Œå¯ä»¥é€‚å½“æ”¾å®½åˆ° 1000 å­—ç¬¦ï¼Œå› ä¸º Base æ¨¡å‹å¾ˆå¿«
                content = self.homepage_contents[name]['content']
                doc_text = f"{prof['name']} {prof['institution']} {content[:1000]}"
                documents.append(doc_text)
        
        if not documents:
            print("âš ï¸  æ²¡æœ‰å¯ç´¢å¼•çš„æ–‡æ¡£")
            return

        # 2. å°è¯•åŠ è½½ç¼“å­˜
        if VECTOR_CACHE_FILE.exists():
            print(f"   - å‘ç°ç¼“å­˜æ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½...")
            try:
                self.doc_embeddings = np.load(VECTOR_CACHE_FILE)
                # æ ¡éªŒæ•°é‡æ˜¯å¦ä¸€è‡´
                if len(self.doc_embeddings) == len(documents):
                    # ç®€å•æ ¡éªŒç»´åº¦ (Baseæ¨¡å‹æ˜¯768ç»´ï¼ŒM3æ˜¯1024ç»´)
                    if self.doc_embeddings.shape[1] == 768:
                        print("   âœ… å‘é‡ç¼“å­˜åŠ è½½æˆåŠŸ")
                        return
                    else:
                        print("   âš ï¸ ç¼“å­˜ç»´åº¦ä¸åŒ¹é…ï¼ˆå¯èƒ½æ˜¯æ—§æ¨¡å‹ç•™ä¸‹çš„ï¼‰ï¼Œå‡†å¤‡é‡æ–°è®¡ç®—...")
                else:
                    print(f"   âš ï¸ ç¼“å­˜æ•°é‡ä¸ä¸€è‡´ï¼Œé‡æ–°è®¡ç®—...")
            except Exception as e:
                print(f"   âš ï¸ è¯»å–ç¼“å­˜å¤±è´¥: {e}")
        
        # 3. è®¡ç®—å‘é‡ (å¦‚æœæ²¡æœ‰æœ‰æ•ˆç¼“å­˜)
        print(f"   - æ­£åœ¨è®¡ç®— {len(documents)} ä½æ•™æˆçš„å‘é‡...")
        print("   - ä½¿ç”¨ bge-base æ¨¡å‹ï¼Œé¢„è®¡éœ€è¦ 2-3 åˆ†é’Ÿ...")
        
        # [ä¼˜åŒ–é‡ç‚¹] 
        # Base æ¨¡å‹æ¯”è¾ƒè½»ï¼ŒM4 å¯ä»¥è·‘ Batch Size 32 ç”šè‡³ 64
        # Max Length è®¾ä¸º 512 è¶³å¤Ÿè¦†ç›–ç®€ä»‹
        self.embedding_model.max_seq_length = 512
        
        self.doc_embeddings = self.embedding_model.encode(
            documents,
            batch_size=32,          # M4 è·‘ Base æ¨¡å‹å¯ä»¥ç”¨ 32
            show_progress_bar=True,
            normalize_embeddings=True, 
            device=self.device
        )
        
        # 4. ä¿å­˜ç¼“å­˜
        print(f"   - æ­£åœ¨ä¿å­˜å‘é‡ç¼“å­˜...")
        np.save(VECTOR_CACHE_FILE, self.doc_embeddings)
        print("   âœ… ç´¢å¼•æ„å»ºå®Œæˆ")

    def get_filter_options(self) -> Dict:
        """è·å–è¿‡æ»¤é€‰é¡¹"""
        regions = set()
        sub_regions = set()
        countries_list = []
        
        for alpha_2, info in self.countries.items():
            if info['region']:
                regions.add(info['region'])
            if info['sub_region']:
                sub_regions.add(info['sub_region'])
            countries_list.append({
                'name': info['name'],
                'alpha_2': alpha_2,
                'region': info['region'],
                'sub_region': info['sub_region']
            })
        
        return {
            'regions': sorted(list(regions)),
            'sub_regions': sorted(list(sub_regions)),
            'countries': sorted(countries_list, key=lambda x: x['name'])
        }

    def filter_by_location(self, regions=None, sub_regions=None, countries=None) -> List[Dict]:
        """æŒ‰åœ°åŒº/å›½å®¶è¿‡æ»¤æ•™æˆ"""
        filtered = []
        regions = [r.strip() for r in (regions or [])]
        sub_regions = [sr.strip() for sr in (sub_regions or [])]
        countries = [c.strip().lower() for c in (countries or [])]
        
        if not regions and not sub_regions and not countries:
            return self.indexed_professors
        
        for prof in self.indexed_professors:
            country_code = prof.get('country', '').lower()
            country_info = self.countries.get(country_code, {})
            
            if not country_info:
                # å¦‚æœæ‰¾ä¸åˆ°å›½å®¶ä¿¡æ¯ï¼Œè·³è¿‡è¿™ä¸ªæ•™æˆ
                continue
            
            country_name = country_info.get('name', '').lower()
            country_region = country_info.get('region', '')
            country_sub_region = country_info.get('sub_region', '')
            
            match = True
            if regions and country_info.get('region') not in regions: match = False
            if match and sub_regions and country_info.get('sub_region') not in sub_regions: match = False
            # å¦‚æœæŒ‡å®šäº† countriesï¼Œå¿…é¡»åŒ¹é…
            if countries and match:
                country_match = False
                # æ£€æŸ¥å›½å®¶ä»£ç 
                if country_code in countries:
                    country_match = True
                # æ£€æŸ¥å›½å®¶åç§°ï¼ˆå®Œæ•´åŒ¹é…æˆ–éƒ¨åˆ†åŒ¹é…ï¼‰
                if not country_match:
                    for c in countries:
                        if c in country_name or country_name in c:
                            country_match = True
                            break
                if not country_match:
                    match = False
            
            if match:
                filtered.append(prof)
        
        return filtered

    def search(self, 
               query: str,
               top_k: int = 20,
               regions: List[str] = None,
               sub_regions: List[str] = None,
               countries: List[str] = None,
               normalize_by_length: bool = True,
               generate_summary: bool = True) -> Dict:
        """æ‰§è¡Œè¯­ä¹‰æœç´¢"""
        
        if not query or not query.strip():
            return {'query': query, 'results': [], 'ai_summary': ''}
            
        # 1. è¿‡æ»¤
        filtered_profs = self.filter_by_location(regions, sub_regions, countries)
        if not filtered_profs:
            return {'query': query, 'results': [], 'ai_summary': ''}
            
        target_indices = [p['vector_index'] for p in filtered_profs]
        
        print(f"ğŸ” è¯­ä¹‰æœç´¢: '{query}' (èŒƒå›´: {len(target_indices)} äºº)")
        
        # 2. Query å‘é‡åŒ–
        query_embedding = self.embedding_model.encode([query], normalize_embeddings=True)
        
        # 3. ç›¸ä¼¼åº¦è®¡ç®—
        target_embeddings = self.doc_embeddings[target_indices]
        scores = query_embedding @ target_embeddings.T
        scores = scores[0]
        
        # 4. æ’åº
        top_k_indices = np.argsort(scores)[::-1][:top_k]
        
        results = []
        for rank, idx in enumerate(top_k_indices, 1):
            score = float(scores[idx])
            prof = filtered_profs[idx]
            
            content = prof['homepage_data']['content']
            # ç®€å•å±•ç¤ºå‰ 300 å­—ç¬¦
            snippet = self._extract_snippet(content, query)
            
            results.append({
                'rank': rank,
                'name': prof['name'],
                'institution': prof['institution'],
                'country': prof.get('countryName', prof.get('country', '')),
                'region': prof.get('region', ''),
                'homepage': prof.get('homepage', ''),
                'scholarId': prof.get('scholarId', ''),
                'similarity_score': score,
                'content_length': prof['homepage_data']['content_length'],
                'snippet': snippet,
                'research_areas': prof.get('researchAreas', [])
            })
            
        # 5. AI æ€»ç»“
        ai_summary = ""
        if generate_summary and self.enable_ai_summary:
            ai_summary = self.generate_ai_summary(query, results)
            
        return {
            'query': query,
            'results': results,
            'ai_summary': ai_summary
        }
    def _extract_snippet(self, content: str, query: str, context_length: int = 200) -> str:
        """æå–åŒ…å«æŸ¥è¯¢å…³é”®è¯çš„æ–‡æœ¬ç‰‡æ®µ"""
        # æ¸…ç†æ–‡æœ¬
        content = ' '.join(content.split())
        
        # æŸ¥æ‰¾å…³é”®è¯ä½ç½®
        query_lower = query.lower()
        content_lower = content.lower()
        
        # å°è¯•æ‰¾åˆ°æŸ¥è¯¢è¯çš„ä½ç½®
        pos = content_lower.find(query_lower)
        
        if pos == -1:
            # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´åŒ¹é…ï¼Œå°è¯•æ‰¾ç¬¬ä¸€ä¸ªè¯
            words = query_lower.split()
            for word in words:
                pos = content_lower.find(word)
                if pos != -1:
                    break
        
        if pos == -1:
            # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œè¿”å›å¼€å¤´
            return content[:context_length] + "..."
        
        # æå–ä¸Šä¸‹æ–‡
        start = max(0, pos - context_length // 2)
        end = min(len(content), pos + len(query) + context_length // 2)
        
        snippet = content[start:end]
        
        # æ·»åŠ çœç•¥å·
        if start > 0:
            snippet = "..." + snippet
        if end < len(content):
            snippet = snippet + "..."
        
        return snippet
    def generate_ai_summary(self, query: str, results: List[Dict]) -> str:
        """è°ƒç”¨ Qwen ç”Ÿæˆæ€»ç»“"""
        if not self.enable_ai_summary or not self.qwen_client:
            return ""
        if not results:
            return "æ²¡æœ‰æœç´¢ç»“æœå¯ä»¥æ€»ç»“"

        num_results = min(len(results), 50) # å‡å°‘ç»™ LLM çš„ä¸Šä¸‹æ–‡é‡ï¼Œæé«˜é€Ÿåº¦
        professors_info = []
        for i, result in enumerate(results[:num_results], 1):
            info = f"{i}. {result['name']} ({result['institution']})\n"
            info += f"   ç ”ç©¶å†…å®¹: {result['snippet'][:150]}\n"
            professors_info.append(info)
        
        professors_text = "\n".join(professors_info)
        prompt = f"""ä½ æ˜¯ä¸€ä½å­¦æœ¯ç ”ç©¶åŠ©æ‰‹ã€‚ç”¨æˆ·æœç´¢äº†å…³é”®è¯"{query}"ï¼Œæ‰¾åˆ°äº†ä»¥ä¸‹{num_results}ä½æ•™æˆã€‚

è¯·æ ¹æ®è¿™äº›æ•™æˆçš„ä¿¡æ¯ï¼Œç”Ÿæˆä¸€æ®µç®€æ´çš„æ€»ç»“ï¼ˆ200-300å­—ï¼‰ï¼ŒåŒ…æ‹¬ï¼š
1. è¿™äº›æ•™æˆçš„ä¸»è¦ç ”ç©¶æ–¹å‘å’Œå…±åŒç‚¹
2. ä»–ä»¬æ‰€åœ¨çš„ä¸»è¦æœºæ„å’Œåœ°åŒºåˆ†å¸ƒ
3. ç ”ç©¶é¢†åŸŸçš„ç‰¹ç‚¹å’Œè¶‹åŠ¿
4. å¯¹ç”¨æˆ·å¯»æ‰¾åˆé€‚å¯¼å¸ˆçš„å»ºè®®

æ•™æˆä¿¡æ¯ï¼š
{professors_text}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€ç®€æ´ä¸“ä¸šã€‚"""

        try:
            print("\nğŸ¤– æ­£åœ¨ç”Ÿæˆ AI æ€»ç»“...")
            response = self.qwen_client.chat.completions.create(
                model="qwen-plus",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦æœ¯ç ”ç©¶åŠ©æ‰‹ï¼Œæ“…é•¿åˆ†ææ•™æˆçš„ç ”ç©¶æ–¹å‘å’Œæä¾›å­¦æœ¯å»ºè®®ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"AI æ€»ç»“ç”Ÿæˆå¤±è´¥: {e}")
            return "AI æ€»ç»“ç”Ÿæˆå¤±è´¥ã€‚"

    def print_results(self, search_result: Dict):
        """æ‰“å°ç»“æœ"""
        results = search_result.get('results', [])
        ai_summary = search_result.get('ai_summary', '')
        if not results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç»“æœ")
            return
            
        # æ˜¾ç¤º AI æ€»ç»“
        if show_ai_summary and ai_summary:
            print(f"\n{'='*80}")
            print(f"ğŸ¤– AI æ€»ç»“")
            print(f"{'='*80}")
            print(ai_summary)
            print()
        # æ˜¾ç¤ºæœç´¢ç»“æœ
        print(f"\n{'='*80}")
        print(f"æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        print(f"{'='*80}\n")
        
        for res in results[:5]:
            print(f"#{res['rank']} {res['name']} ({res['similarity_score']:.4f})")
            print(f"   ğŸ« {res['institution']} ({res['country']})")
            #print(f"   ğŸ“Š ç›¸ä¼¼åº¦: {res['similarity_score']:.4f}")
            print(f"   ğŸ“ å†…å®¹é•¿åº¦: {res['content_length']} å­—ç¬¦")
            if res['homepage']:
                print(f"   ğŸ”— ä¸»é¡µ: {res['homepage']}")
            
            if res['research_areas']:
                print(f"   ğŸ”¬ ç ”ç©¶é¢†åŸŸ: {', '.join(res['research_areas'][:5])}")
            
            if show_snippet and res['snippet']:
                print(f"   ğŸ“ ç‰‡æ®µ: {res['snippet'][:300]}")
            
            print()

def main():
    """æµ‹è¯•å…¥å£"""
    engine = ProfessorSearchEngine()
    print("\nğŸ” æµ‹è¯•: æœç´¢ 'ä¿¡æ¯æ£€ç´¢'...")
    # ç¤ºä¾‹1: æ— è¿‡æ»¤çš„æœç´¢ï¼ˆå¸¦ AI æ€»ç»“ï¼‰
    print("\n" + "="*80)
    print("ç¤ºä¾‹ 1: æœç´¢ 'reinforcement learning, sequential decision making' (æ— è¿‡æ»¤, top 20)")
    print("="*80)
    res = engine.search("reinforcement learning, sequential decision making", top_k=20)
    engine.print_results(res)

    # ç¤ºä¾‹2: æŒ‰åœ°åŒºè¿‡æ»¤ï¼ˆå¸¦ AI æ€»ç»“ï¼‰
    print("\n" + "="*80)
    print("ç¤ºä¾‹ 2: æœç´¢ 'machine learning' (ä»…äºšæ´², top 20)")
    print("="*80)
    res = engine.search("machine learning", top_k=20, regions=["Asia"])
    engine.print_results(res)

    # ç¤ºä¾‹3: æŒ‰å›½å®¶è¿‡æ»¤ï¼ˆå¸¦ AI æ€»ç»“ï¼‰
    print("\n" + "="*80)
    print("ç¤ºä¾‹ 3: æœç´¢ 'recommender system and large language models' (ä»…ç¾å›½, top 20)")
    print("="*80)
    res = engine.search(
        "recommender system and large language models", top_k=20, countries=["United States", "us"]
    )
    engine.print_results(res)

if __name__ == "__main__":
    main()