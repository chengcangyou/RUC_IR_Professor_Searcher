#!/usr/bin/env python3
"""
Professor Search Engine
æ”¯æŒåŒºåŸŸ/å›½å®¶è¿‡æ»¤å’ŒåŸºäºå†…å®¹ç›¸ä¼¼åº¦çš„æœç´¢
"""

import json
import csv
import os
from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
from openai import OpenAI

# è·¯å¾„é…ç½®
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
RAW_DIR = DATA_DIR / "raw"
PROCESSED_DIR = DATA_DIR / "processed"
CRAWLED_DIR = PROCESSED_DIR / "crawled_homepages"

# Qwen AI é…ç½®
QWEN_API_KEY = "sk-a6356e618255431a941a47afeb99e4b1"
QWEN_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"


class ProfessorSearchEngine:
    """æ•™æˆæœç´¢å¼•æ“"""
    
    def __init__(self, enable_ai_summary: bool = True):
        """åˆå§‹åŒ–æœç´¢å¼•æ“"""
        print("ğŸš€ åˆå§‹åŒ–æœç´¢å¼•æ“...")
        
        # åŠ è½½æ•°æ®
        self.countries = self._load_countries()
        self.professors = self._load_professors()
        self.homepage_contents = self._load_homepage_contents()
        
        # æ„å»ºç´¢å¼•
        self._build_search_index()
        
        # åˆå§‹åŒ– Qwen AI å®¢æˆ·ç«¯
        self.enable_ai_summary = enable_ai_summary
        if enable_ai_summary:
            try:
                self.qwen_client = OpenAI(
                    api_key=QWEN_API_KEY,
                    base_url=QWEN_BASE_URL
                )
                print(f"âœ… Qwen AI å·²å¯ç”¨")
            except Exception as e:
                print(f"âš ï¸  Qwen AI åˆå§‹åŒ–å¤±è´¥: {e}")
                self.enable_ai_summary = False
                self.qwen_client = None
        else:
            self.qwen_client = None
        
        print(f"âœ… æœç´¢å¼•æ“åˆå§‹åŒ–å®Œæˆï¼")
        print(f"   - å›½å®¶/åœ°åŒº: {len(self.countries)} ä¸ª")
        print(f"   - æ•™æˆæ€»æ•°: {len(self.professors)} ä½")
        print(f"   - æœ‰ä¸»é¡µå†…å®¹: {len(self.homepage_contents)} ä½")
        print()
    
    def _load_countries(self) -> Dict:
        """åŠ è½½å›½å®¶å’Œåœ°åŒºä¿¡æ¯"""
        countries_file = RAW_DIR / "countries.csv"
        countries = {}
        
        with open(countries_file, 'r', encoding='utf-8-sig') as f:  # å¤„ç†BOM
            reader = csv.DictReader(f)
            for row in reader:
                try:
                    alpha_2 = row['alpha_2'].lower()
                    countries[alpha_2] = {
                        'name': row['name'],
                        'region': row['region'],
                        'sub_region': row['sub_region'],
                        'alpha_2': alpha_2
                    }
                except KeyError as e:
                    print(f"âš ï¸  è·³è¿‡è¡Œï¼ˆç¼ºå°‘å­—æ®µ {e}ï¼‰: {row}")
                    continue
        
        return countries
    
    def _load_professors(self) -> List[Dict]:
        """åŠ è½½æ•™æˆåŸºæœ¬ä¿¡æ¯"""
        professors_file = PROCESSED_DIR / "professors.json"
        
        with open(professors_file, 'r', encoding='utf-8') as f:
            professors = json.load(f)
        
        return professors
    
    def _load_homepage_contents(self) -> Dict[str, Dict]:
        """åŠ è½½æ•™æˆä¸»é¡µå†…å®¹"""
        contents = {}
        
        if not CRAWLED_DIR.exists():
            print(f"âš ï¸  è­¦å‘Š: çˆ¬å–ç›®å½•ä¸å­˜åœ¨: {CRAWLED_DIR}")
            return contents
        
        # éå†æ‰€æœ‰JSONæ–‡ä»¶
        for json_file in CRAWLED_DIR.glob("*.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    # åªä¿ç•™æˆåŠŸçˆ¬å–çš„å†…å®¹
                    if data.get('status') == 'success' and data.get('content'):
                        name = data.get('name', json_file.stem.replace('_', ' '))
                        contents[name] = {
                            'content': data['content'],
                            'content_length': data.get('content_length', len(data['content'])),
                            'homepage': data.get('homepage', ''),
                            'method': data.get('method', 'unknown')
                        }
            except Exception as e:
                print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {json_file.name}: {e}")
                continue
        
        return contents
    
    def _build_search_index(self):
        """æ„å»ºæœç´¢ç´¢å¼•ï¼ˆTF-IDFï¼‰"""
        print("ğŸ“Š æ„å»ºæœç´¢ç´¢å¼•...")
        
        # å‡†å¤‡æ–‡æ¡£åˆ—è¡¨
        self.indexed_professors = []
        documents = []
        
        for prof in self.professors:
            name = prof['name']
            if name in self.homepage_contents:
                self.indexed_professors.append({
                    **prof,
                    'homepage_data': self.homepage_contents[name]
                })
                documents.append(self.homepage_contents[name]['content'])
        
        print(f"   - å¯æœç´¢çš„æ•™æˆ: {len(self.indexed_professors)} ä½")
        
        # æ„å»ºTF-IDFå‘é‡
        if documents:
            self.vectorizer = TfidfVectorizer(
                max_features=5000,  # å‡å°‘ç‰¹å¾æ•°
                stop_words='english',
                ngram_range=(1, 1),  # åªä½¿ç”¨1-gramï¼Œå‡å°‘å†…å­˜
                min_df=3,  # å¢åŠ æœ€å°æ–‡æ¡£é¢‘ç‡
                max_df=0.7  # é™ä½æœ€å¤§æ–‡æ¡£é¢‘ç‡
            )
            self.tfidf_matrix = self.vectorizer.fit_transform(documents)
            print(f"   - TF-IDFçŸ©é˜µ: {self.tfidf_matrix.shape}")
            print(f"   - çŸ©é˜µå¤§å°: ~{self.tfidf_matrix.data.nbytes / 1024 / 1024:.2f} MB")
        else:
            self.vectorizer = None
            self.tfidf_matrix = None
            print("   âš ï¸  æ²¡æœ‰æ–‡æ¡£å¯ä»¥ç´¢å¼•")
    
    def get_filter_options(self) -> Dict:
        """è·å–è¿‡æ»¤é€‰é¡¹"""
        regions = set()
        sub_regions = set()
        countries_list = []
        
        for alpha_2, info in self.countries.items():
            if info['region']:
                regions.add(info['region'])
            if info['sub_region']:
                sub_regions.add(info['sub_region'])
            countries_list.append({
                'name': info['name'],
                'alpha_2': alpha_2,
                'region': info['region'],
                'sub_region': info['sub_region']
            })
        
        return {
            'regions': sorted(list(regions)),
            'sub_regions': sorted(list(sub_regions)),
            'countries': sorted(countries_list, key=lambda x: x['name'])
        }
    
    def filter_by_location(self, 
                          regions: List[str] = None,
                          sub_regions: List[str] = None,
                          countries: List[str] = None) -> List[Dict]:
        """
        æŒ‰åœ°åŒº/å›½å®¶è¿‡æ»¤æ•™æˆ
        
        Args:
            regions: åœ°åŒºåˆ—è¡¨ï¼ˆå¦‚ ["Asia", "Europe"]ï¼‰
            sub_regions: å­åœ°åŒºåˆ—è¡¨ï¼ˆå¦‚ ["Southern Asia", "Western Europe"]ï¼‰
            countries: å›½å®¶åˆ—è¡¨ï¼ˆå¦‚ ["United States", "China", "us", "cn"]ï¼‰
        
        Returns:
            è¿‡æ»¤åçš„æ•™æˆåˆ—è¡¨
        """
        filtered = []
        
        # æ ‡å‡†åŒ–è¾“å…¥
        regions = [r.strip() for r in (regions or [])]
        sub_regions = [sr.strip() for sr in (sub_regions or [])]
        countries = [c.strip().lower() for c in (countries or [])]
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•è¿‡æ»¤æ¡ä»¶ï¼Œè¿”å›æ‰€æœ‰æœ‰ä¸»é¡µå†…å®¹çš„æ•™æˆ
        if not regions and not sub_regions and not countries:
            return self.indexed_professors
        
        for prof in self.indexed_professors:
            country_code = prof.get('country', '').lower()
            
            # è·å–å›½å®¶ä¿¡æ¯ï¼ˆä» countries.csvï¼Œä¸ä½¿ç”¨ professors.json ä¸­çš„ regionï¼‰
            country_info = self.countries.get(country_code, {})
            if not country_info:
                # å¦‚æœæ‰¾ä¸åˆ°å›½å®¶ä¿¡æ¯ï¼Œè·³è¿‡è¿™ä¸ªæ•™æˆ
                continue
            
            country_name = country_info.get('name', '').lower()
            country_region = country_info.get('region', '')
            country_sub_region = country_info.get('sub_region', '')
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ï¼ˆä½¿ç”¨ AND é€»è¾‘ï¼šæ‰€æœ‰æŒ‡å®šçš„æ¡ä»¶éƒ½å¿…é¡»æ»¡è¶³ï¼‰
            match = True
            
            # å¦‚æœæŒ‡å®šäº† regionsï¼Œå¿…é¡»åŒ¹é…
            if regions:
                if country_region not in regions:
                    match = False
            
            # å¦‚æœæŒ‡å®šäº† sub_regionsï¼Œå¿…é¡»åŒ¹é…
            if sub_regions and match:
                if country_sub_region not in sub_regions:
                    match = False
            
            # å¦‚æœæŒ‡å®šäº† countriesï¼Œå¿…é¡»åŒ¹é…
            if countries and match:
                country_match = False
                # æ£€æŸ¥å›½å®¶ä»£ç 
                if country_code in countries:
                    country_match = True
                # æ£€æŸ¥å›½å®¶åç§°ï¼ˆå®Œæ•´åŒ¹é…æˆ–éƒ¨åˆ†åŒ¹é…ï¼‰
                if not country_match:
                    for c in countries:
                        if c in country_name or country_name in c:
                            country_match = True
                            break
                if not country_match:
                    match = False
            
            if match:
                filtered.append(prof)
        
        return filtered
    
    def search(self, 
               query: str,
               top_k: int = 20,
               regions: List[str] = None,
               sub_regions: List[str] = None,
               countries: List[str] = None,
               normalize_by_length: bool = True,
               generate_summary: bool = True) -> Dict:
        """
        æœç´¢æ•™æˆ
        
        Args:
            query: æœç´¢å…³é”®è¯
            top_k: è¿”å›å‰Kä¸ªç»“æœï¼ˆé»˜è®¤20ï¼‰
            regions: åœ°åŒºè¿‡æ»¤
            sub_regions: å­åœ°åŒºè¿‡æ»¤
            countries: å›½å®¶è¿‡æ»¤
            normalize_by_length: æ˜¯å¦æŒ‰å†…å®¹é•¿åº¦å½’ä¸€åŒ–
            generate_summary: æ˜¯å¦ç”Ÿæˆ AI æ€»ç»“
        
        Returns:
            åŒ…å«æœç´¢ç»“æœå’Œ AI æ€»ç»“çš„å­—å…¸
            {
                'query': str,
                'results': List[Dict],
                'ai_summary': str
            }
        """
        if not query or not query.strip():
            print("âš ï¸  æœç´¢å…³é”®è¯ä¸ºç©º")
            return {'query': query, 'results': [], 'ai_summary': ''}
        
        if self.vectorizer is None or self.tfidf_matrix is None:
            print("âš ï¸  æœç´¢ç´¢å¼•æœªæ„å»º")
            return {'query': query, 'results': [], 'ai_summary': ''}
        
        # å…ˆæŒ‰åœ°åŒº/å›½å®¶è¿‡æ»¤
        filtered_profs = self.filter_by_location(regions, sub_regions, countries)
        
        if not filtered_profs:
            print("âš ï¸  æ²¡æœ‰ç¬¦åˆè¿‡æ»¤æ¡ä»¶çš„æ•™æˆ")
            return {'query': query, 'results': [], 'ai_summary': ''}
        
        print(f"ğŸ” æœç´¢: '{query}'")
        print(f"   - è¿‡æ»¤åçš„æ•™æˆæ•°: {len(filtered_profs)} ä½")
        
        # è·å–è¿‡æ»¤åæ•™æˆçš„ç´¢å¼•
        filtered_indices = []
        for prof in filtered_profs:
            try:
                idx = self.indexed_professors.index(prof)
                filtered_indices.append(idx)
            except ValueError:
                continue
        
        if not filtered_indices:
            print("âš ï¸  æ²¡æœ‰å¯æœç´¢çš„æ•™æˆ")
            return {'query': query, 'results': [], 'ai_summary': ''}
        
        # å°†æŸ¥è¯¢è½¬æ¢ä¸ºTF-IDFå‘é‡
        query_vector = self.vectorizer.transform([query])
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vector, self.tfidf_matrix[filtered_indices]).flatten()
        
        # å¦‚æœéœ€è¦æŒ‰é•¿åº¦å½’ä¸€åŒ–
        if normalize_by_length:
            for i, idx in enumerate(filtered_indices):
                prof = self.indexed_professors[idx]
                content_length = prof['homepage_data']['content_length']
                
                # å½’ä¸€åŒ–å› å­ï¼šä½¿ç”¨å¯¹æ•°ç¼©æ”¾ï¼Œé¿å…è¿‡åº¦æƒ©ç½šé•¿æ–‡æ¡£
                length_factor = np.log(1 + content_length) / np.log(1 + 10000)  # å‡è®¾10000æ˜¯å¹³å‡é•¿åº¦
                length_factor = min(length_factor, 1.5)  # é™åˆ¶æœ€å¤§å½±å“
                
                similarities[i] = similarities[i] / length_factor
        
        # è·å–top-kç»“æœ
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for rank, i in enumerate(top_indices, 1):
            idx = filtered_indices[i]
            prof = self.indexed_professors[idx]
            
            # è·å–åŒ¹é…çš„å…³é”®è¯ç‰‡æ®µ
            content = prof['homepage_data']['content']
            snippet = self._extract_snippet(content, query)
            
            results.append({
                'rank': rank,
                'name': prof['name'],
                'institution': prof['institution'],
                'country': prof.get('countryName', prof.get('country', '')),
                'region': prof.get('region', ''),
                'homepage': prof.get('homepage', ''),
                'scholarId': prof.get('scholarId', ''),
                'similarity_score': float(similarities[i]),
                'content_length': prof['homepage_data']['content_length'],
                'snippet': snippet,
                'research_areas': prof.get('researchAreas', [])
            })
        
        # ç”Ÿæˆ AI æ€»ç»“
        ai_summary = ""
        if generate_summary and self.enable_ai_summary:
            ai_summary = self.generate_ai_summary(query, results)
        
        return {
            'query': query,
            'results': results,
            'ai_summary': ai_summary
        }
    
    def _extract_snippet(self, content: str, query: str, context_length: int = 200) -> str:
        """æå–åŒ…å«æŸ¥è¯¢å…³é”®è¯çš„æ–‡æœ¬ç‰‡æ®µ"""
        # æ¸…ç†æ–‡æœ¬
        content = ' '.join(content.split())
        
        # æŸ¥æ‰¾å…³é”®è¯ä½ç½®
        query_lower = query.lower()
        content_lower = content.lower()
        
        # å°è¯•æ‰¾åˆ°æŸ¥è¯¢è¯çš„ä½ç½®
        pos = content_lower.find(query_lower)
        
        if pos == -1:
            # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´åŒ¹é…ï¼Œå°è¯•æ‰¾ç¬¬ä¸€ä¸ªè¯
            words = query_lower.split()
            for word in words:
                pos = content_lower.find(word)
                if pos != -1:
                    break
        
        if pos == -1:
            # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œè¿”å›å¼€å¤´
            return content[:context_length] + "..."
        
        # æå–ä¸Šä¸‹æ–‡
        start = max(0, pos - context_length // 2)
        end = min(len(content), pos + len(query) + context_length // 2)
        
        snippet = content[start:end]
        
        # æ·»åŠ çœç•¥å·
        if start > 0:
            snippet = "..." + snippet
        if end < len(content):
            snippet = snippet + "..."
        
        return snippet
    
    def generate_ai_summary(self, query: str, results: List[Dict]) -> str:
        """
        ä½¿ç”¨ Qwen AI ç”Ÿæˆæœç´¢ç»“æœçš„æ€»ç»“
        
        Args:
            query: æœç´¢å…³é”®è¯
            results: æœç´¢ç»“æœåˆ—è¡¨
        
        Returns:
            AI ç”Ÿæˆçš„æ€»ç»“æ–‡æœ¬
        """
        if not self.enable_ai_summary or not self.qwen_client:
            return "AI æ€»ç»“åŠŸèƒ½æœªå¯ç”¨"
        
        if not results:
            return "æ²¡æœ‰æœç´¢ç»“æœå¯ä»¥æ€»ç»“"
        
        # ä½¿ç”¨æ‰€æœ‰ç»“æœï¼ˆæœ€å¤š50ä¸ªï¼‰è¿›è¡Œæ€»ç»“
        num_results = min(len(results), 50)
        
        # å‡†å¤‡æç¤ºè¯
        professors_info = []
        for i, result in enumerate(results[:num_results], 1):
            info = f"{i}. {result['name']} - {result['institution']} ({result['country']})\n"
            info += f"   ç›¸ä¼¼åº¦: {result['similarity_score']:.4f}\n"
            if result['snippet']:
                info += f"   ç ”ç©¶å†…å®¹: {result['snippet'][:500]}\n"
            professors_info.append(info)
        
        professors_text = "\n".join(professors_info)
        
        prompt = f"""ä½ æ˜¯ä¸€ä½å­¦æœ¯ç ”ç©¶åŠ©æ‰‹ã€‚ç”¨æˆ·æœç´¢äº†å…³é”®è¯"{query}"ï¼Œæ‰¾åˆ°äº†ä»¥ä¸‹{num_results}ä½æ•™æˆã€‚

è¯·æ ¹æ®è¿™äº›æ•™æˆçš„ä¿¡æ¯ï¼Œç”Ÿæˆä¸€æ®µç®€æ´çš„æ€»ç»“ï¼ˆ200-300å­—ï¼‰ï¼ŒåŒ…æ‹¬ï¼š
1. è¿™äº›æ•™æˆçš„ä¸»è¦ç ”ç©¶æ–¹å‘å’Œå…±åŒç‚¹
2. ä»–ä»¬æ‰€åœ¨çš„ä¸»è¦æœºæ„å’Œåœ°åŒºåˆ†å¸ƒ
3. ç ”ç©¶é¢†åŸŸçš„ç‰¹ç‚¹å’Œè¶‹åŠ¿
4. å¯¹ç”¨æˆ·å¯»æ‰¾åˆé€‚å¯¼å¸ˆçš„å»ºè®®

æ•™æˆä¿¡æ¯ï¼š
{professors_text}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€ç®€æ´ä¸“ä¸šã€‚"""
        
        try:
            print("\nğŸ¤– æ­£åœ¨ç”Ÿæˆ AI æ€»ç»“...")
            response = self.qwen_client.chat.completions.create(
                model="qwen-plus",  # ä½¿ç”¨ qwen-plus æ¨¡å‹
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦æœ¯ç ”ç©¶åŠ©æ‰‹ï¼Œæ“…é•¿åˆ†ææ•™æˆçš„ç ”ç©¶æ–¹å‘å’Œæä¾›å­¦æœ¯å»ºè®®ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            
            summary = response.choices[0].message.content
            return summary
            
        except Exception as e:
            return f"AI æ€»ç»“ç”Ÿæˆå¤±è´¥: {e}"
    
    def print_results(self, search_result: Dict, show_snippet: bool = True, show_ai_summary: bool = True):
        """
        æ‰“å°æœç´¢ç»“æœ
        
        Args:
            search_result: æœç´¢ç»“æœå­—å…¸ï¼ŒåŒ…å« 'query', 'results', 'ai_summary'
            show_snippet: æ˜¯å¦æ˜¾ç¤ºæ–‡æœ¬ç‰‡æ®µ
            show_ai_summary: æ˜¯å¦æ˜¾ç¤º AI æ€»ç»“
        """
        results = search_result.get('results', [])
        ai_summary = search_result.get('ai_summary', '')
        
        if not results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ç»“æœ")
            return
        
        # æ˜¾ç¤º AI æ€»ç»“
        if show_ai_summary and ai_summary:
            print(f"\n{'='*80}")
            print(f"ğŸ¤– AI æ€»ç»“")
            print(f"{'='*80}")
            print(ai_summary)
            print()
        
        # æ˜¾ç¤ºæœç´¢ç»“æœ
        print(f"\n{'='*80}")
        print(f"æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        print(f"{'='*80}\n")
        
        for result in results:
            print(f"ğŸ† #{result['rank']} - {result['name']}")
            print(f"   ğŸ« {result['institution']} ({result['country']})")
            print(f"   ğŸ“Š ç›¸ä¼¼åº¦: {result['similarity_score']:.4f}")
            print(f"   ğŸ“ å†…å®¹é•¿åº¦: {result['content_length']} å­—ç¬¦")
            
            if result['homepage']:
                print(f"   ğŸ”— ä¸»é¡µ: {result['homepage']}")
            
            if result['research_areas']:
                print(f"   ğŸ”¬ ç ”ç©¶é¢†åŸŸ: {', '.join(result['research_areas'][:5])}")
            
            if show_snippet and result['snippet']:
                print(f"   ğŸ“ ç‰‡æ®µ: {result['snippet'][:300]}")
            
            print()


def main():
    """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""
    # åˆå§‹åŒ–æœç´¢å¼•æ“
    engine = ProfessorSearchEngine()

    # ç¤ºä¾‹1: æ— è¿‡æ»¤çš„æœç´¢ï¼ˆå¸¦ AI æ€»ç»“ï¼‰
    print("\n" + "="*80)
    print("ç¤ºä¾‹ 1: æœç´¢ 'reinforcement learning, sequential decision making' (æ— è¿‡æ»¤, top 20)")
    print("="*80)
    results = engine.search("reinforcement learning, sequential decision making", top_k=20)
    engine.print_results(results)

    # ç¤ºä¾‹2: æŒ‰åœ°åŒºè¿‡æ»¤ï¼ˆå¸¦ AI æ€»ç»“ï¼‰
    print("\n" + "="*80)
    print("ç¤ºä¾‹ 2: æœç´¢ 'machine learning' (ä»…äºšæ´², top 20)")
    print("="*80)
    results = engine.search("machine learning", top_k=20, regions=["Asia"])
    engine.print_results(results)

    # ç¤ºä¾‹3: æŒ‰å›½å®¶è¿‡æ»¤ï¼ˆå¸¦ AI æ€»ç»“ï¼‰
    print("\n" + "="*80)
    print("ç¤ºä¾‹ 3: æœç´¢ 'recommender system and large language models' (ä»…ç¾å›½, top 20)")
    print("="*80)
    results = engine.search(
        "recommender system and large language models", top_k=20, countries=["United States", "us"]
    )
    engine.print_results(results)


if __name__ == "__main__":
    main()
