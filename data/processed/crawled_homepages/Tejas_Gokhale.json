{
  "name": "Tejas Gokhale",
  "homepage": "https://www.tejasgokhale.com",
  "status": "success",
  "content": "Tejas Gokhale Tejas Gokhale SEMINAR TEACHING PEOPLE RESEARCH HOME Assistant Professor Computer Science & Electrical Engineering University of Maryland, Baltimore County Director Cognitive Vision Group Affiliate Faculty UMBC Center for AI Host, PPR Seminar Teaching CMSC 472/672 Computer Vision [F25] CMSC 475/675 Neural Networks [S25] CMSC 491/691 Robust Machine Learning [F24] CMSC 491/691 Computer Vision [S24, F23] Tutorials: Responsibly Building Generative Models [RBGM @ECCV 2024] Reliability of Generative Models in Vision [RGMV @WACV 2024] Semantic Data Engineering for Robustness under Multimodal Settings [SERUM @WACV 2023] Office Hours W 1430--1530; ITE 342-B Upcoming Travel Jun 10-16, CVPR 2025 Oct 17-22, ICCV 2025 I'm a philosopher, scientist, and professor of computing and grapple with questions in computational perception, learning, reasoning, and communication. I direct the Cognitive Vision Group at UMBC, working on research themes such as: concept-level characterization of the visual world; the interpretation of visual data in presence of incomplete information; recognizing and adapting to novelty and variations; leveraging external knowledge and reasoning modules to generalize to new contexts, domains, environments, and tasks; acquiring visual knowledge and communicating it to other machines and humans. I received my Ph.D. from Arizona State University, my M.S. from Carnegie Mellon University, and my B.E. (Honours) degree from Birla Institute of Technology and Science, Pilani. I run the Perception, Prediction, and Reasoning Seminar at UMBC. If you're interested in working with CVG please read this note and use this form. Note: I'm not looking for new Ph.D. students at this time. News OCT 2025 Spotlight talk at ICCV 2025 Area Chair Workshop OCT 2025 Serving at Tutorial Chair for ICCV 2025 JUN 2025 Vocalist for CVPR House Band, Music City Center, Nashville JAN 2025 Received CIDER funding (with GESTAR-II) from UMBC ORD JAN 2025 Received funding from UMBC Cybersecurity Institute DEC 2024 Co-PI for a grant from the DARPA SciFy program. OCT 2024 My alma mater ASU wrote a profile feature about me. SEP 2024 AI Magazine Article summarizing our work so far. SEP 2024 Tutorial at ECCV 2024 on Responsibly Building Generative Models JUL 2024 New Book on Multimodal Retrieval and Generation JUN 2024 Our paper led by Yiran Luo wins Best Paper Award at VDU Workshop @ CVPR 2024 JUL 2024 Received START funding from UMBC ORD JUN 2024 Vocalist for CVPR House Band; Seattle Convention Center JUN 2024 Participating in SCALE 2024 (Video-Based Event Retrieval) at JHU HLTCOE MAY 2024 Received SURFF funding from UMBC ORD APR 2024 Serving as Area Chair for Neurips, ACL, NAACL MAR 2024 Received in-kind support from Microsoft Research under the Accelerate Foundation Models Academic Research Initiative FEB 2024 Invited Talk at AAAI 2024 New Faculty Highlights Publications Side Effects of Erasing Concepts from Diffusion Models EMNLP Findings 2025 Shaswati Saha, Sourajit Saha, Manas Gaur, Tejas Gokhale, pdf code Concerns about text-to-image (T2I) generative models infringing on privacy, copyright, and safety have led to the development of Concept Erasure Techniques (CETs). The goal of an effective CET is to prohibit the generation of undesired ``target'' concepts specified by the user, while preserving the ability to synthesize high-quality images of the remaining concepts. In this work, we demonstrate that (1) CETs can be easily circumvented: using superclass-subclass hierarchy, semantically similar prompts such as compositional variants of the target; and (2) CETs suffer from several side-effects such as attribute leakage and counterintuitive phenomena of attention concentration or dispersal. Latent Diffusion Unlearning: Protecting against Unauthorized Personalization through Trajectory Shifted Perturbations ACM Multimedia 2025 Naresh Kumar Devulapally, Shruti Agarwal, Tejas Gokhale, Vishnu Suresh Lokhande pdf The effectiveness of personalization techniques in text-to-image generaiton has lead to concerns regarding data privacy, intellectual property protection, and unauthorized usage. We present a model-based perturbation strategy that is resistant to inversion and personalization while ensuring that the perturbed images maintain high visual fidelity to the original inputs. This approach integrates unlearnability into the framework of Latent Diffusion Models, alternating between denoising and inversion while modifying the starting point of the denoising trajectory of diffusion models. Voila: Evaluation of MLLMs For Perceptual Understanding and Analogical Reasoning ICLR 2025 Nilay Yilmaz, Maitreya Patel, Yiran Luo, Tejas Gokhale, Chitta Baral, Suren Jayasuriya, Yezhou Yang pdf code data VOILA, a large-scale, open-ended, dynamic benchmark designed to evaluate MLLMsâ€™ perceptual understanding and abstract relational reasoning. VOILA employs an analogical mapping approach in the visual domain, requiring models to generate an image that completes an analogy between two given image pairs, reference and application, without relying on predefined choices. Improving Shift Invariance in Convolutional Neural Networks with Translation Invariant Polyphase Sampling WACV 2025 Sourajit Saha, Tejas Gokhale pdf code We identify that the tendency of existing pooling layers in CNNs to pass larger signals to subsequent layers is a major factor that's strongly correlated with the lack of shift invariance in CNNs. Based on this finding, we design a new pooling operator Translation-Invariant Polyphase Sampling (TIPS) and two regularizations on the intermediate feature maps to learn translation-invariant representations. TIPS results in consistent and architecture-agnostic improvements in accuracy and four measures of shift invariance, across multiple image classification and segmentation benchmarks. TripletCLIP: Improving Compositional Reasoning of CLIP via Vision-Language Negatives NeurIPS 2024 Maitreya Patel, Abhiram Kusumba, Sheng Cheng, Changhoon Kim, Tejas Gokhale, Chitta Baral, Yezhou Yang pdf web code data A method for generating ``hard'' negative captions via in-context learning and synthesizing corresponding negative images with text-to-image generators. A novel contrastive pre-training strategy that leverages these hard negative captions and images in an alternating fashion to train CLIP. Our method \"TripletCLIP\" enhances the compositional capabilities of CLIP as well as improvements in zero-shot image classification and image retrieval. Getting it Right: Improving Spatial Consistency in Text-to-Image Models ECCV 2024 Agneet Chatterjee, Gabriela Ben Melech Stan, Estelle Aflalo, Sayak Paul, Dhruba Ghosh, Tejas Gokhale, Ludwig Schmidt, Hannaneh Hajishirzi, Vasudev Lal, Chitta Baral, Yezhou Yang pdf web code demo We improve spatial understanding of T2I models by creating the SPRIGHT dataset by recaptioning 6M images from widely used vision datasets. Finetuning T2I models with just 500 images from SPRIGHT leads to a large improvement in T2I spatial understanding performance, across several evaluation benchmarks such as T2I-CompBench, VISOR, and GenEval. REVISION: Rendering Tools Enable Spatial Fidelity in Vision-Language Models ECCV 2024 Agneet Chatterjee, Yiran Luo, Tejas Gokhale, Chitta Baral, Yezhou Yang pdf web code data Traditional generative Text-to-Image models struggle to generate images that faithfully represent the spatial relationships mentioned in the input prompt. We develop REVISION, an efficient rendering pipeline that enables a training-free, guidance-based mechanism to address this shortcoming. REVISION takes the objects and their spatial relationships parsed from the given input prompt and synthesizes an image in Blender, placing the respective object assets at coordinates corresponding to the parsed spatial relationship. Given a user-provided input prompt T, we synthesize an image using REVISION and use it to guide existing T2I pipelines such as Stable Diffusion or ControlNet to obtain a spatially accurate output. On the Robustness of Language Guidance for Low-Level Vision Tasks: Findings from Depth Estimation CVPR 2024 Agneet Chatterjee, Tejas Gokhale, Chitta Baral, Yezhou Yang pdf web code The motivation of this work is to analyze the efficacy of language guidance for low-level non-semantic computer vision tasks. We focus on depth estimation and find that language-guided depth estimators benefit only from scene-level language information and counter-intuitively, are worse off when presented with sentences that describe 3D spatial relationships in the scene. With an increasing number of methods using language for depth estimation, our findings highlight the opportunities and pitfalls that require careful consideration for effective deployment in real-world settings. Grounding Stylistic Domain Generalization with Quantitative Domain Shift Measures and Synthetic Scene Images CVPR 2024 Workshop on Vision Dataset Understanding Yiran Luo, Joshua Feinglass, Tejas Gokhale, Kuan-Cheng Lee, Chitta Baral, Yezhou Yang pdf code data [Best Paper Award] Two new quantitative measures ICV and IDD to describe domain shifts in terms of consistency of classes within one domain and similarity between two stylistic domains. New dataset: SuperMarioDomains (SMD) incorporating unique features of consistent classes of video game scenes across stylistic domains in video game graphics that are dissimilar to ImageNet1K. ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models AAAI 2024 and Neurips 2023 Workshop on Diffusion Models Maitreya Patel, Tejas Gokhale, Chitta Baral, Yezhou Yang pdf web code Textual inversion models have the potential to learn novel concepts from a small number of example images. We quantify this concept learning ability with ConceptBed: a dataset that contains 284 unique visual concepts and 33K concept compositions, and CCD (Concept Confidence Deviation): an evaluation metric uses the confidence of oracle ",
  "content_length": 24591,
  "method": "requests",
  "crawl_time": "2025-12-01 14:36:29"
}