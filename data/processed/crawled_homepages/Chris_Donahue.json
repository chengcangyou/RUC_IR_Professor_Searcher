{
  "name": "Chris Donahue",
  "homepage": "https://chrisdonahue.com",
  "status": "success",
  "content": "Chris DonahueChris DonahueDannenberg Assistant ProfessorComputer Science Department, Carnegie Mellon UniversityAbout Email: I am an Assistant Professor in the Computer Science Department at Carnegie Mellon University, and also a Research Scientist at Google DeepMind on the Magenta team (part-time).My research goal is to develop and responsibly deploy generative AI for music and creativity, thereby unlocking and augmenting human creative potential. To this end, my work involves (1) improving machine learning methods for controllable generative modeling for music, audio, and other sequential data, and (2) deploying real-world interactive systems that allow a broader audienceâ€”inclusive of non-musiciansâ€”to harness generative music AI through intuitive controls.I am particularly drawn to research ideas with direct real-world applications, and my work often involves building systems for real users to be evaluated in-the-wild. For example, my work on Piano Genie was used in a live performance by The Flaming Lips, and my work on Dance Dance Convolution powers Beat Sage, a live service used by thousands of users a day to create multimodal music game content.Previously, I was a postdoc at Stanford CS advised by Percy Liang. Before that, I completed a PhD at UCSD co-advised by Miller Puckette and Julian McAuley.NewsğŸ¤ (Dec 2025) Two invited talks at NeurIPS 2025 Workshops: GenProCC and AI4MusicğŸ§‘â€ğŸ« (Nov 2025) Reappointed as Assistant Professor at CMU.ğŸ¤ (Oct 2025) Invited talk What music can teach language models, CMU LTI Colloquium (recording)ğŸ“œ (Sep 2025) Two papers (Music Arena, Live Music Models) accepted to NeurIPS 2025 Creative AI Track, to be presented in the main conference.ğŸ“œ (Sep 2025) Paper accepted to the LLM4Music workshop @ ISMIR 2025ğŸŒ (Aug 2025) My Google research project SingSong featured in the Pixel Recorder appğŸŒ (Jul 2025) Music Arena released (paper)!ğŸ“œ (Jul 2025) Paper accepted at WASPAA 2025 on sound morphing.ğŸ“œ (Jul 2025) Two papers accepted to ISMIR 2025 on music evaluation and real-time adaptation (pre-print forthcoming).ğŸ“œ (Jun 2025) Two papers accepted at ICML 2025 workshops (R2-FM, DataWorld, pre-prints forthcoming).ğŸŒ (Jun 2025) Led the effort of a new open weights real-time music generation model along with my team at Google DeepMind: Magenta RealTime.ğŸ“œ (May 2025) Our paper on Copilot Arena accepted to ICML 2025.ğŸ—ï¸ (May 2025) My PhD student Wayne Chi (co-advised w/ Ameet Talwalkar) quoted in WSJ article.ğŸ—ï¸ (Apr 2025) CMU SCS news article featuring Copilot Arena.ğŸ… (Apr 2025) Our paper on co-design for audio codec LMs received the Best Paper Award (top 1) at the NAACL Student Research Workshop 2025.ğŸ“ (Apr 2025) My PhD student Wayne Chi (co-advised w/ Ameet Talwalkar) has received the NDSEG Fellowship.ğŸ“œ (Mar 2025) Paper on Copilot Arena accepted to the HEAL@CHI workshop.ğŸ¶ (Mar 2025) Project proposal w/ Annie Hsieh (CMU CFA) funded by the AIxArts incubator fund at CMU.ğŸ› ï¸ (Mar 2025) Workshop proposal on ML for Audio accepted at ICML 2025.ğŸ… (Mar 2025) Our paper on AMUSE recognized with a Best Paper Award (top 1% of submissions) at CHI 2025.ğŸ“œ (Mar 2025) Paper accepted at the NAACL Student Research Workshop 2025.ğŸ¶ (Mar 2025) Shoutout from Darkside for helping them train RAVE for their album Nothing.ğŸ“œ (Feb 2025) Our work on VERSA accepted to NAACL Demo Track 2025.ğŸ“œ (Feb 2025) New pre-print on Copilot ArenağŸ“œ (Jan 2025) Our work on AMUSE accepted to CHI 2025.ğŸ—ï¸ (Nov 2024) Blog post on Copilot ArenağŸ“œ (Nov 2024) One paper accepted at the NeurIPS 2024 Open World Agents Workshop.ğŸ¤ (Oct 2024) Invited talk at SANE 2024 (video, slides).ğŸŒ (Oct 2024) Launch of Copilot Arena, a VSCode extension for evaluating LLMs for coding assistance.ğŸ“œ (Oct 2024) Three extended abstracts to appear at ISMIR Late Breaking Demos.ğŸ“œ (Oct 2024) One paper accepted at the NeurIPS 2024 Audio Imagination Workshop.ğŸŒ (Aug 2024) Official launch of Hookpad Aria, a Copilot for songwriters.ğŸ“œ (Jun 2024) Our work on Music-aware Virtual Assistants accepted at UIST 2024.ğŸ“œ (Jun 2024) Two papers accepted to ISMIR 2024.ğŸª‘ (Apr 2024) Named as the Dannenberg Assistant Professor of Computer Science.ğŸ›ï¸ (Apr 2024) Serving as Senior Program Committee Co-chair for ISMIR 2024 - record number of submissions (415).ğŸŒ (Mar 2024) A Copilot-like tool for musicians featuring the Anticipatory Music Transformer was launched in beta.ğŸ“œ (Mar 2024) Music ControlNet to appear in TASLP (IEEE/ACM Transactions on Audio, Speech, and Language Processing).ğŸ“œ (Mar 2024) Anticipatory Music Transformer to appear in TMLR (Transactions on Machine Learning Research).ğŸŒ (Mar 2024) Launch of MusicFX DJ Mode, a real-time music audio generation tool developed by my team at Google.ğŸ—ï¸ (Nov 2023) SingSong incorporated into Google DeepMindâ€™s Music AI Tools.ğŸ“œ (Nov 2023) Work presented at HCMIR Workshop by Michael Feffer.ğŸ“œ (Nov 2023) New preprint on controllable music gen led by Shih-Lun Wu (applying to PhD positions!)ğŸ—ï¸ (Oct 2023) Interviewed for Pitchfork article about MusicLMğŸ¤ (Oct 2023) Invited talk at Stanford HAI Conference (recording, slides)ğŸ§‘â€ğŸ« (Oct 2023) Guest lecture for CMU LLM Course (slides)ğŸ‘‹ (Oct 2023) New PhD students: Irmak Bukey and Wayne ChiğŸ§‘â€ğŸ« (Sep 2023) Started as Assistant Professor at CMUG-CLefI lead the Generative Creativity Lab (G-CLef) at CMU. Our mission is to empower and enrich human creativity and productivity with generative AI. We focus primarily on the intersection of music and AI, though we also work on other applications such as programming, gaming, and natural language. Please visit this page to learn more about our research interests and to apply.MenteesIrmak BukeyCSD PhD studentWayne ChiCSD PhD studentCoadvised w/ Ameet T.Yewon KimCSD PhD studentNathan PruyneCSD PhD studentAlumniAlexander WangMusic Tech MS studentNow PhD @ CMU HCII (incoming)Yichen (William) HuangVisiting researcherXun (Rick) ZhouCS MS studentNow Quant @ MinhongShih-Lun WuLTI MS studentNow PhD @ MIT EECSRecent PapersQuickly discover relevant content by filtering publications. Yewon Kim, Sung-Ju Lee, Chris Donahue (2024). Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations. In CHI 2025.arXiv PDF BibTeX Sound ğŸ”Š Satvik Dixit, Laurie M. Heller, Chris Donahue (2024). Vision Language Models Are Few-Shot Audio Spectrogram Classifiers. In NeurIPS Audio Imagination Workshop 2024.arXiv PDF BibTeX Xun Zhou, Charlie Ruan, Zihe Zhao, Tianqi Chen, Chris Donahue (2024). Local Deployment of Large-Scale Music AI Models on Commodity Hardware. In ISMIR LBD 2024.arXiv PDF BibTeX ğŸ•¹ï¸ Demo Irmak Bukey, Michael Feffer, Chris Donahue (2024). Just Label the Repeats for In-The-Wild Audio-to-Score Alignment. In ISMIR 2024.arXiv PDF BibTeX Code Video Examples Chris Donahue, Shih-Lun Wu, Yewon Kim, Dave Carlton, Ryan Miyakawa, John Thickstun (2024). Hookpad Aria: A Copilot for Songwriters. In ISMIR LBD 2024.arXiv PDF BibTeX Project PageSee all publications Cite Ã— Copy Download",
  "content_length": 6904,
  "method": "requests",
  "crawl_time": "2025-12-01 12:51:35"
}