{
  "name": "Nicolas Loizou",
  "homepage": "https://nicolasloizou.github.io",
  "status": "success",
  "content": "Nicolas Loizou About News Publications Team Talks Bio Professional Services Teaching Open Positions üéì Open PhD and Postdoc positions for Summer 2026. For more details see: Open Positions About I am an Assistant Professor in the Department of Applied Mathematics and Statistics (AMS) and the Mathematical Institute for Data Science (MINDS), with a secondary appointment in the Department of Computer Science at Johns Hopkins University. I am also affiliated with the JHU Machine Learning Group and the Ralph O‚ÄôConnor Sustainable Energy Institute (ROSEI). My research interests include large-scale optimization, machine learning, randomized numerical linear algebra, distributed and decentralized algorithms, game theory, and deep learning. My current research focuses on the theory and applications of convex and non-convex optimization in large-scale machine learning and data science problems. Prior to this, I was an IVADO postdoctoral fellow at Mila - Quebec Artificial Intelligence Institute and DIRO, UdeM, where I worked closely with Simon Lacoste-Julien and Ioannis Mitliagkas. I obtained my PhD from The University of Edinburgh, School of Mathematics in 2019, under the supervision of Peter Richtarik. Before that, I spent 4 beautiful years in Athens as undergraduate student in the Department of Mathematics at National and Kapodistrian University of Athens and 1 year as postgraduate student at Imperial College London where I obtained an MSc in Computing. During the fall of 2018, I was a research intern at Facebook AI Research in Montreal, Canada. For more details please feel free to look my CV (updated August 2025). Selected Recent News (For the full list of news please check the News tab.) 23 Nov 2025: ‚Äî I have recently joined the editorial boards of: Optimization Methods and Software, and Transactions on Machine Learning Research. Please consider submitting your best work at the intersection of optimization and machine learning! 07 Oct 2025: Recent highlights (posted after I enjoyed time with our newborn): September 2025 ‚Äî I‚Äôm delighted to share that my first doctoral student, Sayantan Choudhury, successfully defended his PhD dissertation. We began working together in Summer 2022, and over the past three years it has been a joy to watch him grow into a very strong researcher. During his PhD he co-authored seven papers (including 4 NeurIPS and 2 ICLR). His thesis entitled \"Next-Generation Iterative Algorithms for Large-Scale Min-Max Optimization: Design and Analysis\" will be available online soon. September 2025 ‚Äî Two papers led by members of my group were accepted to NeurIPS 2025: Multiplayer Federated Learning: Reaching Equilibrium with Less Communication, TaeHo Yoon, Sayantan Choudhury, Nicolas Loizou. Extragradient Method for $(L_0, L_1)$-Lipschitz Root-finding Problems, Sayantan Choudhury, Nicolas Loizou. August 2025 ‚Äî Together with Ernest Ryu (UCLA), we received an NSF CISE Medium award for the project ‚ÄúPost-Modern Min-Max Optimization Theory: Departure from Classical Minimization Theory.‚Äù The project focuses on developing specialized theoretical frameworks and efficient algorithms tailored to unconstrained min-max optimization. June 2025 ‚Äî I joined the editorial board of Information and Inference: A Journal of the IMA. Please consider submitting your best work in optimization and machine learning! May 2025 ‚Äî Our paper Federated Learning for Renal Tumor Segmentation and Classification on Multi-Center MRI Dataset was accepted for publication in the Journal of Magnetic Resonance Imaging. January 2025 ‚Äî Two papers, joint work with Dimitris Oikonomou, were accepted to ICLR 2025: Sharpness Aware Minimization: General Analysis and Improved Rates. Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance. 15 Jan 2025: I will be attending the ROSEI Summit 2025 at JHU. During the afternoon session, I will present key highlights of our ongoing work on large-scale min-max optimization and its connections to energy systems. 27 Oct 2024: Over the next 4 weeks, I will be giving a few presentations on our recent and ongoing work in stochastic min-max optimization, variational inequalities (VIPs), and adaptive methods. Slides and videos will be available after the presentations. I look forward to meeting with colleagues at Wisconsin-Madison, Minnesota, Princeton, and Rice. 25 Nov 2024: Rice CMOR Department Colloquium, Computational Applied Mathematics & Operations Research Department, Rice University. 21 Nov 2024: Princeton Optimization Seminar , Operations Research & Financial Engineering Department, Princeton University. 12 Nov 2024: CSE DSI Machine Learning Seminar Series , University of Minnesota. 30 Oct 2024: SILO Seminar, University of Wisconsin-Madison. 25 Sept 2024: Two papers were accepted to NeurIPS 2024: Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad Enhancing Vision-language Models for Medical Imaging: Bridging the 3D Gap with Innovative Slice Selection 22 July 2024: On my way to 25th International Symposium on Mathematical Programming (ISMP 2024). Tomorrow, I‚Äôll be presenting our latest findings on Stochastic Extragradient Methods at the Numerical Optimization for Machine Learning session. On Thursday, we organize a minisymposium on Adaptive, Line-search, and Parameter-Free Stochastic Optimization. 18 June 2024: During summer, I am based in Toronto at Vector Institute. Today I give a talk at the institute's weekly seminar on \"Scalable Optimization Algorithms for Large-Scale Machine Learning Models\". 06 May 2024: This week, I am visting the Department of Computing at Imperial College London. On Tuesday, I will give a talk at the Computational Optimisation Seminar on \"Next-Generation Adaptive Optimization Algorithms for Large-Scale Machine Learning Models\". I had a great time catching up with Panos Parpas, Wolfram Wiesemann, and Ruth Misener! 02-04 May 2024: I am attending AISTATS 2024 in Valencia, Spain. Konstantinos is also here, presenting our work: Stochastic Extragradient with Random Reshuffling: Improved Convergence for Variational Inequalities. 22-24 Mar 2024: I am attending 2024 INFORMS Optimization Society Conference (IOS 2023) at Houston, Texas. On Sunday, 24 March 2024, I will be presenting some of our latest findings on adaptive optimization algorithms. Title of the talk: \"Advancing SGD: Exploring the Interplay of Stochastic Polyak Step-size and Momentum.\" 19 Jan 2024: Our paper \"Stochastic Extragradient with Random Reshuffling: Improved Convergence for Variational Inequalities\", joint work with Konstantinos Emmanouilidis and Rene Vidal, was accepted to AISTATS 2024. Special congrats to Konstantinos Emmanouilidis on the acceptance of his first paper! 16 Jan 2024: Our paper Communication-Efficient Gradient Descent-Accent Methods for Distributed Variational Inequalities: Unified Analysis and Local Updates, joint work with Siqi Zhang, Sayantan Choudhury, and Sebastian U Stich was accepted to ICLR 2024. 10 Dec 2023: I am traveling to New Orleans to attend NeurIPS 2023. Sayantan Choudhury is also here, presenting our work: Single-Call Stochastic Extragradient Methods for Structured Non-monotone Variational Inequalities: Improved Analysis under Weaker Conditions. 15 Nov 2023: This week, I am co-organizing the DeepMath 2023 Conference on the Mathematical Theory of Deep Neural Networks, which takes place at JHU. I am looking forward to the excellent set of speakers and the poster sessions. 18 Oct 2023: This week, I am visting the Combinatorics and Optimization Department at the University of Waterloo. On Friday, 20 October, I will give a talk at the Continous Optimization Seminar on \"Next-Generation Adaptive Optimization Algorithms for Large-Scale Machine Learning Models\". Steve Vavasis and Yaoliang Yu thanks for the great hospitality! 22 Sept 2023: Our paper Single-Call Stochastic Extragradient Methods for Structured Non-monotone Variational Inequalities: Improved Analysis under Weaker Conditions, joint work with Sayantan Choudhury, and Eduard Gorbunov was accepted to NeurIPS 2023. Special congrats to Sayantan Choudhury on the acceptance of his first paper! 18 Sept 2023: I am visiting Flatiron Institute in New York City this week. 28 Aug. 2023: New Phd Student: Dimitris Oikonomou Dimitris Oikonomou is a new member in my Lab. He has just started his Ph.D. in the CS department. Dimitris got his BSc in Mathematics from the National and Kapodistrian University of Athens. He then obtained two MSE degrees: MSc in Mathematics from the University of Gottingen, Germany and MSc in Data Science and Machine Learning from National Technical University of Athens. All BSc and MSc degrees with distinction! He was also very active in Mathematical Olympiads competitions: International Mathematical Olympiad (IMO): Bronze Medal (Colombia 2013) Greek Mathematical Olympiad: Silver Medal (2014) Greek Mathematical Olympiad: Bronze Medal (2013) Dimitri, welcome to the team! 21 Aug 2023: Our paper Unified Analysis of Stochastic Gradient Methods for Composite Convex and Smooth Optimization , joint work with Ahmed Khaled, Othmane Sebbouh, Robert M. Gower, and Peter Richt√°rik was accepted to Journal of Optimization Theory and Applications (JOTA) 12 Jul 2023: New Paper out: Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes, joint work with Sohom Mukherjee, and Sebastian U Stich. 08 Jun 2023: New Paper out: Communication-Efficient Gradient Descent-Accent Methods for Distributed Variational Inequalities: Unified Analysis and Local Updates , joint work with Siqi Zhang, Sayantan Choudhury, and Sebastian U Stich. 31 May 2023: Attending SIAM Conference on Optimization (OP23) in Seattle this week! Reach out if you would like to meet. On June 1, with Siqi Zhang and Sayantan Choudhury , we organize a 4-session mini-symposium on \"Recent Advancements in Optimization Methods for Machine Learning\". 22-24 Mar 2023: I am attending 57th Annual Conference on Information Science an",
  "content_length": 18351,
  "method": "requests",
  "crawl_time": "2025-12-01 14:05:30"
}