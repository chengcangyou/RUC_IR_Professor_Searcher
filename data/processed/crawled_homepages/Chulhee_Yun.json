{
  "name": "Chulhee Yun",
  "homepage": "https://chulheeyun.github.io",
  "status": "success",
  "content": "Chulhee “Charlie” Yun Search Chulhee Yun Ewon Assistant Professor Kim Jaechul Graduate School of AI KAIST My name is Chulhee (I go by Charlie), and I am an Ewon Assistant Professor at KAIST Kim Jaechul Graduate School of AI (KAIST AI). Starting September 2025, I also hold a joint affiliation with KAIST Graduate School of AI for Math and a part-time Visiting Faculty Researcher position at Google Research. I direct the Optimization & Machine Learning (OptiML) Laboratory at KAIST AI. I finished my PhD from the Laboratory for Information and Decision Systems at Massachusetts Institute of Technology, where I was fortunate to study under the joint supervision of Prof. Suvrit Sra and Prof. Ali Jadbabaie. Before MIT, I was a master’s student in Electrical Engineering at Stanford University, where I worked with Prof. John Duchi. I finished my undergraduate program in Electrical Engineering at KAIST. Email: {firstname}.{lastname}@kaist.ac.kr Phone: +82-2-958-3919 Office: KAIST Seoul Campus Building #9, 9401 Job opening: As a member of the Hyper-scale Language Model Innovation Research Group, I am seeking an InnoCORE Fellow (eligible within five years of earning a PhD) to work on both theoretical and practical aspects of large language models (LLMs), with a particular focus on length generalization and reasoning capabilities. Please email me your CV if you’re interested or would like further information. For prospective students: If you are curious about what kinds of research I do, please see this interview article (in Korean). I look for self-motivated graduate students with strong mathematical backgrounds. If you are an undergraduate student interested in interning at our lab, consider applying for summer/winter KAIST AI Research Internship (KAIRI) programs. Interests Deep Learning Theory Optimization Machine Learning Theory Education PhD in Elec. Eng. & Comp. Sci., 2016–2021 Massachusetts Institute of Technology MSc in Electrical Engineering, 2014–2016 Stanford University BSc in Electrical Engineering, 2007–2014 KAIST News [Sep 2025] Three papers got accepted to NeurIPS 2025. Congrats and thanks to my co-authors! [Aug 2025] I will be serving as a Social Chair for ICML 2026, which will be held in Seoul! [Aug 2025] I had the pleasure of attending Mathematical and Scientific Machine Learning 2025 (MSML 2025) as a keynote speaker. [Jun 2025] Four papers got accepted to the 3rd Workshop on High-dimensional Learning Dynamics (HiLD) at ICML 2025. [May 2025] Four papers got accepted to ICML 2025. Congrats to my co-authors! See all Publications Implicit Bias and Loss of Plasticity in Matrix Completion: Depth Promotes Low-Rank Solutions Baekrok Shin, Chulhee Yun ICML 2025 Workshop on High-dimensional Learning Dynamics (HiLD) The Cost of Robustness: Tighter Bounds on Parameter Complexity for Robust Memorization in ReLU Nets Yujun Kim, Chaewon Moon, Chulhee Yun NeurIPS 2025 ICML 2025 Workshop on High-dimensional Learning Dynamics (HiLD) KAIA Outstanding Paper Award at KAIA Summer Conference 2025 (CKAIA 2025) From Linear to Nonlinear: Provable Weak-to-Strong Generalization through Feature Learning Junsoo Oh, Jerry Song, Chulhee Yun NeurIPS 2025 ICML 2025 Workshop on High-dimensional Learning Dynamics (HiLD) KT Best Paper Award at KAIA Summer Conference 2025 (CKAIA 2025) Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training arXiv Minhak Song, Beomhan Baek, Kwangjun Ahn, Chulhee Yun NeurIPS 2025 ICML 2025 Workshop on High-dimensional Learning Dynamics (HiLD) Provable Benefit of Random Permutations over Uniform Sampling in Stochastic Coordinate Descent Paper arXiv Donghwa Kim, Jaewook Lee, Chulhee Yun ICML 2025 Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More Paper arXiv Geonhui Yoo, Minhak Song, Chulhee Yun ICML 2025 Incremental Gradient Descent with Small Epoch Counts is Surprisingly Slow on Ill-Conditioned Problems Paper arXiv Yujun Kim, Jaeyoung Cha, Chulhee Yun ICML 2025 Lightweight Dataset Pruning without Full Training via Example Difficulty and Prediction Uncertainty Paper arXiv Yeseul Cho, Baekrok Shin, Changmin Kang, Chulhee Yun ICML 2025 ICLR 2025 Workshop on Navigating and Addressing Data Problems for Foundation Models Convergence and Implicit Bias of Gradient Descent on Continual Linear Classification Paper arXiv Hyunji Jung, Hanseul Cho, Chulhee Yun ICLR 2025 Best Paper Award at KAIA Fall Conference 2024 (JKAIA 2024) Parameter Expanded Stochastic Gradient Markov Chain Monte Carlo Paper arXiv Hyunsu Kim, Giung Nam, Chulhee Yun, Hongseok Yang, Juho Lee ICLR 2025 Arithmetic Transformers Can Length-Generalize in Both Operand Length and Count Paper arXiv Hanseul Cho, Jaeyoung Cha, Srinadh Bhojanapalli, Chulhee Yun ICLR 2025 Does SGD really happen in tiny subspaces? Paper arXiv Minhak Song, Kwangjun Ahn, Chulhee Yun ICLR 2025 ICML 2024 Workshop on High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning Stochastic Extragradient with Flip-Flop Shuffling & Anchoring: Provable Improvements Paper arXiv Jiseok Chae, Chulhee Yun, Donghwan Kim NeurIPS 2024 DASH: Warm-Starting Neural Network Training in Stationary Settings without Loss of Plasticity Paper arXiv Baekrok Shin, Junsoo Oh, Hanseul Cho, Chulhee Yun NeurIPS 2024 ICML 2024 Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization Provable Benefit of Cutout and CutMix for Feature Learning Paper arXiv Junsoo Oh, Chulhee Yun NeurIPS 2024 (Spotlight) ICML 2024 Workshop on High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning KT Best Paper Award at KAIA Summer Conference 2024 (CKAIA 2024) Position Coupling: Improving Length Generalization of Arithmetic Transformers Using Task Structure Paper arXiv Hanseul Cho, Jaeyoung Cha, Pranjal Awasthi, Srinadh Bhojanapalli, Anupam Gupta, Chulhee Yun NeurIPS 2024 ICML 2024 Workshop on Long-Context Foundation Models Gradient Descent with Polyak's Momentum Finds Flatter Minima via Large Catapults arXiv Prin Phunyaphibarn, Junghyun Lee, Bohan Wang, Huishuai Zhang, Chulhee Yun ICML 2024 Workshop on High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning (Oral) Fundamental Benefit of Alternating Updates in Minimax Optimization Paper arXiv Jaewook Lee, Hanseul Cho, Chulhee Yun ICML 2024 (Spotlight) ICLR 2024 Workshop on Bridging the Gap Between Practice and Theory in Deep Learning Linear attention is (maybe) all you need (to understand transformer optimization) Paper arXiv Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, Suvrit Sra ICLR 2024 NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning (Oral) Fair Streaming Principal Component Analysis: Statistical and Algorithmic Viewpoint Paper arXiv Junghyun Lee, Hanseul Cho, Se-Young Yun, Chulhee Yun NeurIPS 2023 Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory Paper arXiv Minhak Song, Chulhee Yun NeurIPS 2023 PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning Paper arXiv Hojoon Lee, Hanseul Cho, Hyunseung Kim, Daehoon Gwak, Joonkee Kim, Jaegul Choo, Se-Young Yun, Chulhee Yun NeurIPS 2023 Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima Paper arXiv Dongkuk Si, Chulhee Yun NeurIPS 2023 (Spotlight) KAIA Outstanding Paper Award at KAIA Summer Conference 2023 (CKAIA 2023) Provable Benefit of Mixup for Finding Optimal Decision Boundaries Paper arXiv Junsoo Oh, Chulhee Yun ICML 2023 Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond Paper arXiv Jaeyoung Cha, Jaewook Lee, Chulhee Yun ICML 2023 (Oral) On the Training Instability of Shuffling SGD with Batch Normalization Paper arXiv David X. Wu, Chulhee Yun, Suvrit Sra ICML 2023 SGDA with shuffling: faster convergence for nonconvex-PŁ minimax optimization Paper arXiv Hanseul Cho, Chulhee Yun ICLR 2023 NAVER Outstanding Theory Paper Award at KAIA-NAVER Joint Fall Conference 2022 (JKAIA 2022) Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond Paper arXiv Chulhee Yun, Shashank Rajput, Suvrit Sra ICLR 2022 (Oral) Open Problem: Can Single-Shuffle SGD be Better than Reshuffling SGD and GD? Paper Long version Chulhee Yun, Suvrit Sra, Ali Jadbabaie COLT 2021 Provable Memorization via Deep Neural Networks using Sub-linear Parameters Paper arXiv Sejun Park, Jaeho Lee, Chulhee Yun, Jinwoo Shin COLT 2021 Presented as part of a contributed talk at DeepMath 2020 A Unifying View on Implicit Bias in Training Linear Neural Networks Paper arXiv Chulhee Yun, Shankar Krishnan, Hossein Mobahi ICLR 2021 NeurIPS 2020 Workshop on Optimization for Machine Learning (OPT 2020) Minimum Width for Universal Approximation Paper arXiv Sejun Park, Chulhee Yun, Jaeho Lee, Jinwoo Shin ICLR 2021 (Spotlight) Presented as part of a contributed talk at DeepMath 2020 SGD with shuffling: optimal rates without component convexity and large epoch requirements Paper arXiv Kwangjun Ahn, Chulhee Yun, Suvrit Sra NeurIPS 2020 (Spotlight) $O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers Paper arXiv Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, Sanjiv Kumar NeurIPS 2020 Low-Rank Bottleneck in Multi-head Attention Models Paper arXiv Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank J. Reddi, Sanjiv Kumar ICML 2020 Are Transformers universal approximators of sequence-to-sequence functions? Paper arXiv Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J. Reddi, Sanjiv Kumar ICLR 2020 NeurIPS 2019 Workshop on Machine Learning with Guarantees Honorable Mention at NYAS Machine Learning Symposium 2020 Poster Awards Are deep ResNets provably better than linear predictors? Paper arXiv Chulhee Yun, Suvrit S",
  "content_length": 12946,
  "method": "requests",
  "crawl_time": "2025-12-01 12:53:49"
}