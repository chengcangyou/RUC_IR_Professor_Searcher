{
  "name": "Jun-Yan Zhu",
  "homepage": "https://people.csail.mit.edu/junyanz",
  "status": "success",
  "content": "Jun-Yan Zhu's Homepage Jun-Yan Zhu Assistant Professor School of Computer Science Carnegie Mellon University Email: junyanz at cs dot cmu dot edu Lab | Students | CV | Google Scholar | GitHub | arXiv Papers | Talks | Events | Teaching I am the Michael B. Donohue Assistant Professor of Computer Science and Robotics in the School of Computer Science of Carnegie Mellon University. I also hold affiliated faculty appointments in the Computer Science Department and Machine Learning Department. I study computer vision, graphics, computational photography, and generative models. Prior to joining CMU, I was a Research Scientist at Adobe Research. I did a postdoc at MIT CSAIL, working with William T. Freeman, Josh Tenenbaum, and Antonio Torralba. I obtained my Ph.D. from UC Berkeley, under the supervision of Alexei A. Efros. I received my B.E. from Tsinghua University, working with Zhuowen Tu, Shi-Min Hu, and Eric Chang. Code & Events Notes for prospective CMU students. If you are stressed out by graduate school application, check out some Cat Papers. Check out BrickGPT, the first approach for generating physically stable toy brick models from text prompts. Also see CMU News. Check out SVDQuant repo for 4-bit diffusion models with faster inference and less memory. It supports FLUX, FLUX.1-Tools, and img2img-turbo. Copyright and compensation issues in generative models: GenDataAttribution and concept-ablation (see CMU News and the Quartz article). E-latent LPIPS code has been released. It can compute the perceptual loss between two latents for many models (e.g., FLUX, SD 1.5/2.1/XL/3) Check out img2img-turbo repo for pix2pix-turbo and CycleGAN-turbo: one-step image translation for both paired and unpaired settings. Modelverse platform for helping everyone share, discover, and study generative models more easily. Image-to-image translation repos: CycleGAN-and-pix2pix, pix2pixHD, BicycleGAN, vid2vid, GauGAN/SPADE, CUT. Image editing with diffusion models: SDEdit (used in Stable Diffusion Image-to-Image), pix2pix-zero, and Rich-Text-to-Image. Model customization and editing: concept-ablation, custom-diffusion, domain-expanision, model-rewriting, GANSketching, and GANWarping. Image editing repos and demos: iGAN (GAN inversion), GANPaint, pix2latent, sam_inversion, SwappingAutoencoder, and interactive-deep-colorization. Neural tactile synthesis: Tactile DreamFusion, visual-tactile-synthesis, VisGel, and scalable tactile glove. GANs training and evaluation libraries: Vision-aided GANs (pip install vision-aided-loss), DiffAugment, and clean-fid (pip install clean-fid). Synthetic data for computer vision: dataset-distillation, mtt-distillation, GLaD, CyCADA, and gan-ensembling. 3D synthesis code: Total-Recon, BlendNeRF, pix2pix3D, Depth-supervised NeRF, Editing NeRF, Visual Object Networks, and 3D scene de-rendering. Network visualization tools: GANDissect, GANSeeing, and Network Dissect. Efficient generative models: SIGE (for SDEdit w/ Stable Diffusion and GauGAN), gan-compression (for cGANs) and anycost-gan (for GANs). CVPR 2025 AI for Content Creation Workshop. CatPapers: Cool vision, learning, and graphics papers on Cats. Generative Intelligence Lab Our lab studies the collaboration between Human Creators and Generative Models, with the goal of building intelligent machines capable of helping everyone tell their visual stories. We are studying the following questions: Interaction between creators and generative models: How can we help creators control the model outputs more easily? We develop algorithms and interfaces for controllable visual synthesis (e.g., images, videos, 3D, visual+tactile) Rewriting and searching generative models: How can creators repurpose existing models for new tasks, concepts, and styles? How could they rewrite the rules of models? Which model shall they use as a starting point? Co-existence of creators and generative models: How can we allow creators to opt in or out of generative models at any time? If opting in, how do we credit creators for contributing training data? Synthetic data generation with generative models: How can we use generative models to produce useful data for improving computer vision and robotics systems? Our lab is part of Carnegie Mellon Graphics Lab and Carnegie Mellon Computer Vision Group. Ruihan Gao RI PhD (w/ Wenzhen Yuan) Maxwell Jones MLD PhD (w/ Ruslan Salakhutdinov) Nupur Kumari RI PhD Beijia Lu RI MSCV Gaurav Parmar RI PhD (w/ Srinivasa Narasimhan) Ava Pun CSD PhD Sheng-Yu Wang RI PhD Alumni Former Ph.D. students and postdocs Kangle Deng (RI Ph.D. w/ Deva Ramanan, now Research Scientist at Roblox) PhD Thesis: Learning to Create 3D Content: Geometry, Appearance, and Physics Microsoft Research Ph.D. Fellowship, ICCV 2025 Best Paper Award Sean Liu (Postdoc, now Principal AI Research Scientist at Autodesk Research) Former members and visitors: Grace Su (MSR, now PhD student at UMD), Richa Mishra (MSCV, now at HeyGen), Aniruddha Mahapatra (MSCV, now at Adobe), Or Patashnik (Visiting PhD from TAU), Songwei Ge (Visiting PhD from UMD), Chonghyuk (Andrew) Song (MSR, now PhD student at MIT), Muyang Li (MSR, now PhD student at MIT), Daohan (Fred) Lu (MSCV, now PhD student at NYU), Mia Tang (Undergrad, now MS student at Stanford), Bingliang Zhang (Undergrad, now PhD student at Caltech), Rohan Agarwal (MSCV, now at Runway ML), George Cazenavette (MSR, now PhD student at MIT). Teaching 16-726: Learning-based Image Synthesis (Spring 2024, Spring 2023, Spring 2022, Spring 2021) 16-824: Visual Learning and Recognition (Fall 2023, Fall 2022, Fall 2021) Deep Learning at Udacity (Co-instructor) Software Adobe Firefly Our work Custom Diffusion (CVPR 2023) and GigaGAN (CVPR 2023) contribues to the development of Firefly. Web | Video Landscape Mixer Photoshop 2022's Landscape Mixer can transform landscape images in various ways. This feature is based on our work Swapping Autoencoder (NeurIPS 2020). Web | Video NVIDIA Canvas: Turn Simple Brushstrokes into Realistic Images Download Windows 10 app based on our work SPADE (CVPR 2019) and GauGAN demo (SIGGRAPH 2019). Web | Video Photoshop Neural Filters Photoshop 2021 introduces \"Neural Filters\". Several features are partly built on our work iGAN (ECCV 2016), ideepcolor (SIGGRAPH 2017), and CycleGAN (ICCV 2017). Web | Video Selected Publications See the full list on Google Scholar Learning an Image Editing Model without Image Editing Pairs Nupur Kumari, Sheng-Yu Wang, Sheng-Yu Wang, Nanxuan Zhao, Yotam Nitzan, Yuheng Li, Krishna Kumar Singh, Richard Zhang, Eli Shechtman, Jun-Yan Zhu, Xun Huang arXiv 2025 Project | Paper Generating Physically Stable and Buildable Brick Structures from Text Ava Pun*, Kangle Deng*, Ruixuan Liu*, Deva Ramanan, Changliu Liu, Jun-Yan Zhu ICCV 2025 (Best Paper Award, Marr Prize) Code | Project | Demo | Paper Generating Multi-Image Synthetic Data for Text-to-Image Customization Nupur Kumari, Xi Yin, Jun-Yan Zhu, Ishan Misra, Samaneh Azadi ICCV 2025 Project | Code | Paper Efficient Autoregressive Shape Generation via Octree-Based Adaptive Tokenization Kangle Deng, Hsueh-Ti Derek Liu, Yiheng Zhu, Xiaoxia Sun, Chong Shang, Kiran Bhat, Deva Ramanan, Jun-Yan Zhu, Maneesh Agrawala, Tinghui Zhou ICCV 2025 Project | Paper Expressive Text-to-Image Generation with Rich Text Songwei Ge, Taesung Park, Jun-Yan Zhu, Jia-Bin Huang ICCV 2023 | IJCV 2025 Code | Project | Paper | Demo Generative Photomontage Sean J. Liu, Nupur Kumari, Ariel Shamir, Jun-Yan Zhu CVPR 2025 Project | Code | Paper Multi-subject Open-set Personalization in Video Generation Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, Sergey Tulyakov CVPR 2025 Project | Code | Paper SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models Muyang Li*, Yujun Lin*, Zhekai Zhang*, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, Song Han ICLR 2025 Code | Project | Paper | Demo One-Step Image Translation with Text-to-Image Models Gaurav Parmar, Taesung Park, Srinivasa Narasimhan, Jun-Yan Zhu arXiv 2024 Code | Paper | Demo Tactile DreamFusion: Exploiting Tactile Sensing for 3D Generation Ruihan Gao, Kangle Deng, Gengshan Yang, Wenzhen Yuan, Jun-Yan Zhu NeurIPS 2024 Code | Project | Paper | Data Data Attribution for Text-to-Image Models by Unlearning Synthesized Images Sheng-Yu Wang, Aaron Hertzmann, Alexei A. Efros, Jun-Yan Zhu, Richard Zhang NeurIPS 2024 Project | Code | Paper | BibTex Customizing Text-to-Image Diffusion with Camera Viewpoint Control Nupur Kumari*, Grace Su*, Richard Zhang, Taesung Park, Eli Shechtman, Jun-Yan Zhu SIGGRAPH Asia 2024 Code | Project | Paper Customizing Text-to-Image Models with a Single Image Pair Maxwell Jones, Nupur Kumari, Sheng-Yu Wang, David Bau, Jun-Yan Zhu SIGGRAPH Asia 2024 Code | Project | Paper Consolidating Attention Features for Multi-view Image Editing Or Patashnik, Rinon Gal, Daniel Cohen-Or, Jun-Yan Zhu, Fernando De la Torre SIGGRAPH Asia 2024 Project | Paper Distilling Diffusion Models into Conditional GANs Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Jaesik Park, Suha Kwak, Eli Shechtman, Jun-Yan Zhu, Taesung Park ECCV 2024 Project | E-LatentLPIPS code | Paper | BibTex FlashTex: Fast Relightable Mesh Texturing with LightControlNet Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Ramanan, Jun-Yan Zhu, Tinghui Zhou, Maneesh Agrawala ECCV 2024 Project | Paper CoFRIDA: Self-Supervised Fine-Tuning for Human-Robot Co-Painting Peter Schaldenbrand, Gaurav Parmar, Jun-Yan Zhu, James McCann, Jean Oh ICRA 2024 (Best Paper on Human-Robot Interaction) ICRA EXPO 2024 Best Demo Award Finalist Code | Paper | Project On the Content Bias in Fr√©chet Video Distance Songwei Ge, Aniruddha Mahapatra, Gaurav Parmar, Jun-Yan Zhu, Jia-Bin Huang CVPR 2024 Download: pip install cd-fvd Code | Project | Paper Content-Based Search for Deep Generative Models Daohan Lu*, Sheng-Yu Wang*, Nupur Kumari*, Roha",
  "content_length": 25423,
  "method": "requests",
  "crawl_time": "2025-12-01 13:38:02"
}