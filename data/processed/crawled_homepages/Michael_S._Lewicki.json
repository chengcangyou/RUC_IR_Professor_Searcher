{
  "name": "Michael S. Lewicki",
  "homepage": "http://www.cs.cmu.edu/~lewicki",
  "status": "success",
  "content": "Michael S. Lewicki Michael S. Lewicki Associate Professor Computer Science Department and Center for the Neural Basis of Cognition Carnegie Mellon University Here is my cv: pdf. Please go to my lab web page for up-to-date research and publication information. News I am moving. In the academic year of 2008-2009, I will be in residence at the Institute for Advanced Study (Wissenschaftskolleg) in Berlin, Germany. In August 2009, I will be moving to the Electrical Engineering and Computer Science Department at Case Western Reserve University. Contract information. Please contact me for my physical mail adress if you need to send me something. My email is michael.lewicki at case.edu. Teaching Spring 2008: Computational Perception (15-485/785) Spring 2007: Artificial Intelligence (15-381) Spring 2006: Computational Perception and Scene Analysis (15-485/785) Spring 2005: Graduate AI (15-780/16-731) Research Interests I am interested the computational principles underlying the ability of the brain to represent and process complex, real-world patterns. To understand how we perform such tasks as speech or object recognition, we must not only tackle the question of how to code sensory information, but also how this code is processed to represent more abstract properties of the stimulus.  For example, what are the basic computations underlying the formation of perceptual invariances such as the ability to recognize words independent of speaker or objects independent of orientation?  Research in my lab works toward these goals by developing and applying principles of information representation and processing. Computational Principles of Sensory Coding Part of my work focuses on the issue of sensory coding: How should natural images or sounds be encoded?  One approach that my research has developed is the application of probabilistic models to learn codes that are efficient in an information theoretic sense. Given a particular sensory environment, this top-down computational approach makes predictions about the properties of neural codes at the population level.  When this approach is applied to natural images, the resulting representation very closely matches the receptive field properties of visual cortex cells.  This framework also demonstrates that the model system encodes natural images more efficiently than many common Fourier or wavelet-based coding schemes.  Because the theory based on general probabilistic models of a high-dimensional data space, it can be applied to a variety of sensory patterns including static and dynamic visual images as well as patterns from different acoustic environments.  A recent extention of this theoretical framework to the temporal domain led to a theory for how continuous time-varying signals can be optimally represented by a population of spiking neurons. Higher-Order Representation and Inference A second aim of my work is to develop algorithms for learning hierarchical representations.  These are important because they can extract successively more abstract properties of the sensory patterns and may provide insight into how the brain carries out more complex aspects of perception that subserve object recognition.  One result of this work has been a computational explanation for the role of feedback, which is ubiquitous in the brain, but poorly understood. The long term goal of this research is to develop abstract neural architectures and learning algorithms that help elucidate the details of the higher-level processes that allow the brain to perceive and process natural stimuli under a wide range of conditions.  These include the computational principles underlying the representation and learning of perceptual invariances and attentional processes such as auditory and visual scene segmentation. Signal Processing As we better understand ways in which to represent and process sensory information, we can also develop better algorithms for signal processing.  For example, the application efficient coding framework to natural images yields a code that is demonstrably more efficient than many standard methods of compression such as wavelets or JPEG.  Furthermore, because this framework is based on probability theory, it leads naturally to algorithms for optimal inference in the face of uncertainty such as denoising and filling-in of missing information.  Further extensions have led to novel algorithms for texture recognition and segmentation. Efficient Algorithms for Processing Natural Signals Another important aspect of my research is the development of computationally efficient algorithms.  Many algorithms are attractive from the viewpoint of theory but are of sufficient complexity that they can only be applied to small toy problems, which may not provide a valid test of the underlying computational theory.  Developing efficient implementations of these algorithms allows them to be tested on more realistic problems and also affords the possibility of developing practical applications. Publications See the lab publication page for an up-to-date list.",
  "content_length": 5066,
  "method": "requests",
  "crawl_time": "2025-12-01 13:58:52"
}