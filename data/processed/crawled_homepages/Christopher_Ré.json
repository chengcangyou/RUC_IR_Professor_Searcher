{
  "name": "Christopher Ré",
  "homepage": "https://cs.stanford.edu/people/chrismre",
  "status": "success",
  "content": "Homepage of Christopher Re (Chris Re) News! Students Papers Teaching and Awards Christopher RÃ© Email: chrismre at cs.stanford.edu Our Lab's Github | Blog | Twitter Department of Computer Science Stanford University 353 Jane Stanford Way Stanford, CA 94305-9025 I'm a professor in the Stanford AI Lab (SAIL), the center for research on foundation models (CRFM), and the Machine Learning Group (bio). Our lab works on the foundations of the next generation of AI systems. On the AI side, I am fascinated by how we can learn from increasingly weak forms of supervision, the basis of new architectures, the role of data, and by the mathematical foundations of such techniques. On the systems side, I am broadly interested in how machine learning is changing how we build software and hardware. I'm particularly excited when we can blend AI and systems, e.g,. Snorkel, Overton (YouTube), or Together. Our work is inspired by the observation that data is central to these systems, and so data management principles (re-imagined) play a starring role in our work. This sounds like Silicon Valley nonsense, but oddly enough, these ideas get used due to amazing students and collaborations with Google ads, YouTube, Apple, and more. While we're very proud of our research ideas and their impact, the lab's real goal is to help students become professors, entrepreneurs, and researchers. To that end, over a dozen members of our group have started their own professorships. With students and collaborators, I've been fortunate enough to cofound a number of companies and a venture firm. For transparency, I try to list companies I advise or invest in here and our research sponsors here. My students run the ML Sys Podcast. We're interested in improving the foundations of foundation models. We released ThunderKittens (quick blog|paper|repo) for our opinionated take on building AI kernels. Now with ParallelKittens (paper) for multi-GPU and HipKittens (paper) for AMD. Intelligence per Watt (paper|repo) measuring efficiency of AI and foundation models! Blog post on sequence length and more. See the blog for more details Flash Attention is an IO-Aware algorithm for attention. This is widely used now including in ML Perf, see MLPerf Story on Tri!. Tri's Version 2 We continue to work on long sequences. An explainer of a simplified version of S4 (S4 Explainer Blog). It's a convolution and an RNN based on simple ideas from signal processing. SOTA on long range arena and first to solve Path-X. update on this line of work. We've been working on Hyena using ideas from signal processing, and its application to HyenaDNA and now Evo led by Arc Institute--and Brian Hie. Evo selected for the cover of Science. Now Evo 2 extends to all domains of life. Some Talks and resources Neurips23 Keynote (pptx|pdf|video) about building blocks for foundation models. GitHub for SysAI building blocks. Some resources for a budding community in Data-Centric AI and a blog post about it. SIGMOD keynote on Data-centric AI, Declarative ML, and Foundation Models in data slides (YouTube) SIGMOD panel on Service, Science and Startups changing research Software 2.0 Overview at HAI Thanks, NeurIPS! Our Test-of-time Award talk for Hogwild! is on YouTube A quick overview of video our work on Hidden Stratification. MLSys 20 keynote talk (pdf|pptx) or WWW BIG. More articles on new group website also see github. A messy, incomplete log of old updates is here. I'm a professor in the Stanford AI Lab (SAIL), the center for research on foundation models (CRFM), and the Machine Learning Group (bio). Our lab works on the foundations of the next generation of AI systems. On the AI side, I am fascinated by how we can learn from increasingly weak forms of supervision, the basis of new architectures, the role of data, and by the mathematical foundations of such techniques. On the systems side, I am broadly interested in how machine learning is changing how we build software and hardware. I'm particularly excited when we can blend AI and systems, e.g,. Snorkel, Overton (YouTube), or Together. Our work is inspired by the observation that data is central to these systems, and so data management principles (re-imagined) play a starring role in our work. This sounds like Silicon Valley nonsense, but oddly enough, these ideas get used due to amazing students and collaborations with Google ads, YouTube, Apple, and more. While we're very proud of our research ideas and their impact, the lab's real goal is to help students become professors, entrepreneurs, and researchers. To that end, over a dozen members of our group have started their own professorships. With students and collaborators, I've been fortunate enough to cofound a number of companies and a venture firm. For transparency, I try to list companies I advise or invest in here and our research sponsors here. My students run the ML Sys Podcast. We're interested in improving the foundations of foundation models. We released ThunderKittens (quick blog|paper|repo) for our opinionated take on building AI kernels. Now with ParallelKittens (paper) for multi-GPU and HipKittens (paper) for AMD. Intelligence per Watt (paper|repo) measuring efficiency of AI and foundation models! Blog post on sequence length and more. See the blog for more details Flash Attention is an IO-Aware algorithm for attention. This is widely used now including in ML Perf, see MLPerf Story on Tri!. Tri's Version 2 We continue to work on long sequences. An explainer of a simplified version of S4 (S4 Explainer Blog). It's a convolution and an RNN based on simple ideas from signal processing. SOTA on long range arena and first to solve Path-X. update on this line of work. We've been working on Hyena using ideas from signal processing, and its application to HyenaDNA and now Evo led by Arc Institute--and Brian Hie. Evo selected for the cover of Science. Now Evo 2 extends to all domains of life. Some Talks and resources Neurips23 Keynote (pptx|pdf|video) about building blocks for foundation models. GitHub for SysAI building blocks. Some resources for a budding community in Data-Centric AI and a blog post about it. SIGMOD keynote on Data-centric AI, Declarative ML, and Foundation Models in data slides (YouTube) SIGMOD panel on Service, Science and Startups changing research Software 2.0 Overview at HAI Thanks, NeurIPS! Our Test-of-time Award talk for Hogwild! is on YouTube A quick overview of video our work on Hidden Stratification. MLSys 20 keynote talk (pdf|pptx) or WWW BIG. More articles on new group website also see github. A messy, incomplete log of old updates is here. This is AI generated, so there are probably fewer errors than when I did it... and something to blame. Manuscripts Index by year 2025 Minions: Cost-efficient collaboration between on-device and cloud language models Narayan, Biderman, et al. ICML 2025. Genome modeling and design across all domains of life with Evo 2 Brixi, Durrant, et al. BioRxiv 2025. Kernelbench: Can llms write efficient gpu kernels? Ouyang, Guo, et al. 2025. Codemonkeys: Scaling test-time compute for software engineering Ehrlich, Brown, et al. 2025. Restructuring Vector Quantization with the Rotation Trick Fifty, Junkins, et al. ICLR 2025. Oral Scaling laws for precision Kumar, Ankner, et al. ICLR 2025. LoLCATs: On low-rank linearizing of large language models Zhang, Arora, et al. ICLR 2025. Aioli: A unified optimization framework for language model data mixing Chen, Hu, et al. ICLR 2025. Archon: An architecture search framework for inference-time techniques Saad-Falcon, Lafuente, et al. ICML 2025. HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation Kumbong, Liu, et al. CVPR 2025. BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs Liu, Baig, et al. 2025. Shrinking the Generation-Verification Gap with Weak Verifiers Saad-Falcon, Buchanan, et al. 2025. Cartridges: Lightweight and general-purpose long context representations via self-study Eyuboglu, Ehrlich, et al. 2025. Towards learning high-precision least squares algorithms with sequence models Liu, Grogan, et al. 2025. Systems and algorithms for convolutional multi-hybrid language models at scale Ku, Nguyen, et al. 2025. ParallelKittens: Systematic and Practical Simplification of Multi-GPU AI Kernels Sul, Arora, et al. 2025. Intelligence per Watt: Measuring Intelligence Efficiency of Local AI Saad-Falcon, Narayan, et al. 2025. HipKittens: Fast and Furious AMD Kernels Hu, Wadsworth, et al. 2025. A Unifying Framework for Parallelizing Sequential Models with Linear Dynamical Systems Gonzalez, Buchanan, et al. 2025. Building GenAI Benchmarks: A Case Study in Legal Applications Guha, Nyarko, et al. 2025. Precise high-dimensional asymptotics for quantifying heterogeneous transfers Yang, Zhang, et al. Journal of Machine Learning Research 2025. 2024 Sequence modeling and design from molecular to genome scale with Evo Nguyen, Poli, et al. Science 2024. Selected for Cover WONDERBREAD: A Benchmark for Evaluating Multimodal Foundation Models on Business Process Management Tasks Wornow, Narayan, et al. NeurIPS24 (Benchmark) Smoothie: Label Free Language Model Routing Guha, Chen, et al. NeurIPS24. Red Pajama Data NeurIPS24 (data) Large language monkeys: Scaling inference compute with repeated sampling Brown, Juravsky, et al. 2024. Just read twice: closing the recall gap for recurrent language models Arora, Timalsina, et al. 2024. Thunderkittens: Simple, fast, and adorable ai kernels Spector, Arora, et al. 2024. Automated Rewards via LLM-Generated Progress Functions Sarukkai, Shacklett, et al. 2024. Cookbook: A framework for improving LLM generative abilities via programmatic data generating templates Narayan, Chen, et al. NeurIPS 2024 (Datasets and Benchmarks). Context clues: Evaluating long context models for clinical prediction tasks on ehrs Wornow, Bedi, et al. 2024. Model changelists: Characterizing updates to ml models Eyuboglu, Goel, et al. FAccT 2024. Hydrage",
  "content_length": 55103,
  "method": "requests",
  "crawl_time": "2025-12-01 12:53:30"
}