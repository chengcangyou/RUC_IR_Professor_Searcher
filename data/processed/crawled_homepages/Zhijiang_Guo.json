{
  "name": "Zhijiang Guo",
  "homepage": "https://cartus.github.io",
  "status": "success",
  "content": "Zhijiang Guo White Cliffs of Dover, England Hi! My name is Zhijiang Guo (郭志江). I am an Assistant Professor at the DSA Thrust, HKUST (GZ). I am also an Affiliated Assistant Professor of HKUST. I was a Senior Researcher at Huawei Noah’s Ark Lab. Before that, I was a Postdoc at the Department of Computer Science and Technology at the University of Cambridge, working with Prof. Andreas Vlachos. I am also a member of Trinity College. I earned my PhD in Computer Science from SUTD in 2020, under the supervision of Prof. Wei Lu. I was a visiting student at the University of Edinburgh from 2019-2020, working with Prof. Shay Cohen and Prof. Giorgio Satta on Structured Prediction. I also gained valuable insights from Prof. Zhiyang Teng. Before that, I was an undergraduate student at Sun Yat-sen University. I actively seek strong and motivated students to join our group! Feel free to email me if you are interested. More details can be found in Prospective Students and Visitors/中文版. I am interested in natural language processing and machine learning, with a particular focus on large language models (LLMs). My research explores fundamental research questions about the knowledge and reasoning of LLMs, examining how these systems understand and process information. news Sep 19, 2025 Five papers are accepted at NeurIPS 2025 (1 Spotlight) Aug 22, 2025 Five papers are accepted at EMNLP 2025 (3 Main+2 Findings) Aug 20, 2025 Glad to serve as the Area Chair (AC) for ICLR 2026 May 01, 2025 Two papers are accepted at ICML 2025 (1 Spotlight) Apr 09, 2025 Glad to serve as the Area Chair (AC) for NeurIPS 2025 selected publications ArXiv Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR Xiao Liang*, Zhong-Zhi Li*, Yeyun Gong†, Yelong Shen, Ying Nian Wu, Zhijiang Guo†, and Weizhu Chen† In ArXiv, 2025 Abs HTML Code Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a key paradigm for post-training Large Language Models (LLMs), particularly for complex reasoning tasks. However, vanilla RLVR training has been shown to improve Pass@1 performance at the expense of policy entropy, leading to reduced generation diversity and limiting the Pass@k performance, which typically represents the upper bound of LLM reasoning capability. In this paper, we systematically analyze the policy’s generation diversity from the perspective of training problems and find that augmenting and updating training problems helps mitigate entropy collapse during training. Based on these observations, we propose an online Self-play with Variational problem Synthesis (SvS) strategy for RLVR training, which uses the policy’s correct solutions to synthesize variational problems while ensuring their reference answers remain identical to the originals. This self-improving strategy effectively maintains policy entropy during training and substantially improves Pass@k compared with standard RLVR, sustaining prolonged improvements and achieving absolute gains of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model sizes from 3B to 32B consistently demonstrate the generalizability and robustness of SvS. ArXiv Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Dongchun Xie, Yiwei Wang, Xiaodan Liang, and Jing Tang In ArXiv, 2025 Abs HTML Code Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO’s mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR. ArXiv TL; DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression Zhong-Zhi Li*, Xiao Liang*, Zihao Tang, Lei Ji, Peijie Wang, Haotian Xu, Haizhen Huang, Weiwei Deng, Ying Nian Wu, Yeyun Gong, Zhijiang Guo†, Xiao Liu†, Fei Yin, and Cheng-Lin Liu In ArXiv, 2025 Abs HTML Code Large Language Models (LLMs) have recently achieved remarkable progress by leveraging Reinforcement Learning and extended Chain-of-Thought (CoT) techniques. However, the challenge of performing efficient language reasoning–especially during inference with extremely long outputs–has drawn increasing attention from the research community. In this work, we propose a dynamic ratio-based training pipeline that does not rely on sophisticated data annotations or interpolation between multiple models. We continuously balance the weights between the model’s System-1 and System-2 data to eliminate redundant reasoning processes while preserving the model’s reasoning capability. We validate our approach across models on DeepSeek-R1-Distill-7B and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying difficulty levels. Our method significantly reduces the number of output tokens by nearly 40% while maintaining the accuracy of the reasoning. Our code and data will be available soon. TPAMI From System 1 to System 2: A Survey of Reasoning Large Language Models Zhong-Zhi Li*, Duzhen Zhang*, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhijiang Guo†, Le Song†, and Cheng-Lin Liu† In TPAMI, 2025 Abs HTML Code Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI’s o1/o3 and DeepSeek’s R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \\hrefthis https URLGitHub Repository to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field. ICML Aligning with Logic: Measuring, Evaluating and Improving Logical Consistency in Large Language Models Yinhong Liu, Zhijiang Guo, Tianya Liang, Ehsan Shareghi, Ivan Vuli’c, and Nigel Collier In ICML (Spotlight), 2025 Abs HTML Recent research in Large Language Models (LLMs) has shown promising progress related to LLM alignment with human preferences. LLM-empowered decision-making systems are expected to be predictable, reliable and trustworthy, which implies being free from paradoxes or contradictions that could undermine their credibility and validity. However, LLMs still exhibit inconsistent and biased behaviour when making decisions or judgements. In this work, we focus on studying logical consistency of LLMs as a prerequisite for more reliable and trustworthy systems. Logical consistency ensures that decisions are based on a stable and coherent understanding of the problem, reducing the risk of erratic or contradictory outputs. We first propose a universal framework to quantify the logical consistency via three fundamental proxies: transitivity, commutativity and negation invariance. We then evaluate logical consistency, using the defined measures, of a wide range of LLMs, demonstrating that it can serve as a strong proxy for overall robustness. Additionally, we introduce a data refinement and augmentation technique that enhances the logical consistency of LLMs without sacrificing alignment to human preferences. It augments noisy and sparse pairwise-comparison annotations by estimating a partially or totally ordered preference rankings usin",
  "content_length": 23101,
  "method": "requests",
  "crawl_time": "2025-12-01 14:55:31"
}