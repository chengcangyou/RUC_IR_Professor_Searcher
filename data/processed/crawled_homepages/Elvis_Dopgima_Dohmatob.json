{
  "name": "Elvis Dopgima Dohmatob",
  "homepage": "https://www.concordia.ca/faculty/elvis-dohmatob.html",
  "status": "success",
  "content": "Elvis Dohmatob - Concordia University Elvis Dohmatob, PhD Associate Professor, Computer Science and Software Engineering Are you the profile owner? Sign in to edit Research areas: artificial intelligence, machine learning, learning theory, algorithms, neural networks, neural scaling laws, adversarial robustness, algorithmic bias, optimization, determinantal point processes Contact information Email: elvis.dohmatob@concordia.ca Availability: I'm always looking for passionate graduate students. If you are thinking about a doing PhD in fundamental aspects of ML (theoretical and algorithmic), and you genuinely feel that your profile matches the kind of research I do (see my papers), don't hesitate to drop me an email. Note that I will not reply to random supervision requests! BiographyI am a researcher working on various topics in artificial intelligence (AI) and machine learning (ML), with a theoretical flavor. I joined Concordia as a professor in 2024. I'm also an affiliate faculty at the Mila Institute. Prior to Concordia, I held positions at INRIA (Paris, France), Criteo (Paris, France), and FAIR/Meta (Paris, France). My current research agenda focuses around the following themes: Learning Theory (neural scaling laws, robustness, model collapse, etc.), Neural Networks (attention, associative memories, etc.), Representation Learning, Trustworthy AI/ML (algorithmic bias, adversarial examples, etc.), Optimization Publications2025 Elvis Dohmatob, Mohammad Pezeshki, Reyhane Askari-Hemmat \"Why Less is More (Sometimes): A Theory of Data Curation\", ArXiv Preprint, 2025 Elvis Dohmatob, \"Understanding Softmax Attention Layers: Exact Mean-Field Analysis on a Toy Problem\" (to appear), NeurIPS, 2025 Reyhane Askari-Hemmat, Mohammad Pezeshki, Elvis Dohmatob, Florian Bordes, Pietro Astolfi, Melissa Hall, Jakob Verbeek, Michal Drozdzal, Adriana Romero-Sorianoe \"Improving the Scaling Laws of Synthetic Data with Deliberate Practice\", ICML, 2025 A. Subramonian, S. Bell, L. Sagun, E. Dohmatob, \"An Effective Theory of Bias Amplification\", ICLR, 2025 Y. Feng, E. Dohmatob, P. Yang, F. Charton, J. Kempe, \"Beyond Model Collapse: Scaling Up with Synthesized Data Requires Reinforcement\", ICLR, 2025 R. Bayat, M. Pezeshki, E. Dohmatob, D. Lopez-Paz, P. Vincent, \"The Pitfalls of Memorization: When Memorization Hurts Generalization\", ICLR, 2025 E. Dohmatob, Y. Feng, A. Subramonian, J. Kempe \"Strong Model Collapse\", ICLR, 20252024 E. Dohmatob, Y. Feng, P. Yang, F. Charton, J. Kempe, \"A Tale of Tails: Model Collapse as a Change of Scaling Laws\", ICML, 2024 E. Dohmatob, Y. Feng, J. Kempe, \"Model Collapse Demystified: The Case of Regression\", NeurIPS, 2024 V. Cabannes, E. Dohmatob, A. Bietti, \"Scaling Laws for Associative Memories\", ICLR, 2024 E. Dohmatob, M. Scetbon, \"Precise Accuracy / Robustness Tradeoffs in Regression: Case of General Norms\", ICML, 2024 E. Dohmatob, \"Consistent Adversarially Robust Linear Classification: Non-Parametric Setting\", ICML, 2024Complete List DBLP Google Scholar Are you the profile owner? Sign in to edit Took 32 milliseconds Back to top © Concordia University",
  "content_length": 3110,
  "method": "requests",
  "crawl_time": "2025-12-01 13:06:11"
}