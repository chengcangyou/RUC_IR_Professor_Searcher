{
  "name": "Rishabh K. Iyer",
  "homepage": "https://sites.google.com/view/rishabhiyer",
  "status": "success",
  "content": "Rishabh IyerSearch this siteEmbedded FilesSkip to main contentSkip to navigationWelcome to Rishabh Iyer's webpageAssistant Professor, University of Texas DallasI am currently an Assistant Professor at the University of Texas, Dallas, where I lead the CARAML Lab. I'm also a Visiting Assistant Professor at the Indian Institute of Technology, Bombay. Before this, I was a Senior Research Scientist at Microsoft from 2016 - 2019.  Below are some of the research topics my group is currently actively working on:Combinatorial Representation Learning: A new family of Combinatorially inspired loss functions for representation learning, self-supervised learning, and few shot learning problems: SCORE (ICML 2024), SMILE (ECCV 2024), CROWD (NeurIPS 2025), SHaSaM (WACV 2026).Compute-Efficient Learning via Data Subset Selection and Coresets: We developed approaches which have obtained 5x - 10x speedups/energy efficiency with small data subsets with negligible loss in accuracy (generalization performance) applied to varied applications like supervised, semi-supervised, domain adaptation, hyper-parameter tuning, NAS, Federated Learning and Large Language Model Finetuning.  We have applied these methods on a number of domains like image classification, NLP, speech recognition - GLISTER (AAAI 2021), GRADMATCH (ICML 2021), RETRIEVE (NeurIPS 2021), SELCON (ICML 2021), ORIENT (NeurIPS 2022), AUTOMATA (NeurIPS 2022), SubSelNet (NeurIPS 2023), PGM (EMNLP 2022), INGENIOUS (EMNLP 2023), GCFL (WACV 2024)Data Efficient Learning: Learning with fewer labels, reducing labeling costs by 2x - 5x (see this blog page on some of our work in active learning), particularly in realistic scenarios like rare classes/slices, imbalance, OOD instances, redundancy, etc. We have come up with a framework, that we call, Guided Active Learning to enable navigating several data issues listed above. We have applied this on various problems including image classification, object detection, segmentation, text classification and NLP, and 3D Object Detection - FASS (ICML 2015), Learning with Less Data (WACV 2019), SIMILAR (NeurIPS 2021), TALISMAN (ECCV 2022), PRISM (AAAI 2022), CLINICAL (MICCAI 2022), DIAGNOSE (MICCAI 2022), DITTO (ACL 2023), CLARIFIER (WACV 2024), STONE (NeurIPS 2024). Robust Deep Learning in the presence of outliers, label noise, out-of-distribution data, and so on - WRSSL (ICDM 2022), NestedMAML (AAAI 2022), WISDOM (ACL 2022).Data Programming and Weak Supervision, Learning with Rules, Labeling Functions - SPEAR (ACL 2021), WISDOM (ACL 2022), RuleAugmentedCP (ACL 2021)Continuous & Meta/Few-Shot Learning: Coresets for Continuous Learning - GCR (CVPR 2022), Few Shot Object Detection - SMILE (ECCV 2024), Semi-Supervised Meta Learning - PLATINUM (ICML 2022)Discrete Optimization (specifically submodular optimization), Combinatorial (Submodular) Information Measures - See My PhD Thesis, The SMI Journal PaperOur research is currently supported by grants from NSF, Adobe Data Science Award, a Google Gift, Amazon Research Award, and the UT Dallas Seed grant. Thank you! Our research is motivated by real-world problems in machine learning, computer vision, text, and NLP! For more on my research, please see my research page, my publications, or my lab webpage.I completed my Ph.D. in 2015 from the University of Washington, Seattle where I worked with Jeff Bilmes. I am excited about making machines assist humans in processing massive amounts of data, particularly in understanding videos and images. I am interested in building intelligent systems which organize, analyze and summarize massive amounts of data, and also automatically learn from this. I received the best paper awards at Neural Information Processing Systems (NeurIPS/NIPS) in 2013, the International Conference of Machine Learning (ICML) in 2013, and an Honorable Mention at CODS-COMAD in 2021. I also received several research awards including an NSF Medium Grant, an Adobe Data Science Research Award, Microsoft Research Ph.D. Fellowship, Facebook Ph.D. Fellowship, and the Yang Award for Outstanding Graduate Student from the University of Washington.For more information, please see my Google Scholar Profile, LinkedIn Profile, DBLP, or my GitHub page. I also maintain a YouTube channel where I add videos of my lectures and research talks.Twitter: @rishiyerAwards and RecognitionAmazon Research Award on Fairness for AI, July 2022 (Main PI: Preethi Jyothi)Adobe Data Science Research Award, February 2022NSF Medium Grant, September 2021.Honorable Mention for our paper at CODS-COMAD 2021Outstanding Reviewer Award for NeurIPS 2020 and 2021!Finalist in the LDV Computer Vision Conference, New York in 2017Yang Outstanding Graduate Student Award, University of Washington, SeattleMicrosoft Research Fellowship Award, 2014Facebook Fellowship Award. 2014 (Declined in favor of Microsoft)Best Paper Award at the International Conference of Machine Learning, 2013Best Paper Award at the Neural Information Processing Systems Conference, 2013Work Experience and EducationSpring 2020 to Present, Assistant Professor at the CS Department, UT DallasAugust 2020 to Present, Visiting Assistant Professor at CSE Department, IIT BombayMarch 2016 - December 2019, Senior Research Scientist, MicrosoftMarch 2015 - March 2016, Post-Doctoral Researcher, University of WashingtonSeptember 2011 - March 2015, M.S and Ph.D., University of Washington, SeattleAugust 2011 - May 2011, B.Tech, IIT-BombayTeachingUniversity of Texas at DallasIntroduction to Machine Learning (CS 4374) (Undergraduate Course): Spring 2022, Spring 2024, Spring 2025Machine Learning (CS 6375): Fall 2020, Fall 2021, Fall 2022, Spring 2023, Fall 2023, Fall 2024 (Course Website, Youtube Playlist of Video Lectures)Advanced Optimization in Machine Learning (CS 7301) (Course Website, Youtube Playlist of Video Lectures): Spring 2021Optimization in Machine Learning (CS 6301) (Course Website): Spring 2020University of WashingtonSpring 2014: Teaching Assistant for Submodular Functions, Optimization, and Applications to Machine Learning (Course Link)Fall 2011: Introduction to Electrical EngineeringIndian Institute of Technology BombaySummer 2015: Instructor at the Mini-course on Submodular Optimization at the  Non-Convex Optimization in Machine Learning, IIT Bombay (Slides)Spring 2010: Teaching Assistant for Introduction to Probability and StatisticsTutorials/Workshops at ConferencesAAAI 2023 (Lab)AAAI 2022 (tutorial website)SubSetML-2021 at ICML 2021 (workshop website)IJCAI-PRICAI 2020 (tutorial website)ECAI 2020 (tutorial website)WACV 2019 (tutorial website) Recent newsCROWD (Open World Data Discovery) accepted at NeurIPS 2025, InSQUAD accepted at ICDM 2025, and SHaSaM accepted at WACV 2026!Serving as an Area Chair for ICLR 2025 and AAAI 2025. STONE (An Active Learning Framework for 3D Object Detection) is accepted at NeurIPS 2024!Invited Talk at the 3rd Imaging and Data Science Workshop (UTD/UTSW Joint Workshop) 2024SMILE (Submodular Mutual Information Based Loss Functions for Few Shot Object Detection) is accepted at ECCV 2024!SCORE (Submodular Combinatorial Representation Learning) is accepted at ICML 2024! We introduce a new family of loss functions for representations learning motivated by submodular functions! Served as a Panelist for NSF Medium Proposals, April 2024. Serving as an Area Chair for ICLR 2024 and AAAI 2024!CLARIFIER and GCFL accepted at WACV 2024!SubSelNet accepted at NeurIPS 2023!INGENIOUS (compute-efficient LLM pre-training) accepted at Findings of EMNLP 2023!DITTO (targeted subset selection for ASR Accent adaptation) is accepted at ACL 2023!PRESTO on Mixed Discrete Continuous Optimization for Mixture Modeling is accepted at ICML 2023!Joined the Editorial Board of Transactions of Pattern Analysis and Machine Intelligence (TPAMI) as an Action Editor!Invited talk at IndoML 2022 (video here) on Subset Selection for Human in the Loop ML.Two papers: ORIENT on data subset selection for distribution shift, and AUTOMATA on gradient-based subset selection for compute-efficient hyper-parameter tuning are accepted at NeurIPS 2022!Our work on Robust Semi-supervised Learning was accepted at ICDM 2022! Congrats Krishnateja and Xujiang!Excited to Recieve an Amazon Research Award in Alexa Fairness for AI track for our proposal on Fair Speech Recognition using Targeted Subset Selection and Active Semi-supervised Learning (joint with Preethi Jyothi and Ganesh Ramakrishnan from IIT Bombay).Our work TALISMAN, which is on Targeted Active Learning for Object Detection in Autonomous Driving with Rare Scenarios (e.g, Mining instances of \"Motorcycles at night\" or \"Pedestrians on a Highway in Foggy Scenarios\") accepted at ECCV 2022!Our work on Semi-supervised Meta-Learning (PLATINUM) was accepted to ICML 2022!Excited to receive the Adobe Data Science Research Award for February 2022! Thanks, Adobe for the support! Our work on gradient coresets for continuous learning was accepted at CVPR 2022 and our work on robust learning of labeling functions was accepted at Findings of ACL 2022! Congrats Krishnateja and all our coauthors!Invited Speaker at the \"Bi-level Optimization in Machine Learning\" and \"Submodular Optimization\" sessions at INFORMS Optimization Society (IOS) 2022!Two papers from CARAML lab are accepted at AAAI 2022! Congrats Krishnateja and Suraj!Our work on Submodular Information Measures was accepted to Transactions of Information Theory Journal.Outstanding Reviewer Award from NeurIPS 2021! (received the same award in 2020 as well). Excited to be giving a tutorial at AAAI 2022 on Subset Selection in Machine Learning: Theory, Applications, and Hands-on. Stay tuned for more updates!Three papers from CARAML lab are accepted at NeurIPS 2021! Congrats Krishna, Ping, Nathan, and Suraj!Received an NSF Collaborative Medium Award on Submodular Information Functions with Applications to Machine Learning. Thanks, NSF!Received gift funding from Adobe for Targeted Subset Selection! Thanks, Ado",
  "content_length": 22384,
  "method": "requests",
  "crawl_time": "2025-12-01 14:18:10"
}