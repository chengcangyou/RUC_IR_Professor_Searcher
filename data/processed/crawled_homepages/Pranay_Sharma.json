{
  "name": "Pranay Sharma",
  "homepage": "https://sites.google.com/view/pranay-sharma/home",
  "status": "success",
  "content": "Pranay SharmaSearch this siteEmbedded FilesSkip to main contentSkip to navigationAbout MeI am an Assistant Professor at IIT Bombay in the Centre for Machine Intelligence and Data Science. Till Jan'2025, I was a Research Scientist in the Dept. of Electrical and Computer Engineering, at Carnegie Mellon University, working with Prof. Gauri Joshi. In August 2021, I finished my PhD in Electrical Engineering and Computer Science at Syracuse University with Prof. Pramod K. Varshney. Before that, I finished my B.Tech-M.Tech dual-degree in Electrical Engineering from IIT Kanpur.For interested students:Looking for one PhD student to work on online learning and portfolio optimizationLooking for PhD and master’s students, and 1 or 2 full-time research assistants to work in the general areas of optimization and machine learning theoryEmail: pranaysh[AT]iitb[DOT]ac[DOT]inCV    Google Scholar Research InterestsFederated and Collaborative Learning, Stochastic Optimization, Deep Learning Theory, Reinforcement Learning, Differential PrivacyI'm always looking for collaborations. If you are a researcher with similar research interests, feel free to email me and we can set up a time to chat. Also, if you're a beginner in the fascinating, though often overwhelming, world of research and would like some friendly advice, do reach out. Recent NewsFeb 2025: I have joined CMInDS at IIT Bombay as an Assistant Professor. Excited about the opportunity to work with the amazing faculty and students here!Feb 2025: Check out our new paper.S. Jiang, P. Sharma, Z. S. Wu, G. Joshi, \"The Cost of Shuffling in Private Gradient Based Optimization.\"Jan 2025: One paper accepted in ICLR'25 (acceptance rate 32%) . Congrats to all the co-authors.Z. Sun, Z. Zhang, Z. Xu, G. Joshi, P. Sharma, and E. Wei, \"Debiasing Federated Learning with Correlated Client Participation.\"Jan 2025: Two papers accepted in AISTATS'25 (acceptance rate 31.3%). Congrats to all the co-authors.B. Askin, P. Sharma, G. Joshi, C. Joe-Wong, \"Federated Communication-Efficient Multi-Objective Optimization.\"A. Armacki, S. Yu, P. Sharma, G. Joshi, D. Bajovic, D. Jakovetic, and S. Kar, \"High-probability Convergence Bounds for Online Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise.\"I presented our recent work on federated multi-objective optimization at the following venues. Check out the slides here. Jan 2025: BIRS Workshop on \"Machine Learning and Statistics: From Theory to Practice\" held at CMI Chennai Nov 2024: Federated Learning One World (FLOW) Seminar. Thanks, Samuel Horvath and Eduard Gorbunov, for hosting me. Check out the talk video.Dec 2024: One paper accepted in ICASSP 2025. Congrats to all the co-authors.S. D. Sharma, P. Sharma, and K. Rajawat, \"On Decentralized Learning with Stochastic Subspace Descent.\"Oct-Nov 2024: I took three guest lectures in the course 18-667: Algorithms for Large-scale Distributed Machine Learning and Optimization offered this semester by my advisor Prof. Gauri Joshi. Check out my slides below:Multi-task LearningFederated Min-max OptimizationDifferential Privacy in Dist. OptimizationOct 2024: Check out three new papers.B. Askin, P. Sharma, G. Joshi, and C. Joe-Wong, \"Federated Communication-Efficient Multi-Objective Optimization.\"A. Armacki, S. Yu, P. Sharma, G. Joshi, D. Bajovic, D. Jakovetic, and S. Kar, \"Nonlinear Stochastic Gradient Descent and Heavy-tailed Noise: A Unified Framework and High-probability Guarantees.\"Z. Sun, Z. Zhang, Z. Xu, G. Joshi, P. Sharma, and E. Wei, \"Debiasing Federated Learning with Correlated Client Participation.\"July 2024: Along with Zheng Chen and Erik G. Larsson (from Linköping University), I am organizing a special session on Distributed optimization and learning with resource-constrained communication at ICASSP'25.Jun 2024: Attended the AIMACCS workshop organized by the NSF-AI Egde Institute at Ohio State University. Thanks to all the organizers!Apr 2024: One paper accepted in UAI 2024. Congrats to all the co-authors.B. Askin, P. Sharma, G. Joshi, and C. Joe-Wong, \"FedAST: Federated Asynchronous Simultaneous Training.\" (acceptance rate 27%)Dec 2023-April 2024: talks on Computation and Communication-Efficient Distributed LearningSchool of Artificial Intelligence (ScAI), Indian Institute of Technology DelhiEE, Indian Institute of Technology MadrasCentre for Machine Intelligence and Data Science (CMInDS), Indian Institute of Technology Bombay (see slides)ECE, Indian Institute of Science, BangaloreSchool of Technology and Computer Science (STCS), Tata Institute of Fundamental Research (TIFR), MumbaiIEOR, Indian Institute of Technology BombayDec 2023 II: Our paper on Random Reshuffling over Networks got accepted in ICASSP'24.Dec 2023 I: Attended NeurIPS'23 in New Orleans from Dec 11-16. Do check our papers and posters.Correlation Aware Sparsified Mean Estimation Using Random Projection. Check out the paper and poster.Model Sparsity Can Simplify Machine Unlearning. Check out the Spotlight paper and poster.Nov 2023: Invited talk at the Google FL seminar at Google Research, Mountain View. Thanks, Zheng Xu for the invite.Oct 2023 III: Presented our recent work on minimax optimization and cyclic federated learning at the Missouri S&T CS Department seminar. Thanks, Sid Nadendla for the invite. Check out the slides here.Oct 2023 II: Our paper on min-max optimization just got accepted (with minor revisions) in TMLR. Congrats to all the co-authors.P. Sharma, R. Panda, and G. Joshi, \"Federated Minimax Optimization with Client Heterogeneity.\"Oct 2023 I: Presented our recent work on minimax optimization at the AI-EDGE SPARKS seminar. Thanks for the invite. Check out the slides here.Sept 2023 II: Two papers accepted in NeurIPS 2023. Congratulations to all the co-authors.S. Jiang, P. Sharma, and G. Joshi, \"Correlation Aware Sparsified Mean Estimation Using Random Projection.\" Poster presentation.J. Jia, J. Liu, P. Ram, Y. Yao, G. Liu, Y. Liu, P. Sharma, and S. Liu, \"Model Sparsity Can Simplify Machine Unlearning.\" Spotlight presentation.Sept 2023 I: Attended the New Frontiers in Federated Learning Workshop at the Toyota Technological Institute in Chicago. Thanks to all the organizers!July 2023: Attended ICML'23 in Honolulu, Hawaii from July 24-27. Do check our paper and poster.June 2023: Presented our recent work on minimax optimization at the SIAM Conference on Optimization (OP23) in Seattle, in the session on Recent Advancements in Optimization Methods for Machine Learning. Thanks, Nicolas Loizou and Siqi Zhang for the invite!May 2023: One paper accepted in ICML 2023. Congrats to all the co-authors.Y-J. Cho, P. Sharma, G. Joshi, Z. Xu, S. Kale, T. Zhang, \"On the Convergence of Federated Averaging with Cyclic Client Participation.\" Short presentation (acceptance rate 27.9%).March-April 2023: Presented our recent work on minimax optimization and cyclic federated learning at the following places: Prof. Mingyi Hong's (ECE, UMN) research group, EE-IIT Bombay, CNI-IISc Bengaluru, EE IIT Madras, and EE IIT Kanpur. The talk at CNI was live-streamed on youtube and can be found here.Feb 2023: Check out two new papers.P. Sharma, R. Panda, and G. Joshi, \"Federated Minimax Optimization with Client Heterogeneity.\"Y-J. Cho, P. Sharma, G. Joshi, Z. Xu, S. Kale, T. Zhang, \"On the Convergence of Federated Averaging with Cyclic Client Participation.\"Jan 2023: One paper accepted in ICLR 2023.Y. Zhang, P. Sharma, P. Ram, M. Hong, K. R. Varshney, and S. Liu, \"What Is Missing in IRM Training and Evaluation? Challenges and Solutions.\" Poster presentation (acceptance rate: 31.8%).Nov 2022: Presented our work on Minimax Optimization at the 57th Annual Asilomar Conference on Signals, Systems, and Computers (held in Pacific Grove, CA from October 29th to November 1st). Check out the 15-min version of the talk here.Oct 2022: Presented our work on Minimax Optimization at the 2022 Informs Annual Meeting (held in Indianapolis from October 16th to 19th) in the session \"Learning and Inference for Designing Policies in Stochastic Systems,\" chaired by Prof. Weina Wang (CMU). Thanks for the invitation!May 2022 II: One paper accepted in UAI 2022.D. Jhunjhunwala, P. Sharma, A. Nagarkatti, and G. Joshi, \"FedVARP: Tackling the Variance Due to Partial Client Participation in Federated Learning.\" Poster presentation (acceptance rate: 32.3%).May 2022 I: Two papers accepted in ICML 2022.P. Sharma, R. Panda, G. Joshi, P. K. Varshney, \"Federated Minimax Optimization: Improved Convergence Analyses and Algorithms.\" Short presentation (acceptance rate: 19.8%). Check out the 5-min video uploaded for ICML here.S. Khodadadian, P. Sharma, G. Joshi, S. Maguluri, \"Federated Reinforcement Learning: Communication-Efficient Algorithms and Convergence Analysis.\" Long presentation (acceptance rate: 2.1%).Sept 2021: One paper accepted in NeurIPS 2021.P. Khanduri, P. Sharma, H. Yang, M. Hong, J. Liu, K. Rajawat, P. K. Varshney, ''STEM: A Stochastic Two-Sided Momentum Algorithm Achieving Near-Optimal Sample and Communication Complexities for Federated Learning,'' poster.Aug 2021: I joined the ECE Dept. at CMU as a postdoctoral researcher.July 2021: I successfully defended my PhD thesis on Distributed Tracking and Optimization. Thanks to my advisor Prof. Pramod K. Varshney and all the committee members.Google SitesReport abusePage detailsPage updated Google SitesReport abuse",
  "content_length": 9379,
  "method": "requests",
  "crawl_time": "2025-12-01 14:13:04"
}