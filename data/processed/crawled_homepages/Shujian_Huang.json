{
  "name": "Shujian Huang",
  "homepage": "http://nlp.nju.edu.cn/huangsj",
  "status": "success",
  "content": "Shu-Jian HUANGâs HomePage Shu-Jian Huang Ph.D. Professor, Ph. D. Advisor Natural Language Processing Research Group State Key Laboratory of Novel Software Technology School of Computer Science, Nanjing University [Correspondence] [Biography] [Recruiting] [Research Interests] [Academic Services] [Honors and Awards] [Courses] [Presentations] [Selected Publication] Correspondence Office: Room 902, Computer Science and Technology Building, Xianlin Campus of Nanjing University 163 Xianlin Avenue, Nanjing 210023, China E-mail: huangsj at nju dot edu dot cn Biography Currently, I am a professor and Ph. D. Advisor in School of Computer Science (formerly known as the Department of Computer Science and Technology) of Nanjing University, member of the State Key Laboratory of Novel Software Technology. I received my B.Sc. degree and Ph. D. in Computer Science in Jun.Â 2006 and Jun.Â 2012 from Nanjing University, respectively. I am a member of NLP Group since undergraduate in Sep.Â 2005, led by Prof.Â Jiajun Chen. During my Ph. D. study, I spent 11 months (from Oct.Â 2007 to Aug.Â 2008) as visiting student in NLC group, MSRA, where I worked with Long Jiang in Chinese Couplet project and in SMT team with Mu Li, Henry Li and Dongdong Zhang. I also spent 12 months as a visiting student in InterACT lab, LTI, CMU, working with Prof.Â Stephan Vogel. I was awarded the Excellent Young Scholar Research Project by Jiangsu Provincial Research Foundation in 2017, CCF-NLPCC Young Outstanding Scientist Award in 2020, CIPSC Hanwang Youth Innovation Award in 2022. Our alumni Hao Zhou, Zaixiang Zheng, Yu Bao (co-advised with Prof.Â Jiajun Chen) were awarded Best Ph. D. Thesis Awards. Our translation quality estimation research and systems won the first place in WMT2022 QE subtasks (word-level MQM (En-De), sentence-level MQM (En-De)) and all 3 tasks in En-De direction in WMT2023 QE (word-level MQM, sentence-level MQM, error span detection). Our paper is recognized as the Outstanding Paper in EMNLP 2024 (Congrats to Jiahuan and Yiqing!). Recruiting I am looking for highly motivated undergraduate students to work together on NLP problems. If you have no NLP background, please consider taking our course (Machine Translation and Natural Langauge Generation, every spring, for sophomores in Nanjing University) or joining our NLP summer camp first (every summer, mainly for freshman or sophomore). There are some talks about our research on bilibili.com. Please also take a look before you apply. We are also expecting post-doc researchers to work together on NLP research or applications. I am terribly sorry for not being able to reply all emails applying for a Ph.D.Â position (overwhelmed by the applications). My reply is usually fast. Please consider me as unavailable if no reply within 3~4 working days. Research Interests My research is supported by projects from National Natural Science Foundation of China (NSFC), National Key R&D Program of China and the Jiangsu Provincial Research Foundation for Basic Research. We also have wide collaborations with industrial labs in Baidu, Tencent, Alibaba, ByteDance, Huawei, ZTE, China Mobile, etc. My research interests lie in natural language processing (NLP), which automatically understand and generate natural language texts, with special focus on the language related ability of LLMs (including knowledge, reasoning, etc.) and the mutlingual aspect of LLMs (including langauge transfer, alignment, etc.). Weâre trying to exploring the potential of LLMs by reinforcement learning (RL). Compared with supervised training (such as instruction tuning), RL could have a better balance between exploration and exploitation, which may lead the LLMs to learn the internal mechanism of performing a task, rather than some âshort-cutsâ to the answers. Currently, we mainly focus on the following topics: Language/Intelligent Abilities of LLM. Weâre trying to understand the abilities of LLMs, including the abilities for language comprehension, in-context learning, knowledge learning, logic reasoning, long-cot reasoning, safety and alignment, as well as the ability of working as intelligent agents for a given task. It is interesting and also important to know how these abilities are obtained, how they are applied to specific tasks and how to improve them if needed. LLM and Multilingualism (Multilingual LLM). Weâre trying to understand and improve the relation of language abilities across different languages in LLMs. It is important to improve the language ability for Chinese and relatively low resource languages. And one idea way is to align different languages altogether, which may happen or be performed during the pretraining, continue pretraining, or post training phase. In the end, we aim at building LLMs that are equally effective for all languages. Machine Translation (MT). Weâre investigating various methods to improve the capacity of multilingualism, by designing novel MT architectures with the help of both task-specific models and LLMs, evaluating/estimating the quality of MT, bringing human into the learning loop, improving translation performance for low-resource language pairs or domains, etc. Language Generation. Along with MT, Weâre also modelling different natural language generation tasks, e.g., summarization, paraphrasing, text style transfer, text simplification, answering questions with natural text, dialog, etc. Certain controlling factors need to be considered for a specific generation task, which brings interesting challenges. Other topics for generation include the evaluation, factual consistency and explainability, etc. Interdisciplinary Application of NLP/NLG. Weâre actively applying the techniques of machine translation and natural language processing in interdisciplinary research. For example, to analyze ancient Chinese, to generate or translate ancient Chinese, to understand the language comprehension process of human brain, to modeling the structure of protein, etc. Academic Services Services in Academic Society Board Member of the Chinese Information Processing Society of China (CIPSC) (2022-now) Deputy Director of the Technical Committee on Machine Translation, of the Chinese Information Processing Society of China (CIPSC) (2019-now) Executive Board Member of the Youth Working Committee, of the Chinese Information Processing Society of China (CIPSC) (2017-2020) Services in Conferences and Events Program Committee (co-)Chair: Evaluation Chair for CWMT (2017, 2018); Program co-Chair for CCMT2019, NLPCC2022; Demonstration co-Chair for CCL2020 Senior Program Committee Member: IJCAI (2020, 2021), AAAI (2022, 2023, 2024, 2026) Senior Area (co-)Chair: EACL2024 Senior Action Editor: ACL Rolling Review Area (co-)Chair: NLPCC2016, CCL2018, CCL2021, ACL2021, EMNLP2022, EMNLP2023, EACL2022, ACL2023, CoLing2024, NAACL2024 Program Committee Member/Reviewer for Major conferences in NLP and AI: ACL, EMNLP, NAACL, AAAI, IJCAI, NeurIPS, ICLR, ICML Committee Member for international machine translation competitions: WMT2017, AI Challenger (2017,2018) International Journal Reviewer: TPAMI, TKDE, AI, TACL, TALSP, TALLIP, WWWJ, FCS, JCST, NEUNET, IPM, KNOSYS, FMRE, etc. Honors and Awards Excellent Young Scholar Research Project by Jiangsu Provincial Research Foundation (2017) CIPSC Outstanding Services Award (2019) CCMT Best English Paper Award (2019), CCMT Best Paper Award (2023) CCF-NLPCC Young Outstanding Scientist Award (2020) CIPSC Hanwang Youth Innovation Award (2022) JCST Outstanding Reviewer Award (2020-2022) EMNLP Outstanding Paper Award (2024) (Co-)advisor of Awarded Thesis CAAI Best Ph. D. Thesis Award (Hao Zhou (2019, co-advised with Prof.Â Jiajun Chen)) CIPSC Best Ph. D. Thesis Award (Zaixiang Zheng (2022, co-advised with Prof.Â Jiajun Chen)) Excellent Master Thesis in Jiangsu Province (Rongxiang Weng (2020, co-advised with Prof.Â Jiajun Chen), Peng Wu (2020, co-advised with Prof.Â Jianbing Zhang)) JSAI Best Ph. D. Thesis Award (Yu Bao (2022, co-advised with Prof.Â Jiajun Chen)) Excellent Master Thesis in the university/department (Rongxiang Weng (2020), Peng Wu (2020), Yanling Xiao (2022), Dongqi Wang (2022, 2023), Yiming Yan (2023)) Advisor of Excellent Undergraduate Thesis in Nanjing University (Renfei Dang (2025, 2nd price)) Courses Basic Programming. in Autumn semester, for undergraduate students. Advanced Programming. in Spring semester, for undergraduate students. Programming for Artificial Intelligence (intro). in Spring semester, for undergraduate students. Natural Language Processing (Machine Translation part). in Autumn semester, for graduate students. Machine Translation and Natural Language Generation (intro). in Spring semester, for both undergraduate and graduate students. Presentations (mostly in Chinese, hosted on bilibili.com) Research Talks (of our own research) Introduction to the Research of Multilingual LLMs (70 mins, video). Invited talk at BJTU. 2025-11. Knowledge Learning in Large Languae Models (80 mins, video). Invited talk at HIT-SZ. 2024-11. Multilingual LLM Research from an MT perspective (20 mins, video). Invited talk at CCMT-2024. 2024-11. Application of the Multilingual Ability of Large Language Models (50 mins, video). Invited talk at HIT-SZ (later at Huawei, CLLM2024), 2024-05. Exploring the Multilingual Ability of Large Language Models (60 mins, video). Invited talk at Bytedance. 2023-08. Analyzing Multilingual Machine Translation Ability of Large Language Models (30 min, video). Invited talk at HIT. 2023-04. Natural Language Generation with Latent Variables (75 mins, video). Invited Talk at FDU. 2020-06. Tutorials (of related work) Research and Challenges of Multilingual Large Language Models (50 mins, video, slides). Tutorial Talk at NLPCC2024. 2024-10. Research Development of Multilingualism in the Era of LLM (15 mins, video). Invited talk at the Frontier Research Overview session, CCL2024. 2024-07. Research Development of Machine translation and Large",
  "content_length": 34284,
  "method": "requests",
  "crawl_time": "2025-12-01 14:28:00"
}