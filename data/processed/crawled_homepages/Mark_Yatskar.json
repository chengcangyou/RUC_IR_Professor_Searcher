{
  "name": "Mark Yatskar",
  "homepage": "https://www.cis.upenn.edu/~myatskar",
  "status": "success",
  "content": "Mark Yatskar Contact University of Pennsylvania Levine 402 myatskar@cis.upenn.edu Research Statement Mark Yatskar I am an Assistant Professor at University of Pennsylvania in the department of Computer and Information Science. I did my PhD at University of Washington co-advised by Luke Zettlemoyer and Ali Farhadi. I was a Young Investigator at the Allen Institute for Artificial Intelligence for several years working with their computer vision team, Prior. My work is interdisciplinary, spanning Natural Language Processing, Computer Vision, and Fairness in Machine Learning. I received a Best Paper Award at EMNLP for work on gender bias amplification (Wired article). My research broadly explores how language can be used to structure visual perception. I work on machine learning approaches that enable tight coupling between how people express themselves in language and how machine behavior is specified. A central thread in my research is trying to understand how machine learning systems inherit human bias. My lab currently explores two main research themes around expanding the abilities artificial intelligence systems: Natural language as a scaffold for visual intelligence Natural language is an effective human tool for communicating important world knowledge. This knowledge can be extracted, and used to create explict priors for how visual recognition systems need to behave. Such systems can be more data-efficient, interpretable, and capture a wider range of human abilities. Recent Projects: Language Model Guided Bottlenecks Knowledge Enhanced Medical Imaging Synthetic Data from LLM Code Understanding the role of human biases in machine learning Machine learning systems depend on human specification through explict annotation, collected data, and model design. In all parts of this process, people may unknownlingly bias systems and cause them to be brittle. In such cases, systems may fail to generalize given distribution shift, or cause the model to make gender biased predictions when models are uncertain. It is important to characterise and control how human biases are transfered to machine learning systems. Recent Projects: Annotator Cognative Heuristics Ensembles for Reducing Dataset Bias Gender Bias Amplification whylab I supervise a collaboritive group of PhD and masters students that I do research with. I am always looking for talented, motivated students to work with. If you are an interested persepective PhD student, I encourage you to apply to University of Pennsylvania, I am looking for students every year (Please see info below). PhD Students Chaitanya Malaviya (w/ Dan Roth) Yue Yang (w/ Chris Callison-Burch) Artemis Panagopoulou (w/ Chris Callison-Burch) Yifei Li Allen Chang Alumni Yufei Wang (Masters to Pitt PhD) Mona Ghandi(Masters to OSU PhD) Daniel Kim (Undergraduate to UW PhD ) Lucy Yuewei Yuan (Undergraduate to Meta) Perspective PhD Students: info Every year, I am looking for at least one new PhD student. If you are interested, please apply here and mention me as a potential advisor. Because of the volume of requests, I cannot to respond to email from perspective PhD students. Penn Students Interested in Research: info I am always interested in working with current undergraduate, masters, or PhD students at Penn. If you have expierence with machine learning, natural language processing, or computer vision, and are interested in topics I commonly work on, please send me an email. Teaching CIS 5300: Computational Linguistics: SP 2021 , FA 2021 , FA 2022, FA 2023 FA 2024, FA 2025 CIS 7000: Language and Vision: FA 2020 CIS 6300: Efficient NLP: SP 2023, SP 2025 Publications Usually, the most up to date list of my publications is found on semantic scholar.",
  "content_length": 3732,
  "method": "requests",
  "crawl_time": "2025-12-01 13:53:45"
}