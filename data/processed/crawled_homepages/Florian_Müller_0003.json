{
  "name": "Florian Müller 0003",
  "homepage": "https://flomue.com",
  "status": "success",
  "content": "Florian Müller I am an Assistant Professor for Mobile Human-Computer Interaction at TU Darmstadt where I lead the Urban Interaction Lab. My research follows the vision of seamless mobile interaction in and with the physical world, where digital information, AI agents, and physical environments merge into a unified eXtended Reality. Towards this vision, I explore novel interaction techniques for mobile XR and collaboration with AI systems. My approach is informed by a deep understanding of body-centric, tangible and haptic interfaces, as well as strong technical expertise, aiming to make the interaction more efficient and accurate but also more enjoyable and fun. latest publications 2025 The Bitter Taste of Confidence: Exploring Audio-Visual Taste Modulation in Immersive Reality Pooria Ghavamian, Jan Henri Beyer, Sophie Orth, Mia Johanna Nona Zech, Florian Müller, and Andrii Matviienko In Proceedings of the 2025 ACM International Conference on Interactive Media Experiences, May 2025 PDF ACM Extended Reality (XR) technologies present innovative ways to augment sensory experiences, including taste perception. In this study, we investigated how augmented reality (AR) visual filters and synchronized audio cues affect gustation through a controlled experiment with 18 participants. Our findings revealed unexpected crossmodal interactions: while pink visual filter typically associated with sweetness reduced perceived bitterness in isolation, it paradoxically enhanced bitterness perception when combined with sweet-associated audio cue. Furthermore, we observed an inverse correlation between participant confidence levels and their perception of taste intensities across multiple dimensions, highlighting confidence as an overlooked factor in sensory experience design. These findings inform the design of nuanced multisensory experiences in immersive media, where subtle crossmodal interactions significantly influence user perception. Social MediARverse: Investigating Users’ Social Media Content Sharing and Consuming Intentions with Location-Based AR Linda Hirsch, Florian Mueller, Mari Kruse, Andreas Butz, and Robin Welsch Virtual Reality, Jul 2025 PDF Springer arxiv Augmented Reality (AR) is evolving to become the next frontier in social media, merging physical and virtual reality into a living metaverse, a Social MediARverse. With this transition, we must understand how different contexts—public, semi-public, and private—affect user engagement with AR content. We address this gap in current research by conducting an online survey with 110 participants, showcasing 36 AR videos, and polling them about the content’s fit and appropriateness. Specifically, we manipulated these three spaces, two forms of dynamism (dynamic vs. static), and two dimensionalities (2D vs. 3D). Our findings reveal that dynamic AR content is generally more favorably received than static content. Additionally, users find sharing and engaging with AR content in private settings more comfortable than in others. By this, the study offers valuable insights for designing and implementing future Social MediARverses and guides industry and academia on content visualization and contextual considerations. Don’t They Really Hear Us? A Design Space for Private Conversations in Social Virtual Reality Josephus Jasper Limbago, Robin Welsch, Florian Müller, and Mario Di Francesco IEEE Transactions on Visualization and Computer Graphics, Jul 2025 PDF IEEE Seamless transition between public dialogue and private talks is essential in everyday conversations. Social Virtual Reality (VR) has revolutionized interpersonal communication by creating a sense of closeness over distance through virtual avatars. However, existing social VR platforms are not successful in providing safety and supporting private conversations, thereby hindering self-disclosure and limiting the potential for meaningful experiences. We approach this problem by exploring the factors affecting private conversations in social VR applications, including the usability of different interaction methods and the awareness with respect to the virtual world. We conduct both expert interviews and a controlled experiment with a social VR prototype we realized. We then leverage the outcomes of the two studies to establish a design space that considers diverse dimensions (including privacy levels, social awareness, and modalities), laying the groundwork for more intuitive and meaningful experiences of private conversation in social VR. AR You on Track? Investigating Effects of Augmented Reality Anchoring on Dual-Task Performance While Walking Julian Rasch, Matthias Wilhalm, Florian Müller, and Francesco Chiossi In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems - CHI ’25, Jul 2025 PDF ACM arxiv With the increasing spread of AR head-mounted displays suitable for everyday use, interaction with information becomes ubiquitous, even while walking. However, this requires constant shifts of our attention between walking and interacting with virtual information to fulfill both tasks adequately. Accordingly, we as a community need a thorough understanding of the mutual influences of walking and interacting with digital information to design safe yet effective interactions. Thus, we systematically investigate the effects of different AR anchors (hand, head, torso) and task difficulties on user experience and performance. We engage participants (n=26) in a dual-task paradigm involving a visual working memory task while walking. We assess the impact of dual-tasking on both virtual and walking performance, and subjective evaluations of mental and physical load. Our results show that head-anchored AR content least affected walking while allowing for fast and accurate virtual task interaction, while hand-anchored content increased reaction times and workload. CreepyCoCreator? Investigating AI Representation Modes for 3D Object Co-Creation in Virtual Reality Julian Rasch, Julia Töws, Teresa Hirzle, Florian Müller, and Martin Schmitz In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems - CHI ’25, Jul 2025 PDF ACM arxiv Generative AI in Virtual Reality offers the potential for collaborative object-building, yet challenges remain in aligning AI contributions with user expectations. In particular, users often struggle to understand and collaborate with AI when its actions are not transparently represented. This paper thus explores the co-creative object-building process through a Wizard-of-Oz study, focusing on how AI can effectively convey its intent to users during object customization in Virtual Reality. Inspired by human-to-human collaboration, we focus on three representation modes: the presence of an embodied avatar, whether the AI’s contributions are visualized immediately or incrementally, and whether the areas modified are highlighted in advance. The findings provide insights into how these factors affect user perception and interaction with object-generating AI tools in Virtual Reality as well as satisfaction and ownership of the created objects. The results offer design implications for co-creative world-building systems, aiming to foster more effective and satisfying collaborations between humans and AI in Virtual Reality. A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments Julian Rasch, Florian Müller, and Francesco Chiossi Apr 2025 PDF arxiv Augmented Reality (AR) is transforming the way we interact with virtual information in the physical world. By overlaying digital content in real-world environments, AR enables new forms of immersive and engaging experiences. However, existing AR systems often struggle to effectively manage the many interactive possibilities that AR presents. This vision paper speculates on AI-driven approaches for adaptive AR content placement, dynamically adjusting to user movement and environmental changes. By leveraging machine learning methods, such a system would intelligently manage content distribution between AR projections integrated into the external environment and fixed static content, enabling seamless UI layout and potentially reducing users’ cognitive load. By exploring the possibilities of AI-driven dynamic AR content placement, we aim to envision new opportunities for innovation and improvement in various industries, from urban navigation and workplace productivity to immersive learning and beyond. This paper outlines a vision for the development of more intuitive, engaging, and effective AI-powered AR experiences. Through the Expert’s Eyes: Exploring Asynchronous Expert Perspectives and Gaze Visualizations in XR Clara Sayffaerth, Annika Köhler, Julian Rasch, Albrecht Schmidt, and Florian Müller Aug 2025 PDF arxiv Transferring knowledge across generations is fundamental to human civilization, yet the challenge of passing on complex practical skills persists. Methods without a physically present instructor, such as videos, often fail to explain complex manual tasks, where spatial and social factors are critical. Technologies such as eXtended Reality and Artificial Intelligence hold the potential to retain expert knowledge and facilitate the creation of tailored, contextualized, and asynchronous explanations regardless of time and place. In contrast to videos, the learner’s perspective can be different from the recorded perspective in XR. This paper investigates the impact of asynchronous first- and third-person perspectives and gaze visualizations on efficiency, feeling of embodiment, and connectedness during manual tasks. The empirical results of our study (N=36) show that the first-person perspective is better in quantitative measures and preferred by users. We identify best practices for presenting preserved knowledge and provide guidelines for designing future systems. From Pegs to Pixels: A Comparative Analysis of the Nine Hole Peg Test and a Digital Copy Drawing Test for Fine Motor Control Asses",
  "content_length": 16862,
  "method": "requests",
  "crawl_time": "2025-12-01 13:10:38"
}