{
  "name": "Peter Richtárik",
  "homepage": "https://richtarik.org",
  "status": "success",
  "content": "Peter Richtarik News Old News Papers Talks Video Talks Events Code Team Apply Bio Teaching Consulting November 19, 2025 KAUST Workshop on Distributed Training in the Era of Large Models Together with Kaja Gruntkowska, Laurent Condat and Egor Shulgin, I am organizing KAUST Workshop on Distributed Training in the Era of Large Models, to be held at KAUST, Saudi Arabia, during November 24-26, 2025. November 18, 2025 New Paper New paper out: \"Improved Convergence in Parameter-Agnostic Error Feedback through Momentum\" - joint work with Abdurakhmon Sadiev, Yury Demidovich, Igor Sokolov, Grigory Malinovsky, and Sarit Khirirat. Abstract: Communication compression is essential for scalable distributed training of modern machine learning models, but it often degrades convergence due to the noise it introduces. Error Feedback (EF) mechanisms are widely adopted to mitigate this issue of distributed compression algorithms. Despite their popularity and training efficiency, existing distributed EF algorithms often require prior knowledge of problem parameters (e.g., smoothness constants) to fine-tune stepsizes. This limits their practical applicability especially in large-scale neural network training. In this paper, we study normalized error feedback algorithms that combine EF with normalized updates, various momentum variants, and parameter-agnostic, time-varying stepsizes, thus eliminating the need for problem-dependent tuning. We analyze the convergence of these algorithms for minimizing smooth functions, and establish parameter-agnostic complexity bounds that are close to the best-known bounds with carefully-tuned problem-dependent stepsizes. Specifically, we show that normalized EF21 achieve the convergence rate of near $O(1/T^{1/4})$ for Polyak's heavy-ball momentum, $O(1/T^{2/7})$ for Iterative Gradient Transport (IGT), and $O(1/T^{1/3})$ for STORM and Hessian-corrected momentum. Our results hold with decreasing stepsizes and small mini-batches. Finally, our empirical experiments confirm our theoretical insights. October 22, 2025 New Paper New paper out: \"Beyond the Ideal: Analyzing the Inexact Muon Update\" - joint work with Egor Shulgin, Sultan AlRashed, and Francesco Orabona. Abstract: The Muon optimizer has rapidly emerged as a powerful, geometry-aware alternative to AdamW, demonstrating strong performance in large-scale training of neural networks. However, a critical theory-practice disconnect exists: Muon's efficiency relies on fast, approximate orthogonalization, yet all prior theoretical work analyzes an idealized, computationally intractable version assuming exact SVD-based updates. This work moves beyond the ideal by providing the first analysis of the inexact orthogonalized update at Muon's core. We develop our analysis within the general framework of Linear Minimization Oracle (LMO)-based optimization, introducing a realistic additive error model to capture the inexactness of practical approximation schemes. Our analysis yields explicit bounds that quantify performance degradation as a function of the LMO inexactness/error. We reveal a fundamental coupling between this inexactness and the optimal step size and momentum: lower oracle precision requires a smaller step size but larger momentum parameter. These findings elevate the approximation procedure (e.g., the number of Newton-Schulz steps) from an implementation detail to a critical parameter that must be co-tuned with the learning schedule. NanoGPT experiments directly confirm the predicted coupling, with optimal learning rates clearly shifting as approximation precision changes. October 13, 2025 FLTA @ Dubrovnik, Croatia I am in Dubrovnik, Croatia, attending The 3rd IEEE International Conference on Federated Learning Technologies and Applications (FLTA 2025), organized by Feras M. Awaysheh (Umeå University, Sweden). Together with Sebastián Ventura, I am the General Chair of the conference. On October 15, I will give the opening keynote talk entitled \"Handling Device Heterogeneity in Federated Learning: First Optimal Parallel SGD Methods in the Presence of Data, Compute and/or Communication Heterogeneity\". October 12, 2025 New Paper New paper out: \"Second-order Optimization under Heavy-Tailed Noise: Hessian Clipping and Sample Complexity Limits\" - joint work with Abdurakhmon Sadiev and Ilyas Fatkhullin. Abstract: Heavy-tailed noise is pervasive in modern machine learning applications, arising from data heterogeneity, outliers, and non-stationary stochastic environments. While second-order methods can significantly accelerate convergence in light-tailed or bounded-noise settings, such algorithms are often brittle and lack guarantees under heavy-tailed noise -- precisely the regimes where robustness is most critical. In this work, we take a first step toward a theoretical understanding of second-order optimization under heavy-tailed noise. We consider a setting where stochastic gradients and Hessians have only bounded p-th moments, for some p ∈ (1,2], and establish tight lower bounds on the sample complexity of any second-order method. We then develop a variant of normalized stochastic gradient descent that leverages second-order information and provably matches these lower bounds. To address the instability caused by large deviations, we introduce a novel algorithm based on gradient and Hessian clipping, and prove high-probability upper bounds that nearly match the fundamental limits. Our results provide the first comprehensive sample complexity characterization for second-order optimization under heavy-tailed noise. This positions Hessian clipping as a robust and theoretically sound strategy for second-order algorithm design in heavy-tailed regimes. October 8, 2025 Teaching at Saudi Aramco, Dhahran I arrived to Dhahran. During October 9-12, I will be teaching the first half of \"Introduction to Machine Learning\" for a cohort of Saudi Aramco employees enrolled in the KAUST Master in Data Science program. My TAs are my PhD students Grigory Malinovsky and Igor Sokolov. October 2, 2025 New Paper New paper out: \"Drop-Muon: Update Less, Converge Faster\" - joint work with Kaja Gruntkowska, Yassine Maziane, and Zheng Qu. Abstract: Conventional wisdom in deep learning optimization dictates updating all layers at every step-a principle followed by all recent state-of-the-art optimizers such as Muon. In this work, we challenge this assumption, showing that full-network updates can be fundamentally suboptimal, both in theory and in practice. We introduce a non-Euclidean Randomized Progressive Training method-Drop-Muon-a simple yet powerful framework that updates only a subset of layers per step according to a randomized schedule, combining the efficiency of progressive training with layer-specific non-Euclidean updates for top-tier performance. We provide rigorous convergence guarantees under both layer-wise smoothness and layer-wise (L0,L1)-smoothness, covering deterministic and stochastic gradient settings, marking the first such results for progressive training in the stochastic and non-smooth regime. Our cost analysis further reveals that full-network updates are not optimal unless a very specific relationship between layer smoothness constants holds. Through controlled CNN experiments, we empirically demonstrate that Drop-Muon consistently outperforms full-network Muon, achieving the same accuracy up to 1.4x faster in wall-clock time. Together, our results suggest a shift in how large-scale models can be efficiently trained, challenging the status quo and offering a highly efficient, theoretically grounded alternative to full-network updates. October 1, 2025 New Paper New paper out: \"Non-Euclidean Broximal Point Method: A Blueprint for Geometry-Aware Optimization\" - joint work with Kaja Gruntkowska. Abstract: The recently proposed Broximal Point Method (BPM) [Gruntkowska et al., 2025] offers an idealized optimization framework based on iteratively minimizing the objective function over norm balls centered at the current iterate. It enjoys striking global convergence guarantees, converging linearly and in a finite number of steps for proper, closed and convex functions. However, its theoretical analysis has so far been confined to the Euclidean geometry. At the same time, emerging trends in deep learning optimization, exemplified by algorithms such as Muon [Jordan et al., 2024] and Scion [Pethick et al., 2025], demonstrate the practical advantages of minimizing over balls defined via non-Euclidean norms which better align with the underlying geometry of the associated loss landscapes. In this note, we ask whether the convergence theory of BPM can be extended to this more general, non-Euclidean setting. We give a positive answer, showing that most of the elegant guarantees of the original method carry over to arbitrary norm geometries. Along the way, we clarify which properties are preserved and which necessarily break down when leaving the Euclidean realm. Our analysis positions Non-Euclidean BPM as a conceptual blueprint for understanding a broad class of geometry-aware optimization algorithms, shedding light on the principles behind their practical effectiveness. October 1, 2025 New Paper New paper out: \"Error Feedback for Muon and Friends\" - joint work with Kaja Gruntkowska, Alexander Gaponov, and Zhirayr Tovmasyan. Abstract: Recent optimizers like Muon, Scion, and Gluon have pushed the frontier of large-scale deep learning by exploiting layer-wise linear minimization oracles (LMOs) over non-Euclidean norm balls, capturing neural network structure in ways traditional algorithms cannot. Yet, no principled distributed framework exists for these methods, and communication bottlenecks remain unaddressed. The very few distributed variants are heuristic, with no convergence guarantees in sight. We introduce EF21-Muon, the first communication-efficient, non-Euclidean LMO-based optimizer with rigorous convergence guarantees. EF21-Muon supports stochastic gradients,",
  "content_length": 42854,
  "method": "requests",
  "crawl_time": "2025-12-01 14:11:40"
}