{
  "name": "Anshumali Shrivastava",
  "homepage": "https://www.cs.rice.edu/~as143",
  "status": "success",
  "content": "Anshumali Shrivastava Anshumali Shrivastava Professor Computer Science and Ken Kennedy Institute, Rice University I am currently at Meta Superintelligence Labs (on Leave from Rice) Founder: ThirdAI Corp. (Acquired in August 2025) xmad.ai (Sept 2024) --> Workato (July 2025) In past I have had positions at Amazon, AWS, Blackstone, and FICO Office: Duncan Hall 2083 Email: anshumali AT rice.edu Phone: 713 348 3049 Follow @Anshumali_ Blogs I write about AI, Businesses, LLMs.[Medium Articles] Deep Learning Breakthrough: a sub-linear deep learning algorithm that does not need a GPU KDNuggets (Invited Blog) 2020 [blog] Recent Preprints Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention. Sahil Joshi, Agniva Chowdhury, Amar Kanakamedala, Ekam Singh, Evan Tu, Anshumali Shrivastava Arxiv:[Paper] To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration . Zeyu Yang, Tianyi Zhang, Jianwen Xie, Chuan Li, Zhaozhuo Xu, Anshumali Shrivastava Arxiv:[Paper] REFRAG: Rethinking RAG based Decoding . Xiaoqiang Lin, Aritra Ghosh, Bryan Kian Hsiang Low, Anshumali Shrivastava, Vijai Mohan Arxiv:[Paper] Commercial Deployments Amazon Product Search: We depoyed our hashing algorithm based on FLASH Engine (SIGMOD 2018) in the Amazon Search Engine. Every query on amazon.com benefits from our research work! [Details] BOLT Engine At ThirdAI Corp. we are developing spasity and hash tables to democratise AI on CPUs [Details] Selected Talks AI Journey Keynote: Scalable and Sustainable AI for Everyone [video] SLIDE A Sub-Linear Deep Learning Algorithm That Does Not Need a GPU [Intel-HPC-AI] Zero-Communication Model Parallelism for Distributed Extreme-Scale Deep Learning [YouTube] Locality Sensitive Hashing for Adaptive Sampling and Unbiased Estimation(Beating Random Sampling in O(1) amortized) [YouTube] Designing Next Generation Resource-Frugal Deep Learning Algorithms [YouTube] Hashing Algorithms for Large-Scale Machine Learning[YouTube] Randomized Algorithms Accelerated over CPU-GPU for Ultra-High Dimensional Similarity Search [YouTube] Hashing for Maximum Inner Product Search (MIPS)[YouTube] An Efficient Replacement for Minwise Hashing[ICML17][KDNuggetsBlog] Research Large Scale Machine Learning Scalable and Sustainable Deep Learning Randomized Algorithms for AI/LLMs Information Retreival More details on my research can be found at RUSHLab RUSH Lab GitHub Page RUSH Lab GitHub Fundings The VMware University Research Fund, Intel Research, Total Research, Adobe, ONR DURIP, SHELL, NSF BIGDATA, ONR BRC, AFOSR YIP, NSF CAREER, NSF-IIS-1718478, Amazon Research Awards, NVIDIA GPU Grant Multiple Postdocs/Research Scientist Positions Avaiable. Drop an email with your CV Research Focus: Large-Scale Machine Learning, Deep Learning, Randomized Algorithms, High-Performance Computing Awards/Honors Charles W. Duncan Jr. Achievement Award for Outstanding Faculty 2023 IIT Kharagpur, Young Alumni Achiever Award (YAAA), 2023 Outstanding Paper Award, MLSys 2022 Awarded Tenure at Rice (Early) [Link] George R. Brown School of Enginnering, Rice University, 2021 Young Faculty Research Award 2021[Link] Adobe Data Science Research Awards 2021[Link] National Academy of Engineers (NAE) 2019 US Frontiers of Engineering Alum [Link] ACM SIGMOD 2018 Most Reproducible Paper Award [Link] Science News 10 Scientist to Watch, 2018 [Science News][Rice News] Best Student Paper Competition Award at IISA, 2018 Amazon Research Awards, 2017 [News] AFOSR Young Investigator Award (YIP), 2017 [News] NSF CAREER AWARD, 2017 [News] NIPS Best Paper, 2014 [News] IEEE/ACM ASONAM Best Paper, 2014 Institute Silver Medal, IIT Kharagpur, 2008 Selected Press Coverage and Live Broadcast ThirdAI Founder Works to Make Artificial Intelligence More Efficient. [Wall Street Journal] ThirdAI raises $6M to democratize AI to any hardware. [TechCrunch] Scientist discovers new way to filter fake news 2020 [Tribune] Two Big, New Threats to NVDA Stock That Should Worry Investors Investorplace 2020 [article] How to test for COVID-19 efficiently Amazon Science (Invited Blog) 2020 [blog] An algorithm could make CPUs a cheap way to train AI Engadget, Science Daily, InsideHPC, and many more 2020 [article] Deep Learning breakthrough made by Rice University scientists ArsTechnica 2019 [article] Hash Your Way To a Better Neural Network IEEE Spectrum 2019 [article] Anshumali Shrivastava uses AI to wrangle torrents of data Science News, SN10 article 2018 [article] Using New Data Techniques To Estimate The Number Syrian Of War Dead Houston Matters, Live Broadcast 2018 [Radio Interview] Making Friends Online is a Number Game Irish Times, Document Journal, and many more. 2018 [News] Fundamental Breakthrough in 2-Decade Old Algorithm Redefines Big-Data Benchmarks KDNuggets 2017 [Blog] Deep learning breakthrough could slash computation and time by 95 percent World News Network, ScienceMagzine, NSF, Innovationtoronto, and many more. 2017 [Article] Beyond Silicon: Squeezing More Out of Chips New York Times 2016 [NYTimes Article] Teaching Fall 2025: Probabilistic Algorithms and Data Structures(COMP 480/580) Spring 2025: Machine Learning (COMP 642) Fall 2024: Probabilistic Algorithms and Data Structures(COMP 480/580) Spring 2024: Machine Learning (COMP 642) Fall 2023: Probabilistic Algorithms and Data Structures(COMP 480/580) Spring 2023: Machine Learning (COMP 642) Fall 2022: Probabilistic Algorithms and Data Structures(COMP 480/580) Spring 2022: Machine Learning (COMP 642) Fall 2021: Graduate Seminar in Machine Learning (COMP 640) Spring 2021: Probabilistic Algorithms and Data Structures(COMP 480/580) Fall 2020: Graduate Seminar in Machine Learning (COMP 640) Spring 2020: Probabilistic Algorithms and Data Structures(COMP 480/580) Fall 2019: Graduate Seminar in Machine Learning (COMP 640) Spring 2019: Probabilistic Algorithms and Data Structures(COMP 480/580) Fall 2017: Graduate Seminar in Machine Learning (COMP 640) Spring 2017: Large Scale Machine Learning (COMP 441/COMP 552) Fall 2016:\t Graduate Seminar in Machine Learning (COMP 640) Spring 2016: Large Scale Machine Learning (COMP 441) Fall 2015:\t Graduate Seminar in Machine Learning (COMP 640) Thesis Probabilistic Hashing Techniques for Big-Data. [pdf] Cornell University, 2015. Publications (Full Papers Only) 2025 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float. [To appear] Tianyi Zhang, Shaochen (Henry) Zhong, Mohsen Hariri, Vipin Chaudhary, Yang Sui, Xia Hu, Anshumali Shrivastava Neural Information Processing Systems (NeurIPS) 2025. Breaking the Frozen Subspace: Importance Sampling for Low-Rank Optimization in LLM Pretraining. [To appear] Haochen Zhang, Junze Yin, Guanchu Wang, Zirui Liu, Lin Yang, Tianyi Zhang, Anshumali Shrivastava, Vladimir Braverman Neural Information Processing Systems (NeurIPS) 2025. Sketch to Adapt: Fine-Tunable Sketches for Efficient LLM Adaptation. [pdf coming soon] Tianyi Zhang, Junda Su, Aditya Desai, Oscar Wu, Zhaozhuo Xu, Anshumali Shrivastava International Conference on Machine Learning (ICML) 2025. CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems. [pdf coming soon] Haochen Zhang, Tianyi Zhang, Junze Yin, Oren Gal, Anshumali Shrivastava, Vladimir Braverman Annual Meeting of the Association for Computational Linguistics (ACL) 2025. Empowering Distributed Training with Sparsity-driven Data Synchronization. [pdf coming soon] Zhuang Wang, Yuke Wang, Zhaozhuo Xu, Jingyi Xi, Anshumali Shrivastava, T. S. Eugene Ng USENIX Symposium on Operating Systems Design and Implementation (OSDI) 2025. LeanQuant: Accurate and Scalable Large Language Model Quantization with Loss-error-aware Grid.[To appear] Tianyi Zhang and Anshumali Shrivastava International Conference on Learning Representations (ICLR) 2025. IDentity with Locality: An ideal hash for gene sequence search. [To appear] Tianyi Zhang, Gaurav Gupta, Aditya Desai, Anshumali Shrivastava SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD) 2025. 2024 KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization. [To appear] Tianyi Zhang, Jonah Wonkyu Yi, Zhaozhuo Xu, Anshumali Shrivastava Neural Information Processing Systems (NeurIPS) 2024. NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention. [To appear] Tianyi Zhang, Jonah Wonkyu Yi, Bowen Yao, Zhaozhuo Xu, Anshumali Shrivastava Neural Information Processing Systems (NeurIPS) 2023. Accelerating Inference with Fast and Expressive Sketch Structured Transform. [To appear] Aditya Desai, Kimia Saedi, Apoorv Walia, Jihyeong Lee, Keren Zhou, Anshumali Shrivastava Neural Information Processing Systems (NeurIPS) 2024. Soft Prompt Recovers Compressed LLMs, Transferably. [pdf coming soon] Zhaozhuo Xu, Zirui Liu, Beidi Chen, Shaochen Zhong, Yuxin Tang, Jue WANG, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava International Conference on Machine Learning (ICML) 2024. In defense of parameter sharing for model-compression . Aditya Desai and Anshumali Shrivastava International Conference on Learning Representations (ICLR) 2024. Learning Scalable Structural Representations for Link Prediction with Bloom Signatures [To appear] Tianyi Zhang, Haoteng Yin, Rongzhe Wei, Pan Li, and Anshumali Shrivastava International World Wide Web Conference (WWW) 2024. 2023 One-Pass Distribution Sketch for Measuring Data Heterogeneity in Federated Learning. [To appear] Zichang Liu, Zhaozhuo Xu, Benjamin Coleman, Anshumali Shrivastava Neural Information Processing Systems (NeurIPS) 2023. DESSERT: An Efficient Algorithm for Vector Set Search with Vector Set Queries. [To appear] Joshua Engels, Benjamin Coleman, Vihan Lakshman, Anshumali Shrivastava Neural Information Processing Systems (NeurIPS) 2023. Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression",
  "content_length": 27536,
  "method": "requests",
  "crawl_time": "2025-12-01 12:59:28"
}