{
  "name": "Norman Ramsey",
  "homepage": "http://www.cs.tufts.edu/~nr",
  "status": "success",
  "content": "Norman Ramsey Norman Ramsey Send email to nr@cs.tufts.edu. Send physical documents to my postal address. If you come yourself you'll want directions. Telephone +1 617 627 4923. Fax +1 617 627 2227 (but if you can, scan and email instead) My vita and public key (now with photo!) are online. I type 75 words per minute. We are typists first, so test yourself. On this page: What I do Papers Teaching For students About me Who I am and what I do I do research in programming languages and teach a mix of classes at Tufts University. I have had my fingers in a lot of pies! Programming-language infrastructure Of all the problems in making programming-language infrastructure reusable, the most challenging is this: if we are given a new machine and are told what its instructions do, how do we generate code for it? Working with my student and postdoc João Dias, I have developed methods of automatically generating an instruction selector—the heart of a code generator—from declarative machine descriptions (POPL 2010). We've also published the abstraction that the instruction selector is built on (POPL 2011). Ideas about infrastructure are most convincing when they are deployed. In 2014, my group’s ideas about code generation were deployed in a new code generator for the Glasgow Haskell Compiler (GHC). Deployment required several years’ effort from João Dias, Simon Marlow of Microsoft Research, Simon Peyton Jones of Microsoft Research, and me. The new code generator’s most interesting component is a reusable, higher-order optimization library, Hoopl (Haskell 2010), which uses generalized algebraic data types to guarantee at compiler-compile time that no matter what Haskell program it is given, GHC never builds an ill-formed control-flow graph. Languages and learning People who want to learn to use programming languages effectively can try starting with an industrial language, but the sheer size of a typical industrial language and library make it hard to discover and apply the ideas that make the language worth learning. People can also consult books, but existing books primarily talk about programming languages, or they talk about how programming languages are implemented, or they steer people to industrial languages. To make it possible for people not just to learn about great ideas in programming languages, but to build software that applies these ideas effectively, I have designed and implemented a collection of tiny programming languages for language learners. Using my languages, learners can build programs that apply some of the greatest ideas in programming languages: functions, types, and objects. The languages form the skeleton of a new book, Programming Languages: Build, Prove, and Compare, which will be published by Cambridge University Press. While implementing the languages I created for the book, I found an interesting problem: there is a big engineering gap between simple, definitional interpreters, which we write to illustrate precisely what a language is supposed to mean, and industrial-strength interpreters and compilers, which we write to run programs efficiently. To narrow this gap, I’ve investigated ways of engineering definitional interpreters to be more efficient, without making them much harder to write (PPDP 2013). Because programming languages are the medium in which software is written, they are also the medium in which beginners learn to build software. The programming language and technique taught in introductory courses are widely believed to affect learning. I have recently piloted a first course based on How to Design Programs by Felleisen et al., which uses functional programming languages and techniques. My analysis, refinements, and recommendations have just been published at the International Conference on Functional Programming (ICFP 2014). Language applications, design, and semantics Languages and techniques are refined, evaluated, and improved by using them on real problems. I’ve applied functional-programming techniques to computational biology; working with graduate students in computational biology, I showed ways in which functional programming works to help solve their problems (ICFP 2012). We also identified obstacles that prevented functional programming from working as well as it could have. Another application area, machine learning, builds on Bayesian reasoning about probabilities. But in current practice, a programmer’s Bayesian ideas are usually hand-translated into a general-purpose programming language like Matlab or C++. Such programmers might be much more productive if given a probabilistic programming language, in which Bayesian reasoning can be expressed directly. With support from DARPA, I'm working with colleagues from BAE Systems and Northeastern University on the design and semantics for probabilistic programming languages. Curricular development Over the past seven years, I have revamped a significant portion of Tufts's required undergraduate curriculum in computer science. Our students are required to take four courses that have programming assignments. As a result of my work, these courses now ask much more of our students than they did formerly, and they also offer more. In particular, we now offer many more challenging, rewarding problems of a sort that students would not think to tackle on their own. In addition to the pilot introductory course mentioned above, I have completely redesigned the third and fourth courses in our programming sequence. Our third course, COMP 40 (Machine Structures and Assembly-Language Programming), is most similar to a course in machine organization or systems programming. I redesigned the course to focus on two sets of skills: applied data abstraction and machine-level programming. The redesigned course, which has now been taught by three other instructors, is viewed by some as the most valuable course in our department. It is highly praised on surveys of graduating seniors, who are given the opportunity to identify just one course that exemplifies \"what a truly excellent college course should be.\" I replaced an older \"paradigms\" course in programming languages with a new required course in programming languages, COMP 105, which demands that students learn to use key programming-language ideas in actual programming, and which also demands some mathematical content (e.g., operational semantics, equational proofs). The course, which uses my draft book Programming Languages: Build, Prove, and Compare, effectively serves as our fourth programming course. COMP 105 is also highly regarded by students and mentioned on senior surveys, although not quite as much as COMP 40. I also have some opinions about what else we should be teaching. My teaching and curricular work were recognized with the 2015 Lerman-Neubauer Prize for Outstanding Teaching and Advising. This prize is awarded annually to a member of the Tufts faculty who has had a profound intellectual impact on his or her students, both inside and outside the classroom. Selected papers This page show my most significant and most recent papers. Links are to abstracts so you can check out the topic without downloading a monster. For a complete view, including older work, see my publications list. Five most significant papers Relocating Machine Instructions by Currying. Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, in SIGPLAN Notices, 31(5):226–236, May 1996. This paper connects two worlds: the ``low cult'' of systems programming and the pure, mathematical world of the lambda-calculus. The key insight is that relocation, which is a low-level operation performed on binary code, is an instance of currying, which is the expression of a multiple-argument function in the lambda-calculus. Stochastic Lambda Calculus and Monads of Probability Distributions (with Avi Pfeffer). Proceedings of the 29th ACM Symposium on the Principles of Programming Languages, in SIGPLAN Notices, 37(1):154–165, January 2002. This paper explores the design of probabilistic languages in a foundational, principled way. Its special contribution is to analyze important implementation techniques in a way that is completely formal and is rigorously connected to the theory of probability. A Transformational Approach to Binary Translation of Delayed Branches (with Cristina Cifuentes). ACM Transactions on Programming Languages and Systems, 25(2):210–224, March 2003. The paper solves a small but difficult problem in analysis of binary codes. This problem is repeatedly a stumbling block for industry groups that work with binary codes, and I believe our solution is definitive. An Expressive Language of Signatures (with Kathleen Fisher and Paul Govereau). In Proceedings of the Tenth ACM SIGPLAN International Conference on Functional Programming (ICFP'05), pages 27–40, September 2005. Selected as one of the best papers of ICFP 2005. In this paper, we identified an important class of programming problems the solutions to which cannot be expressed in current languages, and we showed that these problems can be solved by new mechanisms that cohere with an existing language. Automatically Generating Instruction Selectors Using Declarative Machine Descriptions (with João Dias). In Proceedings of the 37th ACM Symposium on the Principles of Programming Languages, pages 403–416, January 2010. The most beautiful results from João Dias's doctoral dissertation: (a) if all you know is the semantics of the intermediate code and the target instruction set, generating a code generator is undecidable; and (b) by using a clever new heuristic search based on algebraic laws, João can generate code generators for real machines quickly. The core of the algorithm combines Hoare logic and unification to find sequences of machine instructions that implement intermediate code. Will appeal especially to those who like inference rules with their compilers. Five other significant papers Specifying Representat",
  "content_length": 22421,
  "method": "requests",
  "crawl_time": "2025-12-01 14:06:55"
}