{
  "name": "Kevin Tian",
  "homepage": "https://kjtian.github.io",
  "status": "success",
  "content": "Kevin Tian Kevin Tian he/him/his ‖ kjtian@cs.utexas.edu ‖ GDC 4.720 I am an Assistant Professor of Computer Science at UT Austin. I study algorithmic problems in modern data science, often in the span of continuous optimization and high-dimensional statistics. I also have broad interests in trustworthy machine learning, e.g. robustness, privacy, and fairness. From 2022-2023, I was a Postdoctoral Researcher in the Machine Learning Foundations group at Microsoft Research. I completed my Ph.D. in Computer Science at Stanford from 2016-2022 (where I was fortunate to be advised by Aaron Sidford), and my B.S. in Mathematics and Computer Science at MIT from 2012-2015. My work has received generous funding from an NSF Graduate Research Fellowship, a Google Ph.D. Fellowship for Algorithms, Optimizations, and Markets, and a VMware Research Fellowship for the Simons Institute Geometric Methods in Optimization and Sampling program. Please feel free to get in touch if we share interests. Website designed by Sunny Tian (check out some of her cool work here). Photo credits go to the wonderfully talented Amy Zhang. Teaching CS 331: Algorithms and Complexity (Fall 2025) CS 395T: Continuous Algorithms (Spring 2025) CS 331: Algorithms and Complexity (Fall 2024) CS 395T: Continuous Algorithms (Spring 2024) Team Ph.D. students: Anming Gu, Syamantak Kumar (co-advisor: Purnamrita Sarkar), Chutong Yang, Yusong Zhu (co-advisor: Eric Price) M.S. students: Hubert Yang, Jonathan Li (graduated 2025) B.S. students: Srivi Balaji, Satvik Duddukuru, Saloni Modi (graduated 2025), Jennifer Mickel (graduated 2024) Thesis Iterative Methods for Structured Algorithmic Data Science Stanford University, 2022. Arthur Samuel Award for Best Doctoral Thesis in Computer Science. Preprints Perspectives on Stochastic Localization with Bobby Shi and Matthew S. Zhang Preprint, 2025. Publications The Power of Iterative Filtering for Supervised Learning with (Heavy) Contamination with Adam R. Klivans, Konstantinos Stavrapoulos, and Arsen Vasilyan Neural Information Processing Systems (NeurIPS), 2025. Spotlight presentation (top 3% of submissions). Private Geometric Median in Nearly-Linear Time with Syamantak Kumar, Daogao Liu, and Chutong Yang Neural Information Processing Systems (NeurIPS), 2025. More of the Same: Persistent Representational Harms Under Increased Representation with Jennifer Mickel, Maria De-Arteaga, and Leqi Liu Neural Information Processing Systems (NeurIPS), 2025. Radial Isotropic Position via an Implicit Newton's Method with Arun Jambulapati and Jonathan Li Foundations of Computer Science (FOCS), 2025. Spike-and-Slab Posterior Sampling in High Dimensions with Syamantak Kumar, Purnamrita Sarkar, and Yusong Zhu Conference on Learning Theory (COLT), 2025. Omnipredicting Single-Index Models with Multi-Index Models with Lunjia Hu and Chutong Yang Symposium on Theory of Computing (STOC), 2025. Eulerian Graph Sparsification by Effective Resistance Decomposition with Arun Jambulapati, Sushant Sachdeva, Aaron Sidford, and Yibin Zhao Symposium on Discrete Algorithms (SODA), 2025. Learning Noisy Halfspaces with a Margin: Massart is No Harder than Random with Gautam Chandrasekaran, Vasilis Kontonis, and Konstantinos Stavropoulos Neural Information Processing Systems (NeurIPS), 2024. Spotlight presentation (top 2% of submissions). Semi-Random Matrix Completion via Flow-Based Adaptive Reweighting with Jonathan A. Kelner, Jerry Li, Allen Liu, and Aaron Sidford Neural Information Processing Systems (NeurIPS), 2024. Testing Calibration in Nearly-Linear Time with Lunjia Hu, Arun Jambulapati, and Chutong Yang Neural Information Processing Systems (NeurIPS), 2024. Private Stochastic Convex Optimization with Heavy Tails: Near-Optimality from Simple Reductions with Hilal Asi and Daogao Liu Neural Information Processing Systems (NeurIPS), 2024. Closing the Computational-Query Depth Gap in Parallel Stochastic Convex Optimization with Arun Jambulapati and Aaron Sidford Conference on Learning Theory (COLT), 2024. Black-Box k-to-1 PCA Reductions: Theory and Applications with Arun Jambulapati, Syamantak Kumar, Jerry Li, Shourya Pandey, and Ankit Pensia Conference on Learning Theory (COLT), 2024. Linear-Sized Sparsifiers via Near-Linear Time Discrepancy Theory with Arun Jambulapati and Victor Reis Symposium on Discrete Algorithms (SODA), 2024. A CS guide to the quantum singular value transformation with Ewin Tang Symposium on Simplicity in Algorithms (SOSA), 2024. Revisiting Area Convexity: Faster Box-Simplex Games and Spectrahedral Generalizations with Arun Jambulapati Neural Information Processing Systems (NeurIPS), 2023. Structured Semidefinite Programming for Recovering Structured Preconditioners with Arun Jambulapati, Jerry Li, Christopher Musco, Kirankumar Shiragur, and Aaron Sidford Neural Information Processing Systems (NeurIPS), 2023. Matrix Completion in Almost-Verification Time with Jonathan A. Kelner, Jerry Li, Allen Liu, and Aaron Sidford Foundations of Computer Science (FOCS), 2023. ReSQueing Parallel and Private Stochastic Convex Optimization with Yair Carmon, Arun Jambulapati, Yujia Jin, Yin Tat Lee, Daogao Liu, and Aaron Sidford Foundations of Computer Science (FOCS), 2023. Algorithmic Aspects of the Log-Laplace Transform and a Non-Euclidean Proximal Sampler with Sivakanth Gopi, Yin Tat Lee, Daogao Liu, and Ruoqi Shen Conference on Learning Theory (COLT), 2023. Semi-Random Sparse Recovery in Nearly-Linear Time with Jonathan A. Kelner, Jerry Li, Allen Liu, and Aaron Sidford Conference on Learning Theory (COLT), 2023. Quadratic Speedups in Parallel Sampling from Determinantal Distributions with Nima Anari, Callum Burgess, and Thuy-Duong Vuong Symposium on Parallelism in Algorithms and Architectures (SPAA), 2023. Quantum Speedups for Zero-Sum Games via Improved Dynamic Gibbs Sampling with Adam Bouland, Yosheb Getachew, Yujia Jin, and Aaron Sidford International Conference on Machine Learning (ICML), 2023. Private Convex Optimization in General Norms with Sivakanth Gopi, Yin Tat Lee, Daogao Liu, and Ruoqi Shen Symposium on Discrete Algorithms (SODA), 2023. Sharper Rates for Separable Minimax and Finite Sum Optimization via Primal-Dual Extragradient Methods with Yujia Jin and Aaron Sidford Conference on Learning Theory (COLT), 2022. Regularized Box-Simplex Games and Dynamic Decremental Bipartite Matching with Arun Jambulapati, Yujia Jin, and Aaron Sidford International Colloquium on Automata, Languages, and Programming (ICALP), 2022. Clustering Mixture Models in Almost-Linear Time via List-Decodable Mean Estimation with Ilias Diakonikolas, Daniel M. Kane, Daniel Kongsgaard, and Jerry Li Symposium on Theory of Computing (STOC), 2022. Semi-Streaming Bipartite Matching in Fewer Passes and Optimal Space with Sepehr Assadi, Arun Jambulapati, Yujia Jin and Aaron Sidford Symposium on Discrete Algorithms (SODA), 2022. Lower Bounds on Metropolized Sampling Methods for Well-Conditioned Distributions with Yin Tat Lee and Ruoqi Shen Neural Information Processing Systems (NeurIPS), 2021. Oral presentation (top 1% of submissions). List-Decodable Mean Estimation in Nearly-PCA Time with Ilias Diakonikolas, Daniel M. Kane, Daniel Kongsgaard, and Jerry Li Neural Information Processing Systems (NeurIPS), 2021. Spotlight presentation (top 3% of submissions). Robust Regression Revisited: Acceleration and Improved Estimation Rates with Arun Jambulapati, Jerry Li, and Tselil Schramm Neural Information Processing Systems (NeurIPS), 2021. Structured Logconcave Sampling with a Restricted Gaussian Oracle with Yin Tat Lee and Ruoqi Shen Conference on Learning Theory (COLT), 2021. Relative Lipschitzness in Extragradient Methods and a Direct Recipe for Acceleration with Michael B. Cohen and Aaron Sidford Innovations in Theoretical Computer Science (ITCS), 2021. Acceleration with a Ball Optimization Oracle with Yair Carmon, Arun Jambulapati, Qijia Jiang, Yujia Jin, Yin Tat Lee, and Aaron Sidford Neural Information Processing Systems (NeurIPS), 2020. Oral presentation (top 1% of submissions). Robust Sub-Gaussian Principal Component Analysis and Width-Independent Schatten Packing with Arun Jambulapati and Jerry Li Neural Information Processing Systems (NeurIPS), 2020. Spotlight presentation (top 3% of submissions). Coordinate Methods for Matrix Games with Yair Carmon, Yujia Jin, and Aaron Sidford Foundations of Computer Science (FOCS), 2020. Logsmooth Gradient Concentration and Tighter Runtimes for Metropolized Hamiltonian Monte Carlo with Yin Tat Lee and Ruoqi Shen Conference on Learning Theory (COLT), 2020. Variance Reduction for Matrix Games with Yair Carmon, Yujia Jin, and Aaron Sidford Neural Information Processing Systems (NeurIPS), 2019. Oral presentation (top 0.5% of submissions). A Direct Õ(1/ϵ) Iteration Parallel Algorithm for Optimal Transport with Arun Jambulapati and Aaron Sidford Neural Information Processing Systems (NeurIPS), 2019. A Rank-1 Sketch for Matrix Multiplicative Weights with Yair Carmon, John C. Duchi, and Aaron Sidford Conference on Learning Theory (COLT), 2019. Coordinate Methods for Accelerating ℓ∞ Regression and Faster Approximate Maximum Flow with Aaron Sidford Foundations of Computer Science (FOCS), 2018. Invited to the FOCS SICOMP Special Issue. CoVeR: Learning Covariate-Specific Vector Representations with Tensor Decompositions with Teng Zhang and James Zou International Conference on Machine Learning (ICML), 2018. Learning Populations of Parameters with Weihao Kong and Gregory Valiant Neural Information Processing Systems (NeurIPS), 2017. Here is a humorous informational video about the paper. Earlier published work A novel k-mer set memory (KSM) motif representation improves regulatory variant prediction with Yuchun Guo, Haoyang Zeng, Xiaoyun Guo, and David K. Gifford Genome Research, 2018. Predicting gene expression in massively parallel reporter assays: a comparative study with Anat Kreimer, Haoyang Zeng, et al. Human Mutation, 2017. K-mer Set Memor",
  "content_length": 14327,
  "method": "requests",
  "crawl_time": "2025-12-01 13:42:28"
}