{
  "name": "Daniel Hershcovich",
  "homepage": "https://danielhers.github.io",
  "status": "success",
  "content": "Daniel Hershcovich - Home Page Daniel Hershcovich דניאל הרשקוביץ Tenure-Track Assistant Professor at CoAStaL, Natural Language Processing section, Department of Computer Science, University of Copenhagen, Denmark. Research interests: Adapting and generalizing language models and data across cultures and languages. Combining explicit representation of human values and knowledge into language models and their analysis. Developing and evaluating language models with real world impact in multicultural domains, such as literature, law and food. News I was awarded a Villum Experiment grant for my project Aligning Multi-Agent Interactions for Sustainable Food Behaviour (AMAI). My project Cultural Reasoning for Responsible Language Model Development (CuRe) with Jens Bjerring-Hansen received funding from Independent Research Fund Denmark Thematic Research on Artificial Intelligence! I will hire a PhD student for it in early 2027. Projects Cultural Reasoning for Responsible Language Model Development (CuRe). Independent Research Fund Denmark Thematic Research on Artificial Intelligence. PI with Jens Bjerring-Hansen (2026-2030). Aligning Multi-Agent Interactions for Sustainable Food Behaviour (AMAI). Villum Experiment. PI (2026-2028). Automated Legal Information and Knowledge Extraction (ALIKE). Independent Research Fund Denmark. Co-PI with Henrik Palmer Olsen (2025-2027). Explainable Hybrid-AI for Computational Law and Accurate Legal Chatbots (XHAILe). Innovation Fund Denmark Grand Solutions. Co-PI with Thomas Hildebrandt (2025-2028). Haptic Captioning: Using Natural Language Models to Design Haptic Experiences. Villum Experiment. Co-PI with Hasti Seifi (2022-2025). Publications(Google Scholar, Semantic Scholar) Peer-Reviewed Publications Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning. Haijiang Liu, Qiyuan Li, Chao Gao, Yong Cao, Xiangyu Xu, Xun Wu, Daniel Hershcovich and Jinguang Gu. EMNLP 2025. bib proceedings preprint abstract Introducing MARK, the Multi-stAge Reasoning frameworK for cultural value survey response simulation, designed to enhance the accuracy, steerability, and interpretability of large language models in this task. The system is inspired by the type dynamics theory in the MBTI psychological framework for personality research. It effectively predicts and utilizes human demographicnformation for simulation: life-situational stress analysis, group-level personality prediction, and self-weighted cognitive imitation. Experiments on the World Values Survey show that MARK outperforms existing baselines by 10% accuracy and reduces the divergence between model predictions and human preferences. This highlights the potential of our framework to improve zero-shot personalization and help social scientists interpret model predictions. HapticCap: A Multimodal Dataset and Task for Understanding User Experience of Vibration Haptic Signals. Guimin Hu, Daniel Hershcovich and Hasti Seifi. Findings of EMNLP 2025. bib proceedings preprint Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for Cultural Adaptions of Wine Reviews. Chenye Zou, Xingyue Wen, Tianyi Hu, Qian Janice Wang and Daniel Hershcovich. Findings of EMNLP 2025. bib proceedings preprint abstract Recent advances in large language models (LLMs) have opened the door to culture-aware language tasks. We introduce the novel problem of adapting wine reviews across Chinese and English, which goes beyond literal translation by incorporating regional taste preferences and culture-specific flavor descriptors. In a case study on cross-cultural wine review adaptation, we compile the first parallel corpus of professional reviews, containing 8k Chinese and 16k Anglophone reviews. We benchmark both neural-machine-translation baselines and state-of-the-art LLMs with automatic metrics and human evaluation. For the latter, we propose three culture-oriented criteria -- Cultural Proximity, Cultural Neutrality, and Cultural Genuineness -- to assess how naturally a translated review resonates with target-culture readers. Our analysis shows that current models struggle to capture cultural nuances, especially in translating wine descriptions across different cultures. This highlights the challenges and limitations of translation models in handling cultural content. Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users. Antonia Karamolegkou, Malvina Nikandrou, Georgios Pantazopoulos, Danae Sanchez Villegas, Phillip Rust, Ruchira Dhar, Daniel Hershcovich and Anders Søgaard. ACL 2025. SAC Highlight. bib proceedings preprint abstract This paper explores the effectiveness of Multimodal Large Language models (MLLMs) as assistive technologies for visually impaired individuals. We conduct a user survey to identify adoption patterns and key challenges users face with such technologies. Despite a high adoption rate of these models, our findings highlight concerns related to contextual understanding, cultural sensitivity, and complex scene understanding, particularly for individuals who may rely solely on them for visual interpretation. Informed by these results, we collate five user-centred tasks with image and video inputs, including a novel task on Optical Braille Recognition. Our systematic evaluation of twelve MLLMs reveals that further advancements are necessary to overcome limitations related to cultural context, multilingual support, Braille reading comprehension, assistive object recognition, and hallucinations. This work provides critical insights into the future direction of multimodal AI for accessibility, underscoring the need for more inclusive, robust, and trustworthy visual assistance technologies. Towards realistic evaluation of cultural value alignment in large language models: Diversity enhancement for survey response simulation. Haijiang Liu, Yong Cao, Xun Wu, Chen Qiu, Jinguang Gu, Maofu Liu and Daniel Hershcovich. Information Processing & Management, Volume 62, Issue 4, July 2025. bib online pdf abstract Assessing Large Language Models (LLMs) alignment with human values has been a high priority in natural language processing. These models, praised as reservoirs of collective human knowledge, provoke an important question: Do they genuinely reflect the value preferences embraced by different cultures? We measure value alignment by simulating sociological surveys and comparing the distribution of preferences from model responses to human references. We introduce a diversity-enhancement framework featuring a novel memory simulation mechanism, which enables the generation of model preference distributions and captures the diversity and uncertainty inherent in LLM behaviors through realistic survey experiments. To better understand the causes of misalignment, we have developed comprehensive evaluation metrics. Our analysis of multilingual survey data illustrates that our framework improves the reliability of cultural value alignment assessments and captures the complexity of model responses across cultural contexts. Among the eleven models evaluated, the Mistral and Llama-3 series show superior alignment with cultural values, with Mistral-series models notably excelling in comprehending these values in both U.S. and Chinese contexts. Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations. Yong Cao, Haijiang Liu, Arnav Arora, Isabelle Augenstein, Paul Röttger and Daniel Hershcovich. NAACL 2025. bib proceedings preprint abstract Large-scale surveys are essential tools for informing social science research and policy, but running surveys is costly and time-intensive. If we could accurately simulate group-level survey results, this would therefore be very valuable to social science research. Prior work has explored the use of large language models (LLMs) for simulating human behaviors, mostly through prompting. In this paper, we are the first to specialize LLMs for the task of simulating survey response distributions. As a testbed, we use country-level results from two global cultural surveys. We devise a fine-tuning method based on first-token probabilities to minimize divergence between predicted and actual response distributions for a given question. Then, we show that this method substantially outperforms other methods and zero-shot classifiers, even on unseen questions, countries, and a completely unseen survey. While even our best models struggle with the task, especially on unseen questions, our results demonstrate the benefits of specialization for simulation, which may accelerate progress towards sufficiently accurate simulation in the future. Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural Knowledge. Li Zhou, Taelin Karidi, Wanlong Liu, Nicolas Garneau, Yong Cao, Wenyu Chen, Haizhou Li and Daniel Hershcovich. NAACL 2025. bib proceedings preprint abstract Recent studies have highlighted the presence of cultural biases in Large Language Models (LLMs), yet often lack a robust methodology to dissect these phenomena comprehensively. Our work aims to bridge this gap by delving into the Food domain, a universally relevant yet culturally diverse aspect of human life. We introduce FmLAMA, a multilingual dataset centered on food-related cultural facts and variations in food practices. We analyze LLMs across various architectures and configurations, evaluating their performance in both monolingual and multilingual settings. By leveraging templates in six different languages, we investigate how LLMs interact with language-specific and cultural knowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias towards food knowledge prevalent in the United States; (2) Incorporating relevant cultural context significantly improves LLMs' ability to access cultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is highly dependent on the interplay between the probing langu",
  "content_length": 111418,
  "method": "requests",
  "crawl_time": "2025-12-01 12:57:09"
}