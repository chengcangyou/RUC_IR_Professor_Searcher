{
  "name": "Wen Sun 0002",
  "homepage": "https://wensun.github.io",
  "status": "success",
  "content": "Wen Sun Wen Sun I'm an Assistant Professor in the Computer Science Department and Cornell Tech at Cornell University. I'm also a research scientist at Databricks AI. Prior to Cornell, I was a post-doc researcher at Microsoft Research NYC from 2019 to 2020. I completed my PhD at Robotics Institute, Carnegie Mellon University in June 2019, where I was advised by Drew Bagnell. CV  / PhD Thesis  / Google Scholar  / Email Prospective students, please read this. For PhD applicants: thank you for your interests! I am taking on new PhD students. However, there is no need to directly contact me with regard to PhD admissions as it is handled by the admission committee. Instead please mention my name in your research statement. I look forward to your applications! For admitted Cornell PhD students: If you are interested in working with me, please send me your CV with some paragraphs describing your past research experiences and current research interests. For undergraduate/MS students at Cornell and outside visitors: please send me your CV and (unofficial) transcript along with two paragraphs describing your research interests, research experience, and why you want to get invovled. Your chance of getting invovled is higher if the more of the followings hold true: you have a high GPA; you did quite well on courses related to math, statistics, machine learning, robotics, and NLP; you are able to commit 12+ hours per week on research; you have strong programming skills; you have experience on applications like natural language processing, robotics, and computer vision. Research My group works on Reinforcement Learning, AI, and Decision Making. The most recent research directions of the lab are RL for generative models, e.g., fine-tune LLMs w/ RL, fine-tune diffusion models, The transformer RL and IL library RL via generative models, e.g., diffusion model for Inverse RL Core RL theory, e.g., the role of loss function in RL Algorithmic and theoretical foundations of RLHF, e.g., offline RLHF, contextual dueling bandits w/ active query RL with offline and online data, e.g., hybrid RL, hybrid RLHF Representation learning in RL, e.g., theory of representation learning in RL, theory of representation transfer in RL The softwares developed by our group can be found here. Selected Awards Sloan Research Fellowship in Computer Science, 2025 NSF Career Award, 2024 Ann S. Bowers Research Excellence Award from the College of Computing and Information Science, Cornell, 2024. Teaching Spring 2025: CS 4789/5789 Introduction to Reinforcement Learning Fall 2024: CS 6789 Foundations of Reinforcement Learning Fall 2023: CS 4780/5780 Introduction to Machine Learning Spring 2021: CS 4789/5789 Introduction to Reinforcement Learning Recent Talks / Lectures/ Tutorials Simons Institute (Sep 2022): Generalization and robustness in offline reinforcement learning IJCAI 2022 Tutorial: Adversarial sequential decision making Recorded lectures of CS4789 (Intro to RL) Spring 2021 are available here COLT 2021 RL Tutorial: Statistical Foundations of RL, videos are here Monograph Reinforcement Learning: Theory and Algorithms Alekh Agarwal, Nan Jiang, Sham Kakade, Wen Sun We are periodically making updates to the book draft. Content based on the courses taught by Nan at UIUC, the courses taught by Alekh and Sham at UW, and CS 6789 at Cornell. Preprints Expressive Value Learning for Scalable Offline Reinforcement Learning Nicolas Espinosa-Dice, Kiante Brantley, Wen Sun arXiv 2025, [website] Prompt Curriculum Learning for Efficient LLM Post-Training Zhaolin Gao, Joongwon Kim, Wen Sun, Thorsten Joachims, Sid Wang, Richard Yuanzhe Pang, Liang Tan arXiv, 2025 (blog post) Heuristics Considered Harmful: RL With Random Rewards Should Not Make LLMs Reason Owen Oertell, Wenhao Zhan, Gokul Swamy, Zhiwei Steven Wu, Kiante Brantley, Jason Lee, Wen Sun 2025 Efficient Controllable Diffusion via Optimal Classifier Guidance Owen Oertell, Shikun Sun, Yiding Chen, Jin Peng Zhou, Zhiyong Wang, Wen Sun arXiv, 2025 [code] All roads lead to likelihood: The value of reinforcement learning in fine-tuning Gokul Swamy, Sanjiban Choudhury, Wen Sun, Zhiwei Steven Wu, J Andrew Bagnell arXiv, 2025 Orchestrating LLMs with Different Personalizations Jin Peng Zhou, Katie Z Luo, Jingwen Gu, Jason Yuan, Kilian Q. Weinberger, Wen Sun arXiv, 2024 Dataset Reset Policy Optimization for RLHF Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Kiante Brantley, Dipendra Misra, Jason D. Lee, Wen Sun arXiv, 2024 [code] We show that resetting to offline data is an effective way of leveraging offline data in the RLHF pipeline Learning to Generate Better Than Your LLM Jonathan D. Chang, Kiante Brantley, Rajkumar Ramamurthy, Dipendra Misra, Wen Sun arXiv, 2023   [code] A new framework -- RL with Guided Feedback (RLGF), combining RL and pre-trained LLMs via principled interactive learning procedures. Finite Sample Analysis of Minimax Offline Reinforcement Learning: Completeness, Fast Rates and First-Order Efficiency Masatoshi Uehara, Masaaki Imaizumi, Nan Jiang, Nathan Kallus, Wen Sun, Tengyang Xie arXiv, 2021 Publications Accelerating RL for LLM Reasoning with Optimal Advantage Regression Kiante Brantley, Mingyu Chen, Zhaolin Gao, Jason D. Lee, Wen Sun, Wenhao Zhan, Xuezhou Zhang NeurIPS, 2025 [code] [models] Scaling Offline RL via Efficient and Expressive Shortcut Models Nicolas Espinosa-Dice, Yiyi Zhang, Yiding Chen, Bradley Guo, Owen Oertell, Gokul Swamy, Kiante Brantley, Wen Sun NeurIPS, 2025 [code] [blog] Value-Guided Search for Efficient Chain-of-Thought Reasoning Kaiwen Wang, Jin Peng Zhou, Jonathan Chang, Zhaolin Gao, Nathan Kallus, Kiante Brantley, Wen Sun NeurIPS, 2025 [code, data, model] Q#: Provably Optimal Distributional RL for LLM Post-Training Jin Peng Zhou, Kaiwen Wang, Jonathan Chang, Zhaolin Gao, Nathan Kallus, Kilian Q Weinberger, Kiante Brantley, Wen Sun NeurIPS, 2025 [code] Avoiding exp(R) scaling in RLHF through Preference-based Exploration Mingyu Chen, Yiding Chen, Wen Sun, Xuezhou Zhang NeurIPS, 2025 Convergence Of Consistency Model With Multistep Sampling Under General Data Assumptions Yiding Chen, Yiyi Zhang, Owen Oertell, Wen Sun ICML, 2025 Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to Standard RL Kaiwen Wang, Dawen Liang, Nathan Kallus, Wen Sun ICML, 2025 The Central Role of the Loss Function in Reinforcement Learning Kaiwen Wang, Nathan Kallus, Wen Sun Statistical Science, 2025 Diffusing States and Matching Scores: A New Framework for Imitation Learning Runzhe Wu, Yiding Chen, Gokul Swamy, Kiante Brantley, Wen Sun ICLR, 2025 [code] Just as GANs led to GAIL in IRL, how could diffusion models --- a more powerful generative model, be applied to IRL to achieve similar success? We provide an answer in this work. Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF Zhaolin Gao, Wenhao Zhan, Jonathan D. Chang, Gokul Swamy, Kiante Brantley, Jason D. Lee, Wen Sun ICLR, 2025 [code] [models] We provide a new RL policy optimization algorithm for multi-turn RLHF. The algorithm enables efficient optimization for a 8B size model in long conversations against a 70B model. Model-based RL as a Minimalist Approach to Horizon-Free and Second-Order Bounds Zhiyong Wang, Dongruo Zhou, John C.S. Lui, Wen Sun ICLR, 2025 Efficient Imitation under Misspecification Nicolas Espinosa-Dice, Sanjiban Choudhury, Wen Sun, Gokul Swamy ICLR, 2025 Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics Runzhe Wu, Ayush Sekhari, Akshay Krishnamurthy, Wen Sun ICLR, 2025   (Oral) Correcting the Mythos of KL-Regularization: Direct Alignment without Overoptimization via Chi-squared Preference Optimization Audrey Huang, Wenhao Zhan, Tengyang Xie, Jason D. Lee, Wen Sun, Akshay Krishnamurthy, Dylan J. Foster ICLR, 2025   (Spotlight) New offline RLHF algorithms that avoid overoptimization and achieve single policy coverage style guarantees by regularizing with the Chi-squared divergence. On Speeding Up Language Model Evaluation Jin Peng Zhou, Christian K. Belardi, Ruihan Wu, Travis Zhang, Carla P. Gomes, Wen Sun, Kilian Q. Weinberger ICLR, 2025 REBEL: Reinforcement Learning via Regressing Relative Rewards Zhaolin Gao, Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Gokul Swamy, Kiante Brantley, Thorsten Joachims, J. Andrew Bagnell, Jason D. Lee, Wen Sun NeurIPS, 2024 [code] [models] New RL algorithm for generative model optimization which outperforms PPO on both text and image generation. State-of-art performance on LLM benchmarks. Efficient and Sharp Off-Policy Evaluation in Robust Markov Decision Processes Andrew Bennett, Nathan Kallus, Miruna Oprescu, Wen Sun, Kaiwen Wang NeurIPS, 2024 The Importance of Online Data: Understanding Preference Fine-Tuning Through the Lens of Coverage Yuda Song, Gokul Swamy, Aarti Singh, J. Andrew Bagnell, Wen Sun NeurIPS, 2024 We study why DPO is not equivalent to RLHF, and what is the key benefit of online samples in RLHF. The new hybrid algorithm that uses both online and offline dadta outperforms DPO on standard RLHF benchmarks. More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning Kaiwen Wang, Owen Oertell, Alekh Agarwal, Nathan Kallus, Wen Sun ICML, 2024 We show that distributional RL enables faster learning when the systems have low variance. This holds for contextual bandits, online and offline RL simoutaneously. JoinGym: An Efficient Query Optimization Environment for Reinforcement Learning Kaiwen Wang, Junxiong Wang, Yueying Li, Nathan Kallus, Immanuel Trummer, Wen Sun RLC, 2024 [code] A light weight, real-world database query optimization benchmark for RL RL for Consistency Models: Faster Reward Guided Text-to-Image Generation Owen Oertell, Jonathan D. Chang, Yiyi Zhang, Kiante Brantley, Wen Sun RLC, 2024 [Website] Faster Recalibration of an Online Predictor via Approachability Princewill Okoroafor, Bobby Kleinberg, Wen Sun AISTATS, 2024 Provable Reward-Agnostic Preferenc",
  "content_length": 24924,
  "method": "requests",
  "crawl_time": "2025-12-01 14:46:03"
}