{
  "name": "Jordan L. Boyd-Graber",
  "homepage": "http://www.umiacs.umd.edu/~jbg",
  "status": "success",
  "content": "Jordan Boyd-Graber: Home Jordan Boyd-Graber Courses Faq Media Openings Peers Projects Pubs Style Jordan Boyd-Graber Iribe 4146 University of Maryland jbg@umiacs.umd.edu I am a full professor in the University of Maryland Computer Science Department (tenure home), Institute of Advanced Computer Studies, INFO, and Language Science Center. My research focuses on making machine learning more useful, more interpretable, and able to learn and interact from humans. This helps users sift through decades of documents; discover when individuals lie, reframe, or change the topic in a conversation; or to compete against humans in games that are based in natural language. My One-page CV My Long (and ugly) CV Research Statement Teaching Statement Diversity Statement Service Statement My Google Scholar page Book a meeting with me (collaborators and UMD students). News Interviewed in the Baltimore Sun Two papers accepted to EMNLP 2025 I'm teaching Undergraduate NLP Recent Publications Yoo Yeon Sung, Maharshi Gor, Eve Fleisig, Ishani Mondal, and Jordan Boyd-Graber. ADVSCORE: A Metric for the Evaluation and Creation of Adversarial Benchmarks. North American Association for Computational Linguistics, 2025. [Bibtex] @inproceedings{Sung:Gor:Fleisig:Mondal:Boyd-Graber-2025, Title = {ADVSCORE: A Metric for the Evaluation and Creation of Adversarial Benchmarks}, Author = {Yoo Yeon Sung and Maharshi Gor and Eve Fleisig and Ishani Mondal and Jordan Lee Boyd-Graber}, Journal = {North American Association for Computational Linguistics}, Year = {2025}, Url = {http://cs.umd.edu/~jbg//docs/2025_naacl_advscore.pdf}, } This was one of ten papers selected as an Outstanding Paper at NAACL 2025 Accessible Abstract: Adversarial datasets should validate AI robustness by presenting samples that humans handle well but models struggle with. However, as models advance, these datasets risk becoming obsolete. Assessing whether a dataset remains adversarial is challenging due to the absence of a standardized metric for adversarialness. To address this, we introduce AdvScore, a human-grounded evaluation metric that quantifies a dataset's adversarial nature by accounting for the differing abilities of models and humans while also identifying low-quality examples. Zongxia Li , Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, and Jordan Boyd-Graber. VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on Synthetic Video Understanding. Neural Information Processing Systems, 2025. [Bibtex] @inproceedings{Li:Wu:Shi:Qin:Du:Zhou:Manocha:Boyd-Graber-2025, Title = {VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on Synthetic Video Understanding}, Author = {Zongxia Li and Xiyang Wu and Guangyao Shi and Yubin Qin and Hongyang Du and Tianyi Zhou and Dinesh Manocha and Jordan Lee Boyd-Graber}, Booktitle = {Neural Information Processing Systems}, Year = {2025}, Location = {San Diego}, Url = {http://cs.umd.edu/~jbg//docs/2025_neurips_videohallusion.pdf}, } Wichayaporn Wongkamjan, Yanze Wang, Feng Gu, Denis Peskoff, Jonathan K. Kummerfeld, Jonathan May, and Jordan Boyd-Graber. Should I Trust You? Detecting Deception in Negotiations using Counterfactual RL. Findings of the Association for Computational Linguistics, 2025. [Code/Data] [Bibtex] @article{Wongkamjan:Wang:Gu:Peskoff:Kummerfeld:May:Boyd-Graber-2025, Title = {Should I Trust You? Detecting Deception in Negotiations using Counterfactual RL}, Author = {Wichayaporn Wongkamjan and Yanze Wang and Feng Gu and Denis Peskoff and Jonathan K. Kummerfeld and Jonathan May and Jordan Boyd-Graber}, Journal = {Findings of the Association for Computational Linguistics}, Year = {2025}, Url = {http://cs.umd.edu/~jbg//docs/2025_acl_ctrld.pdf}, } Accessible Abstract: When determining when an offer sounds \"too good to be true\", it helps to consider what the person sending the message has to gain. When we provide this information to classifiers tasked with determining if a message is deceptive in the online game of Diplomacy, it dramatically improves ability to detect deception. Ishani Mondal, Jack W. Stokes, Sujay Kumar Jauhar, Longqi Yang, Mengting Wan, Xiaofeng Xu, Xia Song, Jordan Boyd-Graber, and Jennifer Neville. Group Preference Alignment: Customizing LLM Responses from In-Situ Conversations Only When Needed. Empirical Methods in Natural Language Processing (Industry), 2025. [Bibtex] @inproceedings{Mondal:Stokes:Jauhar:Yang:Wan:Xu:Song:Boyd-Graber:Neville-2025, Url = {http://cs.umd.edu/~jbg//docs/2025_emnlp_grouppreference.pdf}, Author = {Ishani Mondal and Jack W. Stokes and Sujay Kumar Jauhar and Longqi Yang and Mengting Wan and Xiaofeng Xu and Xia Song and Jordan Boyd-Graber and Jennifer Neville}, Booktitle = {Empirical Methods in Natural Language Processing (Industry)}, Title = {Group Preference Alignment: Customizing LLM Responses from In-Situ Conversations Only When Needed}, Location = {Suzhou, China}, Year = {2025}, } Lorena Calvo-Bartolomé, Valérie Aldana, Karla Cantarero, Alonso Madroñal de Mesa, Jerónimo Arenas-García, and Jordan Boyd-Graber. Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering. Empirical Methods in Natural Language Processing, 2025. [code+data] [Bibtex] @inproceedings{Calvo-Bartolome:Aldana:Cantarero:Mesa:o:Boyd-Graber-2025, Author = {Lorena Calvo-Bartolom\\'{e} and Val\\'{e}rie Aldana and Karla Cantarero and Alonso Madro\\~nal de Mesa and Jer\\'{o}nimo Arenas-Garc\\'{i}a and Jordan Boyd-Graber}, Booktitle = {Empirical Methods in Natural Language Processing}, Title = {Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering}, Location = {Suzhou, China}, Year = {2025}, Url = {http://cs.umd.edu/~jbg//docs/2025_emnlp_mind.pdf}, } Accessible Abstract: Imagine asking an AI chatbot for health advice and getting conflicting guidanceâor turning to a chatbot in a crisis only to receive unclear instructions. Confusing or inconsistent AI isnât just frustrating; it can put peopleâs health and safety at risk. To address this our system proactively identifies discrepancies across languages before they appear in AI-generated answers. Dubbed MIND (Multilingual Inconsistent Notion Detection), the system aligns documents from different languages in a shared conceptual space, compares interpretations, and flags factual or culturally divergent information. For example, guidance on childbirth practices can vary by region, and MIND highlights these differences so users can trust the information. Nishant Balepur, Matthew Shu, Yoo Yeon Sung, Seraphina Goldfarb-Tarrant, Shi Feng, Fumeng Yang, Rachel Rudinger, and Jordan Boyd-Graber. A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users. Empirical Methods in Natural Language Processing, 2025. [code+data] [video] [Bibtex] @inproceedings{Balepur:Shu:Sung:Goldfarb-Tarrant:Feng:Yang:Rudinger:Boyd-Graber-2025, Title = {A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users}, Author = {Nishant Balepur and Matthew Shu and Yoo Yeon Sung and Seraphina Goldfarb-Tarrant and Shi Feng and Fumeng Yang and Rachel Rudinger and Jordan Boyd-Graber}, Booktitle = {Empirical Methods in Natural Language Processing}, Location = {Suzhou and China}, Year = {2025}, Url = {http://cs.umd.edu/~jbg//docs/2025_emnlp_planorama.pdf}, } Accessible Abstract: One of the ways that AI can help users with a task is by developing a plan: a set of steps to solve a problem or complete a task. Through a user study with human--AI teams, we show that AIs are poor judges of what plan is going to be more helpful to more helpful to a user trying to answer math questions or questions that require multiple steps of research (e.g., what's the tallest building in the most populous city in Germany). Nishant Balepur, Vishakh Padmakumar, Fumeng Yang, Shi Feng, Rachel Rudinger, and Jordan Boyd-Graber. Whose Boat Does it Float? Improving Personalization in Preference Tuning via Inferred User Personas. Association for Computational Linguistics, 2025. [Code/Data] [Bibtex] @inproceedings{Balepur:Padmakumar:Yang:Feng:Rudinger:Boyd-Graber-2025, Title = {Whose Boat Does it Float? Improving Personalization in Preference Tuning via Inferred User Personas}, Author = {Nishant Balepur and Vishakh Padmakumar and Fumeng Yang and Shi Feng and Rachel Rudinger and Jordan Lee Boyd-Graber}, Booktitle = {Association for Computational Linguistics}, Location = {Vienna, Austria}, Year = {2025}, Url = {http://cs.umd.edu/~jbg//docs/2025_acl_boat.pdf}, } Accessible Abstract: Language models are optimized to learn which responses you prefer, but they don't learn why you preferred a particular response. This limits their ability to tailor to personalized requests (e.g., \"What should I eat for dinner? I'm vegetarian\"), so we introduce a simple fix: have models infer personas that explain why users could prefer responses. We show training on these inferred personas leads to responses that are significantly more personalized for user needs. Nishant Balepur, Rachel Rudinger, and Jordan Boyd-Graber. Which of These Best Describes Multiple Choice Evaluation with LLMs? A) Forced B) Flawed C) Fixable D) All of the Above. Association for Computational Linguistics, 2025. [Bibtex] @inproceedings{Balepur:Rudinger:Boyd-Graber-2025, Title = {Which of These Best Describes Multiple Choice Evaluation with LLMs? A) Forced B) Flawed C) Fixable D) All of the Above}, Author = {Nishant Balepur and Rachel Rudinger and Jordan Boyd-Graber}, Booktitle = {Association for Computational Linguistics}, Location = {Vienna, Austria}, Year = {2025}, Url = {http://cs.umd.edu/~jbg//docs/2025_acl_mcqa_bad.pdf}, } Accessible Abstract: Most people dislike taking multiple-choice tests, so why are they the default way we evaluate NLP systems? This position paper argues that, despite its simplicity and popularity, multiple-choice evaluation is flawed, both in its format and the datasets i",
  "content_length": 19797,
  "method": "requests",
  "crawl_time": "2025-12-01 13:34:26"
}