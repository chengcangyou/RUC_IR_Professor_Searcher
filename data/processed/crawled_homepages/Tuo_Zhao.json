{
  "name": "Tuo Zhao",
  "homepage": "https://www2.isye.gatech.edu/~tzhao80",
  "status": "success",
  "content": "Tuo Zhao - Alchimia vos liberabit! Tuo Zhao 赵拓 I am an Associate Professor of ISyE and CSE at Georgia Tech. I received my Ph.D. in Computer Science from Johns Hopkins University. My research focuses on methodologies and theories of machine learning, especially deep learning. My recent efforts have been primarily dedicated to Large Language Models (LLMs), collaborating closely with Microsoft and Amazon. I am working with talented alchemists in the FLASH (Foundations of LeArning Systems for alcHemy) group. If you are interested in joining my group, please see more information here. Google ScholarTwitter知乎 Recent News (Check all the past news) Jun. 2025: Zichong Li, in collaboration with Microsoft, has led the release of the newest additions to the Phi model family—SlimMOE Framework and the Open-Source Models Phi-mini-MoE-instruct and Phi-Tiny-MoE-instruct. Collaborators include alumni Chen Liang and fellow group members Zixuan Zhang and Ilgee Hong. Apr. 2025: Alexander Bukharin has successfully defended his Ph.D. Dissertation: Robust and Flexible Reward Modeling for LLM Alignment. He will join NVIDIA as a research scientist. Apr. 2025: Qingru Zhang has successfully defended his Ph.D. Dissertation: On the Efficiency and Steerability of Self-Attention Mechanism of LLMs. He will join Microsoft as a research scientist. June. 2024: Yan Li has successfully defended his Ph.D. Dissertation: Theories and Algorithms for Efficient and Robust Sequential Decision Making. He will join Department of Industrial and Systems Engineering at Texas A&M University as a tenure-track assistant professor in 2024 Fall. Feb. 2024: Minshuo Chen has accepted an offer of tenure-track assistant professor position from Department of Industrial Engineering and Management Sciences at Northwestern University. He will start in 2024 Fall. Nov. 2023: Chen Liang has successfully defended her Ph.D. Dissertation: On Parameter Efficiency of Neural Language Models. She will join Microsoft as a senior research scientist. Oct. 2023: Prof. Shihao Yang and I co-organized Georgia Statistics Day 2023. Apr. 2023: Simiao Zuo has successfully defended his Ph.D. Dissertation: On Training, Inference and Sample Efficiency of Language Models. He will join Microsoft as a research scientist. Mar. 2023: Qingru Zhang's recent collabrative work with Microsoft Azure AI on parameter efficient fine-tuning is available on Hugging Face now. See more information here. Oct. 2022: One Ph.D. position is avaiable in my group. Prof. Yongsheng Chen in School of Civil and Environmental Engineering and I are recruiting a Ph.D. student to work on the interface of computational chmistry and machine learning. See more information here. Please contact us if you are interested and have a background in molecular dynamics simulation. Sep. 2022: One Ph.D. position is avaiable in my group. Prof. Hua Wang at ETH Zurich and I are recruiting a PhD student to work on the interface of morden circuit design and machine learning. See more information here. Please contact me if you are interested and have a background in electromagnetics, especially EM simulation. Sep. 2022: Two Ph.D. positions are avaiable in my group. See more information here. Please contact me if you are interested in deep learning theory or natural language processing. Jul. 2022: Minshuo Chen has successfully defended his Ph.D. Dissertation: Representaiton and Statistical Properties of Deep Neural Networks for Structured Data. He will join Princeton Univesity as a postdoctral fellow. Jul. 2022: Siawpeng Er has successfully defended his Ph.D. Dissertation: Deep Learning in Biomedical Informatics and Modern Circuit Design. He will join Home Depot as a Data Scientist. Preprints and Working Papers (* indicates equal contributions, and ‡ indicates advisees) IDEA Prune: An Integrated Enlarge-and-Prune Pipeline in Generative Language Model Pretraining Yixiao Li‡, Yixiao Li, Xianzhi Du, Ajay Jaiswal, Tao Lei, Tuo Zhao, Chong Wang and Jianyu WangPreprint available on arXiv [Link] LLMs can generate a better answer by aggregating their own responses Zichong Li‡, Xinyu Feng, Yuheng Cai‡, Zixuan Zhang‡, Tianyi Liu, Chen Liang, Weizhu Chen, Haoyu Wang and Tuo ZhaoPreprint available on arXiv [Link] COSMOS: A hybrid adaptive optimizer for memory-efficient training of LLMs Liming Liu‡, Zhenghao Xu‡, Zixuan Zhang‡, Hao Kang, Zichong Li‡, Chen Liang, Weizhu Chen and Tuo ZhaoPreprint available on arXiv [Link] Model Tells Itself Where to Attend: Faithfulness Meets Automatic Attention Steering Qingru Zhang‡, Xiaodong Yu, Chandan Singh, Xiaodong Liu, Liyuan Liu, Jianfeng Gao, Tuo Zhao, Dan Roth and Hao ChengPreprint available on arXiv [Link] GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLMs Hao Kang, Qingru Zhang‡, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tusha Krishna and Tuo ZhaoPreprint available on arXiv [Link] Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems Xiang Ji, Huazheng Wang, Minshuo Chen, Tuo Zhao and Mengdi WangPreprint available on arXiv [Link] First-order Policy Optimization for Robust Markov Decision Process George Lan, Yan Li‡ and Tuo ZhaoPreprint available on arXiv [Link] DiP-GNN: Discriminative Pre-Training of Graph Neural Networks Simiao Zuo‡, Haoming Jiang, Qingyu Yin, Xianfeng Tang, Bing Yin and Tuo ZhaoPreprint available on arXiv [Link] Differentially Private Estimation of Hawkes Process Simiao Zuo‡, Tianyi Liu‡, Tuo Zhao and Hongyuan ZhaPreprint available on arXiv [Link] Implicit Regularization of Bregman Proximal Point Algorithm and Mirror Descent on Separable Data Yan Li‡, Caleb Ju, Ethan Fang and Tuo Zhao Preprint available on arXiv [Link] Statistical Guarantees of Generative Adversarial Networks for Distribution Estimation Minshuo Chen‡, Wenjing Liao, Hongyuan Zha and Tuo Zhao (Alphabetical order) Preprint available on arXiv [Link] Selected Publications (* indicates equal contributions, # indicates alphabetical order, and ‡ indicates advisees) A Minimalist Example of Edge-of-Stability and Progressive Sharpening Liming Liu‡, Zixuan Zhang‡, Simon Du and Tuo ZhaoConference on Neural Information Processing Systems (NeurIPS), 2025 [arXiv] AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders Yuezhou Hu‡*, Jiaxin Guo‡*, Xinyu Feng, and Tuo Zhao Conference on Neural Information Processing Systems (NeurIPS), 2025 [arXiv] Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models Ilgee Hong‡, Changlong Yu, Liang Qiu, Weixiang Yan, Zhenghao Xu‡, Haoming Jiang, Qingru Zhang‡, Qin Lu, Xin Liu, Chao Zhang, and Tuo Zhao Conference on Neural Information Processing Systems (NeurIPS), 2025 [arXiv] Ask a Strong LLM Judge when Your Reward Model is Uncertain Zhenghao Xu‡, Qin Lu, Qingru Zhang‡, Liang Qiu, Ilgee Hong‡, Changlong Yu, Wenlin Yao, Yao Liu, Haoming Jiang, Lihong Li, Hyokun Yun, and Tuo Zhao Conference on Neural Information Processing Systems (NeurIPS), 2025 [arXiv] Doubly Robust Off-Policy Learning on Low-Dimensional Manifolds by Deep Neural Networks Minshuo Chen‡*, Hao Liu*, Wenjing Liao and Tuo Zhao Accepted by Mathematics of Operations Research (with minor revision) [arXiv] Good regularity creates large learning rate implicit biases: edge of stability, balancing, and catapult Yuqing Wang, Zhenghao Xu‡, Tuo Zhao and Molei TaoAccepted by Journal of Machine Learning Research (with minor revision) [arXiv] DORM: Preference Data Weights Optimization for Reward Modeling in LLM Alignment Rongzhi Zhang, Chenwei Zhang, Xinyang Zhang, Liang Qiu, Haoming Jiang, Yuchen Zhuang, Qingru Zhang, Hyokun Yun, Xian Li, Bing Yin, Tuo Zhao and Chao Zhang Conference on Empirical Methods in Natural Language Processing (EMNLP), 2025 SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation Zichong Li‡, Chen Liang, Zixuan Zhang‡, Ilgee Hong‡, Young Jin Kim, Weizhu Chen and Tuo Zhao Conference on Language Modeling (COLM), 2025 [arXiv] Adversarial Training of Reward Models Alexander Bukharin‡, Haifeng Qian, Shengyang Sun, Adithya Renduchintala, Soumye Singhal, Zhilin Wang, Oleksii Kuchaiev, Olivier Delalleau and Tuo Zhao Conference on Language Modeling (COLM), 2025 [arXiv] Self-Rewarding PPO: Aligning Large Language Models with Demonstrations Only Qingru Zhang‡, Liang Qiu, Ilgee Hong‡, Zhenghao Xu‡, Tianyi Liu, Shiyang Li, Rongzhi Zhang, Zheng Li, Lihong Li, Bing Yin, Chao Zhang, Jianshu Chen, Haoming Jiang and Tuo Zhao Conference on Language Modeling (COLM), 2025 [arXiv] NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models Lawrence Ray Liu, Inesh Chakrabarti, Yixiao Li‡, Mengdi Wang, Tuo Zhao, Lin Yang Conference on Language Modeling (COLM), 2025 [arXiv] RoseRAG: Robust retrieval-augmented generation with small-scale LLMs via margin-aware preference optimization Tianci Liu, Haoxiang Jiang, Tianze Wang, Ran Xu, Yue Yu, Linjun Zhang, Tuo Zhao and Haoyu Wang Annual Meeting of the Association for Computational Linguistics (ACL), 2025 [arXiv] Deep Reinforcement Learning with Hierarchical Preference Design Alexander Bukharin‡, Yixiao Li‡, Pengcheng He, Weizhu Chen and Tuo Zhao International Conference on Machine Learning (ICML), 2025 [arXiv] Discriminative Finetuning of Generative LLMs without Reward Models and Preference Data Siqi Guo, Ilgee Hong‡, Vicente Balmaseda, Tuo Zhao and Tianbao Yang International Conference on Machine Learning (ICML), 2025 [arXiv] Robust Reinforcement Learning from Corrupted Human Feedback Alexander Bukharin‡, Ilgee Hong‡, Haoming Jiang, Zichong Li‡, Qingru Zhang‡, Zixuan Zhang‡ and Tuo Zhao#Conference on Neural Information Processing Systems (NeurIPS), 2024 [arXiv] Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks Zixuan Zhang*‡, Kaiqi Zhang*, Minshuo Chen, Mengdi Wang, Tuo Zhao and Yuxiang Wang Conference on Neural Information Processing Systems (NeurIPS), 2024 [arXiv] Adaptive Preference S",
  "content_length": 33377,
  "method": "requests",
  "crawl_time": "2025-12-01 14:41:08"
}