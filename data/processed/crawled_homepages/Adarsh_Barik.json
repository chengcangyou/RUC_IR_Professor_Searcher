{
  "name": "Adarsh Barik",
  "homepage": "https://adarsh-barik.github.io",
  "status": "success",
  "content": "Adarsh's webpage Announcement I am actively seeking motivated PhD and master's students interested in working in the fields of optimization (both online and offline) and statistical learning theory. If you wish to apply, please send me an email with your CV. Undergraduate students from IIT Delhi are also welcome to apply for long-term projects. However, I am currently not offering short-term projects or internships. Due to the high volume of emails and limited availability, I may not be able to respond to every inquiry. About Me I am an Assistant Professor in the Department of Computer Science and Engineering at Indian Institute of Technology Delhi (IIT Delhi). I am also an Associate Faculty in the The Amar Nath and Shashi Khosla School of Information Technology . Prior to joining IIT Delhi, I was a Research Fellow in the Institute of Data Science at the National University of Singapore (NUS), where I was hosted by Prof. Vincent Y. F. Tan. I obtained my Ph.D. from Department of Computer Science at Purdue University with Prof. Jean Honorio as my advisor and completed my B.Tech and M.Tech (Dual Degree) from Indian Institute of Technology, Madras. Research Interests Theoretical and computational aspects of Optimization, Machine Learning, High-Dimensional Statistics, and Information Theory. I am interested in solving optimization problems in both online and offline setting. My main focus revolves around developing provably correct learning algorithms whith theoretical gurantees related to convergence, sample complexity and computational complexity for machine learning problems. News 2025: Gave a talk on \"Sequential Decision Making under Uncertainty: Game Theory, Parameter Free Algorithms, and Other New Results\" organized by the Indian Game Theory Society (IGTS) at IIT Delhi. 2025: Paper \"Parameter-free Algorithms for the Stochastically Extended Adversarial Model\" accepted to NeurIPS 2025. Joint work with Shuche Wang, Peng Zhao and Vincent Y.F. Tan. 2025: Paper \"A Sample Efficient Alternating Minimization-based Algorithm For Robust Phase Retrieval\" accepted to the IEEE Transactions on Information Theory. Joint work with Anand Krishna and Vincent Y.F. Tan. 2025: I have joined Department of Computer Science and Engineering at IIT Delhi as an Assistant Professor. Excited about working with amazing faculty and talented students here. 2025: Paper \"On Exact Solutions of the Inner Optimization Problem of Adversarial Robustness\" accepted to the IEEE ICASSP. Joint work with Deepak Maurya and Jean Honorio. 2025: Paper \"p-Mean Regret for Stochastic Bandits\" accepted to the AAAI. Joint work with Anand Krishna, Philips George John, and Vincent Y.F. Tan. See more Publications LEARN: An Invex Loss for Outlier Oblivious Robust Online Optimization We study a robust online convex optimization framework, where an adversary can introduce outliers by corrupting loss functions in an arbitrary number of rounds k, unknown to the learner. Our focus is on a novel setting allowing unbounded domains and large gradients for the losses without relying on a Lipschitz assumption. We introduce the Log Exponential Adjusted Robust and iNvex (LEARN) loss, a non-convex (invex) robust loss function to mitigate the effects of outliers and develop a robust variant of the online gradient descent algorithm by leveraging the LEARN loss. We establish tight regret guarantees (up to constants), in a dynamic setting, with respect to the uncorrupted rounds and conduct experiments to validate our theory. Furthermore, we present a unified analysis framework for developing online optimization algorithms for non-convex (invex) losses, utilizing it to provide regret bounds with respect to the LEARN loss, which may be of independent interest. Adarsh Barik, Anand Krishna, Vincent Y. F. Tan Preprint Parameter-free Algorithms for the Stochastically Extended Adversarial Model We develop the first parameter-free algorithms for the Stochastically Extended Adversarial (SEA) model, a framework that bridges adversarial and stochastic online convex optimization. Existing approaches for the SEA model require prior knowledge of problem-specific parameters, such as the diameter of the domain $D$ and the Lipschitz constant of the loss functions $G$, which limits their practical applicability. Addressing this, we develop parameter-free methods by leveraging the Optimistic Online Newton Step (OONS) algorithm to eliminate the need for these parameters. We first establish a comparator-adaptive algorithm for the scenario with unknown domain diameter but known Lipschitz constant, achieving an expected regret bound of $\\tilde{O}\\big(\\|u\\|_2^2 + \\|u\\|_2(\\sqrt{\\sigma^2_{1:T}} + \\sqrt{\\Sigma^2_{1:T}})\\big)$, where $u$ is the comparator vector and $\\sigma^2_{1:T}$ and $\\Sigma^2_{1:T}$ represent the cumulative stochastic variance and cumulative adversarial variation, respectively. We then extend this to the more general setting where both $D$ and $G$ are unknown, attaining the comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound exhibits the same dependence on $\\sigma^2_{1:T}$ and $\\Sigma^2_{1:T}$, demonstrating the efficacy of our proposed methods even when both parameters are unknown in the SEA model. Shuche Wang, Adarsh Barik, Peng Zhao, Vincent Y. F. Tan Accepted, NeurIPS 2025 A Sample Efficient Alternating Minimization-based Algorithm For Robust Phase Retrieval In this work, we study the robust phase retrieval problem where the task is to recover an unknown signal $\\theta^* \\in \\mathbb{R}^d$ in the presence of potentially arbitrarily corrupted magnitude-only linear measurements. We propose an alternating minimization approach that incorporates an oracle solver for a non-convex optimization problem as a subroutine. Our algorithm guarantees convergence to $\\theta^*$ and provides an explicit polynomial dependence of the convergence rate on the fraction of corrupted measurements. We then provide an efficient construction of the aforementioned oracle under a sparse arbitrary outliers model and offer valuable insights into the geometric properties of the loss landscape in phase retrieval with corrupted measurements. Our proposed oracle avoids the need for computationally intensive spectral initialization, using a simple gradient descent algorithm with a constant step size and random initialization instead. Additionally, our overall algorithm achieves nearly linear sample complexity, $\\mathcal{O}(d \\mathrm{polylog}(d))$. Adarsh Barik, Anand Krishna, Vincent Y. F. Tan Accepted, IEEE Transactions on Information Theory, 2025 On Exact Solutions of the Inner Optimization Problem of Adversarial Robustness We propose a robust framework that uses adversarially robust training to safeguard the ML models against perturbed testing data. Our contributions can be seen from both computational and statistical perspectives. Firstly, from a computational/optimization point of view, we derive the ready-to-use exact solution for several widely used loss functions with a variety of norm constraints on adversarial perturbation for various supervised and unsupervised ML problems, including regression, classification, two-layer neural networks, graphical models, and matrix completion. The solutions are either in closed-form, or an easily tractable optimization problem such as 1-D convex optimization, semidefinite programming, difference of convex programming or a sorting-based algorithm. Secondly, from statistical/generalization viewpoint, using some of these results, we derive novel bounds of the adversarial Rademacher complexity for various problems, which entails new generalization bounds. Thirdly, we perform some sanity-check experiments on real world datasets for supervised problems such as regression and classification, as well as for unsupervised problems such as matrix completion and learning graphical models. Deepak Maurya, Adarsh Barik, Jean Honorio Accepted, ICASSP 2025 p-Mean Regret for Stochastic Bandits In this work, we extend the concept of the $p$-mean welfare objective from social choice theory~\\cite{moulin2004fair} to study $p$-mean regret in stochastic multi-armed bandit problems. The $p$-mean regret, defined as the difference between the optimal mean among the arms and the $p$-mean of the expected rewards, offers a flexible framework for evaluating bandit algorithms, enabling algorithm designers to balance fairness and efficiency by adjusting the parameter $p$. Our framework encompasses both average cumulative regret and Nash regret as special cases. We introduce a simple, unified UCB-based algorithm (\\textsc{Explore-Then-UCB}) that achieves novel $p$-mean regret bounds. Our algorithm consists of two phases: a carefully calibrated uniform exploration phase to initialize sample means, followed by the UCB1 algorithm of~\\citet{auer2002finite}. Under mild assumptions, we prove that our algorithm achieves a $p$-mean regret bound of $\\tilde{O}\\left(\\sqrt{\\frac{k}{T^{\\frac{1}{2|p|}}}}\\right)$ for all $p \\leq -1$, where $k$ represents the number of arms and $T$ the time horizon. When $-1 < p < 0$, we achieve a regret bound of $\\tilde{O}\\left(\\sqrt{\\frac{k^{1.5}}{T^{\\frac{1}{2}}}}\\right)$. For the range $0 < p \\leq 1$, we achieve a $p$-mean regret scaling as $\\tilde{O}\\left(\\sqrt{\\frac{k}{T}}\\right)$, which matches the previously established lower bound up to logarithmic factors~\\cite{auer1995gambling}. This result stems from the fact that the $p$-mean regret of any algorithm is at least its average cumulative regret for $p \\leq 1$. In the case of Nash regret (the limit as $p$ approaches zero), our unified approach differs from prior work~\\cite{barman2023fairness}, which requires a new Nash Confidence Bound algorithm. Notably, we achieve the same regret bound up to constant factors using our more general method. Anand Krishna, Philips George John, Adarsh Barik, Vincent Y. F. Tan Accepted, AAAI 2025 An SDP Formulation for Minimizing p-th Order Controversy with Unknown Initial Opinions In ",
  "content_length": 23714,
  "method": "requests",
  "crawl_time": "2025-12-01 12:29:08"
}