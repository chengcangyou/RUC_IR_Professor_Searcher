{
  "name": "Murari Mandal",
  "homepage": "https://murarimandal.github.io",
  "status": "success",
  "content": "Dr. Murari Mandal 101-H, Campus 14 KIIT Bhubaneswar Odisha, India 751024 I am an Asst. Professor at the School of Computer Engineering, KIIT Bhubaneshwar. I lead the RespAI Lab where we focus on advancing large language models (LLMs) by addressing challenges related to long-content processing, inference efficiency, interpretability, and alignment. Our research also explores synthetic persona creation, regulatory issues, and innovative methods for model merging, knowledge verification, and unlearning. My research work has been published in ICML, KDD, AAAI, ACM MM, CVPR. Check out my research groupâ€™s website here: RespAI Lab. I regularly serve as a Reviewer to NeurIPS, ICML, ICLR, AAAI, CVPR, ICCV, and ECCV. Indexed in CSRankings. I am also part of the BrainX AI research community actively working on AI in Healthcare research! Research Impact: My pioneering works had significant impact in the field of Machine Unlearning with follow up works by Anthropic, Yoshua Bengio, Google Deepmind, etc. With 600+ citations, our works Fast Unlearning [TNNLS], Zero shot Unlearning [TIFS], and Bad Teacher [AAAI] are among top 10 highly cited papers in the field of Machine Unlearning. Earlier, I was a Postdoctoral Research Fellow at National University of Singapore (NUS). I worked with Prof. Mohan Kankanhalli in the School of Computing. Long time back, I graduated in 2011 with a Bachelors in Computer Science from BITS, Pilani. Find me on X @murari_ai. \"When you go to hunt, hunt for rhino. If you fail, people will say anyway it was very difficult. If you succeed, you get all the glory\" Note for prospective students interested in joining my research group. News Aug 21, 2025 Paper accepted to the EMNLP 2025 Main Track, Suzhou, China [Acceptance Rate - 22.16%]. Congratulations Debdeep ğŸ‰ğŸ‰ Aug 21, 2025 Paper accepted in EMNLP 2025 Findings, Suzhou, China [Acceptance Rate - 17.35%]. Congratulations Aakash ğŸ‰ğŸ‰ Jul 17, 2025 Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation has been accepted to Unlearning and Model Editing Workshop at ICCV 2025!ğŸ‰ Jul 08, 2025 Agents Are All You Need for LLM Unlearning has been accepted to #COLM2025!!ğŸ‰ Jun 22, 2025 Un-Star paper accepted in TMLR. May 31, 2025 Preprint of â€œOrgAccess: A Benchmark for Role Based Access Control in Organization Scale LLMs â€ is availableÂ onÂ Arxiv. May 25, 2025 Preprint of â€œInvestigating Pedagogical Teacher and Student LLM Agents: Genetic Adaptation Meets Retrieval Augmented Generation Across Learning Styleâ€ is availableÂ onÂ Arxiv. May 25, 2025 Preprint of â€œNine Ways to Break Copyright Law and Why Our LLM Wonâ€™t: A Fair Use Aligned Generation Framework â€ is availableÂ onÂ Arxiv. Apr 25, 2025 Excited to join BrainXAI team to work on cutting edge AI in Healthcare research! Apr 10, 2025 Preprint of â€œRight Prediction, Wrong Reasoning: Uncovering LLM Misalignment in RA Disease Diagnosisâ€ is availableÂ onÂ Arxiv. Mar 20, 2025 Preprint and Source Code of â€œGuardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generationâ€ is available! Mar 17, 2025 RespAI Lab offering â€œIntroduction to Large Language Modelsâ€ at KIIT Bhubaneswar this Spring 2025. Course Website - https://respailab.github.io/llm-101.respailab.github.io Feb 07, 2025 Preprint of â€œReviewEval: An Evaluation Framework for AI-Generated Reviewsâ€ is availableÂ onÂ Arxiv. Jan 20, 2025 Preprint of â€œALU: Agentic LLM Unlearningâ€ is availableÂ onÂ Arxiv. Dec 22, 2024 Invited talk on â€œMachine Unlearning for Responsible AIâ€ at IndoML 2024. Selected Publications Deep Regression Unlearning Ayush Kumar Tarun ,Â  Vikram Singh Chundawat ,Â  Murari Mandal , and 1 more author In Proceedings of the 40th International Conference on Machine Learning , 23â€“29 jul 2023 ICML-2023 Core A* source code Abs PDF With the introduction of data protection and privacy regulations, it has become crucial to remove the lineage of data on demand from a machine learning (ML) model. In the last few years, there have been notable developments in machine unlearning to remove the information of certain training data efficiently and effectively from ML models. In this work, we explore unlearning for the regression problem, particularly in deep learning models. Unlearning in classification and simple linear regression has been considerably investigated. However, unlearning in deep regression models largely remains an untouched problem till now. In this work, we introduce deep regression unlearning methods that generalize well and are robust to privacy attacks. We propose the Blindspot unlearning method which uses a novel weight optimization process. A randomly initialized model, partially exposed to the retain samples and a copy of the original model are used together to selectively imprint knowledge about the data that we wish to keep and scrub off the information of the data we wish to forget. We also propose a Gaussian fine tuning method for regression unlearning. The existing unlearning metrics for classification are not directly applicable to regression unlearning. Therefore, we adapt these metrics for the regression setting. We conduct regression unlearning experiments for computer vision, natural language processing and forecasting applications. Our methods show excellent performance for all these datasets across all the metrics. Source code: https://github.com/ayu987/deep-regression-unlearning EcoVal: An Efficient Data Valuation Framework for Machine Learning Ayush K Tarun ,Â  Vikram S Chundawat ,Â  Murari Mandal , and 3 more authors 23â€“29 jul 2024 KDD-2024 Core A* source code Abs PDF Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall data value can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as a \\textitproduction function, a concept which is popularly used to estimate the amount of output based on factors like labor and capital in a traditional free economic market. We provide a formal proof of our valuation technique and elucidate the principles and mechanisms that enable its accelerated performance. We demonstrate the real-world applicability of our method by showcasing its effectiveness for both in-distribution and out-of-sample data. This work addresses one of the core challenges of efficient data valuation at scale in machine learning models. Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks Using an Incompetent Teacher Vikram S Chundawat ,Â  Ayush K Tarun ,Â  Murari Mandal , and 1 more author Proceedings of the AAAI Conference on Artificial Intelligence, Jun 2023 AAAI-2023 Core A* source code Reachout to me via email @murari.nus@gmail.com.",
  "content_length": 7413,
  "method": "requests",
  "crawl_time": "2025-12-01 14:02:19"
}