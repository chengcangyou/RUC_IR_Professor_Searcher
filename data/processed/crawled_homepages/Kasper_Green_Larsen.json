{
  "name": "Kasper Green Larsen",
  "homepage": "https://cs.au.dk/~larsen",
  "status": "success",
  "content": "Kasper Green Larsen Photo: Tariq Mikkel Khan Kasper Green LarsenProfessor, Ph.D.Department of Computer ScienceAarhus UniversityContact InformationMail: larsen@cs.au.dk I'm a Full Professor at the Department of Computer Science at Aarhus University where I head the Section on Algorithms, Data and Artificial Intelligence as well as the research group on Algorithms, Data Structures and Foundations of Machine Learning. My research interests span many areas of theoretical computer science, including theory of machine learning, data structures, algorithms, lower bounds and cryptography. I also serve as a Senior Scientific Expert at the Quantum and HPC startup Kvantify.I received my Ph.D. from Aarhus University in 2013, under the supervision of Professor Lars Arge. I had the pleasure of being a member of the Young Academy under the Royal Danish Academy of Sciences and Letters from 2017-2022.My CV is here. You can find more information about my publications, awards, etc. further down this page. Awards: Fellow of the EATCS 2025 COLT Best Paper Award 2023 Presburger Award 2019 CRYPTO Best Paper Award 2018 Hartmann Diploma Prize 2017 Aarhus University Ph.D. Prize 2014 STOC Best Paper Award 2012 STOC Best Student Paper Award 2012 (The Danny Lewin Award) FOCS Best Student Paper Award 2011 (The Machtey Award) Danish Minister of Science's Elite Research (EliteForsk) Travel Scholarship 2011 Google European Doctoral Fellowship 2010 Links: Google Scholar Profile X/Twitter, BlueSky YouTube Channel CV Publications: Click on the titles to display abstracts.Note: In theoretical computer science, we always list authors alphabetically by last name. Publications not following this rule are marked with * at the end of their title. Tight Margin-Based Generalization Bounds for Voting Classifiers over Finite Hypothesis Sets We prove the first margin-based generalization bound for voting classifiers, that is asymptotically tight in the tradeoff between the size of the hypothesis set, the margin, the fraction of training points with the given margin, the number of training samples and the failure probability. Kasper Green Larsen, Natascha Schalburg Manuscript. Actively Learning Halfspaces without Synthetic Data In the classic point location problem, one is given an arbitrary dataset X⊂ℝd of n points with query access to an unknown halfspace f : ℝd→{0,1}, and the goal is to learn the label of every point in X. This problem is extremely well-studied and a nearly-optimal Õ(dlgn) query algorithm is known due to Hopkins-Kane-Lovett-Mahajan (FOCS 2020). However, their algorithm is granted the power to query arbitrary points outside of X (point synthesis), and in fact without this power there is an Ω(n) query lower bound due to Dasgupta (NeurIPS 2004). Nonetheless, query access to arbitrary synthesized data points is unrealistic in many contexts.     Our objective in this work is to design efficient algorithms for learning halfspaces without point synthesis. To circumvent the Ω(n) lower bound, we consider learning halfspaces whose normal vectors come from a known set of size D, and show tight bounds of Θ(D+lgn). As a corollary, we obtain an optimal O(d+lgn) query deterministic learner for the fundamental class of decision stumps (depth-one decision trees, or axis-aligned halfspaces), closing a previous gap of O(dlgn) vs. Ω(d+lgn) left open in the active learning literature. In fact, our algorithm solves the more general problem of learning a Boolean function f over n elements which is monotone under at least one of D provided orderings of these elements. Our technical insight is to exploit the structure in these orderings to essentially perform a binary search in parallel rather than considering each ordering sequentially, and we believe our approach may be of broader interest.     Furthermore, we use our exact learning algorithm to obtain nearly optimal algorithms for PAC-learning. We show that O(min(D+lg(1/ε),1/ε)·lgD) queries suffice to learn f within error ε, even in a setting when f can be adversarially corrupted on a cε-fraction of points, for a sufficiently small constant c. This bound is optimal up to a lgD factor, including in the realizable setting. Hadley Black, Kasper Green Larsen, Arya Mazumdar, Barna Saha, Geelon So Manuscript. Improved Replicable Boosting with Majority-of-Majorities We introduce a new replicable boosting algorithm which significantly improves the sample complexity compared to previous algorithms. The algorithm works by doing two layers of majority voting, using an improved version of the replicable boosting algorithm introduced by Impagliazzo et al. (2022) in the bottom layer. Kasper Green Larsen, Markus Engelund Mathiasen, Clement Svendsen Manuscript. An Exponential Separation Between Quantum and Quantum-Inspired Classical Algorithms for Linear Systems Achieving a provable exponential quantum speedup for an important machine learning task has been a central research goal since the seminal HHL quantum algorithm for solving linear systems and the subsequent quantum recommender systems algorithm by Kerenidis and Prakash. These algorithms were initially believed to be strong candidates for exponential speedups, but a lower bound ruling out similar classical improvements remained absent. In breakthrough work by Tang, it was demonstrated that this lack of progress in classical lower bounds was for good reasons. Concretely, she gave a classical counterpart of the quantum recommender systems algorithm, reducing the quantum advantage to a mere polynomial. Her approach is quite general and was named quantum-inspired classical algorithms. Since then, almost all the initially exponential quantum machine learning speedups have been reduced to polynomial via new quantum-inspired classical algorithms. From the current state-of-affairs, it is unclear whether we can hope for exponential quantum speedups for any natural machine learning task.     In this work, we present the first such provable exponential separation between quantum and quantum-inspired classical algorithms. We prove the separation for the basic problem of solving a linear system when the input matrix is well-conditioned and has sparse rows and columns. Allan Grønlund, Kasper Green Larsen Manuscript. Tight Generalization Bounds for Large-Margin Halfspaces We prove the first generalization bound for large-margin halfspaces that is asymptotically tight in the tradeoff between the margin, the fraction of training points with the given margin, the failure probability and the number of training points. Kasper Green Larsen, Natascha Schalburg NeurIPS'25: 39th Conference on Neural Information Processing Systems. Accepted as Spotlight paper (top 3.55% of submissions). Time/Space Tradeoffs for Generic Attacks on Delay Functions Delay functions have the goal of being inherently slow to compute. They have been shown to be useful for generating public randomness in distributed systems in the presence of a dishonest majority of network participants; a task that is impossible to solve without such functions, due to Cleve's seminal impossibility result (STOC 1986). Currently, little is known on how to construct secure delay functions or how to analyze the security of newly proposed candidate constructions.     In this work, we explore the time/space tradeoffs of generic attacks for a large class of potential delay function designs. We consider delay functions FT, which are computed as FT : =F(...F(F(x))...), where F : [N]→[N] is some round function invoked T times, in the presence of an adversary, who is given an advice string of some bounded size, has oracle access to F, and would like to compute FT on a random input x using less than T sequential oracle calls.     We show that for both random and arbitrary functions F there exist non-trivial adversaries, who successfully evaluate FT using only T/4 sequential calls to oracle F, when given a large enough advice string. We also show that there exist round functions F for which the adversary cannot compute FT using less than T/2 sequential queries, unless they receive a large advice string or they can perform a large number of oracle queries to F in parallel. Kasper Green Larsen, Mark Simkin TCC'25: 23rd Conference on Theory of Cryptography. A new lower bound for multi-color discrepancy with applications to fair division A classical problem in combinatorics seeks colorings of low discrepancy. More concretely, the goal is to color the elements of a set system so that the number of appearances of any color among the elements in each set is as balanced as possible. We present a new lower bound for multi-color discrepancy, showing that there is a set system with n subsets over a set of elements in which any k-coloring of the elements has discrepancy at least Ω((n/lnk)1/2). This result improves the previously best-known lower bound of Ω((n/k)1/2) of Doerr and Srivastav [2003] and may have several applications. Here, we explore its implications on the feasibility of fair division concepts for instances with n agents having valuations for a set of indivisible items. The first such concept is known as consensus 1/k-division up to d items (CDd) and aims to allocate the items into k bundles so that no matter which bundle each agent is assigned to, the allocation is envy-free up to d items. The above lower bound implies that CDd can be infeasible for d∈Ω((n/lnk)1/2). We furthermore extend our proof technique to show that there exist instances of the problem of allocating indivisible items to k groups of n agents in total so that envy-freeness and proportionality up to d items are infeasible for d∈Ω((n/(klnk))1/2) and d∈Ω((n/(k3lnk))1/2), respectively. The lower bounds for fair division improve the currently best-known ones by Manurangsi and Suksompong [2022]. Ioannis Caragiannis, Kasper Green Larsen, Sudarshan Shyam SAGT'25: 18th Symposium on Algorithmic Game Theory. Improved Margin Generalization Bounds for Voting Classifiers In this paper w",
  "content_length": 147272,
  "method": "requests",
  "crawl_time": "2025-12-01 13:40:41"
}