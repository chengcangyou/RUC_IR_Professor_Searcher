{
  "name": "Prashant Jayaprakash Nair",
  "homepage": "https://prashantnair.bitbucket.io",
  "status": "success",
  "content": "Prashant Nair Office: 4014 Fred Kaiser, UBC prashantnair [at] ece.ubc.ca ãCVã ãGoogle Scholarã Short Bio: Prashant Nair is an Assistant Professor at the University of British Columbia (UBC) where he leads the âSystems and Architectures (STAR) Labâ. He is also an Affiliate Fellow at the Quantum Algorithms Institute. His primary interests are in the areas Computer Architecture and Systems, Quantum Computing Systems, AI/ML Systems, Memory Systems, Security, and Reliability. Dr. Nair has published over 30 papers in top-tier venues such as ISCA, MICRO, HPCA, ASPLOS, DSN, SC, NeurIPS, and VLDB. He has received several awards, including the 2024 TCCA Young Architect Award, the Best Paper Award at HPCA 2023, two Honorable Mentions in IEEE MICRO Top-Picks, and the ECE Graduate Research Assistant Excellence Award at Georgia Tech. News Jul 20, 2024 Prof. Seokin Hongâs team and Dr. Nairâs paper on accelerating address translation on GPUs is accepted at MICRO 2024. Congratulations Dr. Hong and his team! Jul 20, 2024 Meng Wangâs paper on optimizing Variational Quantum Algorithms (VQAs) in the cloud is accepted at MICRO 2024. Congratulations Meng! Jul 02, 2024 Dr. Nair is honored with the TCCA Young Architect Award at ISCA 2024 May 26, 2024 Dr. Nair will be serving on the Program Committee for HPCA 2025 Apr 27, 2024 Muhammad Adnan is selected for the NSERC Postgraduate Fellowship - Doctoral program Most Recent Papers MICRO A Case for Speculative Address Translation with Rapid Validation for GPUs Junhyeok Park,Â Osang Kwon ,Â Yongho Lee ,Â Seongwook Kim,Â Gwangeun Byeon,Â Jihun Yoon,Â Prashant J. Nair,Â andÂ Seokin Hong In the Proceedings of the 57th IEEE/ACM International Symposium on Microarchitecture (MICRO) , 2024 Abs Bib HTML PDF Code Slides A unified address space is vital for heterogeneous systems as it enables efficient data sharing between CPUs and GPUs. However, GPU address translation faces challenges due to high TLB pressure, particularly with irregular and memory-intensive applications. We observe that, compared to an ideal scenario, address translation overheads cause a slowdown of up to 34.5% in modern heterogeneous systems. This paper introduces Avatar, a novel framework to accelerate address translation in GPUs. Avatar comprises two key components: Contiguity-Aware Speculative Translation (CAST) and In-Cache Validation (CAVA) mechanisms. Avatar identifies the potential for predicting virtual-to-physical address mapping by monitoring contiguous pages that lie in both virtual and physical address spaces. Leveraging this insight, CAST speculatively translates virtual addresses into physical addresses. This enables immediate data fetching into GPUs while addressing translation occurs in the background, reducing TLB-miss overhead. Unfortunately, modern GPUs lack support for speculative execution, which limits CASTâs performance gain. Data fetched from speculated physical addresses is unusable until validation. CAVA addresses this limitation by quickly validating speculated physical addresses. To this end, CAVA embeds page mapping information into each 32B sector of 128B cache lines. Thus, CAVA enables fetching a sector block from memory for a speculated address and rapidly validating the speculative translation using the embedded mapping information. Our experiments show that Avatar achieves a high speculation accuracy of 90.3% and improves GPU performance by 37.2% (on average). @inproceedings{park2024avatar, title = {A Case for Speculative Address Translation with Rapid Validation for GPUs}, author = {Park, Junhyeok and Kwon, Osang and Lee, Yongho and Kim, Seongwook and Byeon, Gwangeun and Yoon, Jihun and Nair, Prashant J. and Hong, Seokin}, booktitle = {Proceedings of the 57th IEEE/ACM International Symposium on Microarchitecture (MICRO)}, location = {Austin, USA}, series = {MICRO 2024}, year = {2024} } MICRO Qoncord: A Multi-Device Job Scheduling Framework for Variational Quantum Algorithms Meng Wang,Â Poulami Das,Â andÂ Prashant J. Nair In the Proceedings of the 57th IEEE/ACM International Symposium on Microarchitecture (MICRO) , 2024 Abs Bib HTML PDF Code Slides Quantum computers face challenges due to limited resources, particularly in cloud environments. Despite these obstacles, Variational Quantum Algorithms (VQAs) are considered promising applications for present-day Noisy Intermediate-Scale Quantum (NISQ) systems. VQAs require multiple optimization iterations to converge on a globally optimal solution. Moreover, these optimizations, known as restarts, need to be repeated from different points to mitigate the impact of noise. Unfortunately, the job scheduling policies for each VQA task in the cloud are heavily unoptimized. Notably, each VQA execution instance is typically scheduled on a single NISQ device. Given the variety of devices in the cloud, users often prefer higher-fidelity devices to ensure higher-quality solutions. However, this preference leads to increased queueing delays and unbalanced resource utilization. We propose Qoncord, an automated job scheduling framework to address these cloud-centric challenges for VQAs. Qoncordleverages the insight that not all training iterations and restarts are equal, Qoncord strategically divides the training process into exploratory and fine-tuning phases. Early exploratory iterations, more resilient to noise, are executed on less busy machines, while fine-tuning occurs on high-fidelity machines. This adaptive approach mitigates the impact of noise and optimizes resource usage and queuing delays in cloud environments. Qoncord also significantly reduces execution time and minimizes restart overheads by eliminating low-performance iterations. Thus, Qoncord offers similar solutions 17.4Ã faster. Similarly, it can offer 13.3% better solutions for the same time budget as the baseline. @inproceedings{wang2024vqacloud, title = {Qoncord: A Multi-Device Job Scheduling Framework for Variational Quantum Algorithms}, author = {Wang, Meng and Das, Poulami and Nair, Prashant J.}, booktitle = {Proceedings of the 57th IEEE/ACM International Symposium on Microarchitecture (MICRO)}, location = {Austin, USA}, series = {MICRO 2024}, year = {2024} } ISCA Heterogeneous Acceleration Pipeline for Recommendation System Training Muhammad Adnan,Â Yassaman Ebrahimzadeh Maboud,Â Divya Mahajan,Â andÂ Prashant J. Nair In the 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA) , 2024 Abs Bib HTML PDF Slides Recommendation models utilize deep learning networks and massive embedding tables, making them compute- and memory-intensive. Typically, these models are trained in hybrid CPU-GPU or GPU-only mode. The hybrid mode uses GPU to accelerate the neural network while employing the CPUs to store and supply the memory-intensive embedding tables. However, this approach can result in significant CPU-to-GPU transfer time. In contrast, the GPU-only mode stores embedding tables in the High Bandwidth Memory (HBM) across multiple GPUs. However, this approach requires GPU-to-GPU backend communication. This increases communication overheads as the number of GPUs and the size of embedding entries increases. This paper introduces a heterogeneous acceleration pipeline called Hotline to address these concerns. The Hotline accelerator uses the insight that only a few embedding entries are frequently accessed (popular) and develops a data-aware and model-aware scheduling pipeline. This pipeline utilizes CPU main memory for non-popular embeddings and GPUsâ HBM for popular embeddings. The Hotline accelerator fragments a single mini-batch into popular and non-popular micro-batches (Î¼-batches). The system then gathers the required working parameters for the non-popular Î¼-batches from the CPU while allowing GPUs to execute popular Î¼-batches. During training, the hardware accelerator dynamically stitches the execution of popular embeddings on GPUs and non-popular embeddings from the CPUâs main memory. Our real-world datasets and models studies show that Hotline reduces average training time by 2.2x compared to Intel-optimized CPU-GPU DLRM baseline. @inproceedings{adnan2024hotline, title = {Heterogeneous Acceleration Pipeline for Recommendation System Training}, author = {Adnan, Muhammad and Maboud, Yassaman Ebrahimzadeh and Mahajan, Divya and Nair, Prashant J.}, booktitle = {2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)}, location = {Buenos Aires, Argentina}, series = {ISCA 2024}, year = {2024}, pages = {1063-1079}, keywords = {Training;Deep learning;Computational modeling;Pipelines;Neural networks;Memory management;Graphics processing units;Recommender Systems;Multi-Node Distributed Training;Accelerators}, doi = {10.1109/ISCA59077.2024.00081}, } ASPLOS Red-QAOA: Efficient Variational Optimization through Circuit Reduction Meng Wang,Â Bo Fang,Â Ang Li,Â andÂ Prashant J. Nair In the Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 , 2024 Abs Bib HTML PDF Code Slides The Quantum Approximate Optimization Algorithm (QAOA) provides a quantum solution for combinatorial optimization problems. However, the optimal parameter searching process of QAOA is greatly affected by noise, leading to non-optimal solutions. This paper introduces a novel approach to optimize QAOA by exploiting the energy landscape concentration of similar instances via graph reduction, thus addressing the effect of noise. We formalize the notion of similar instances in QAOA and develop a Simulated Annealing-based graph reduction algorithm, called Red-QAOA, to identify the most similar subgraph for efficient parameter optimization. Red-QAOA outperforms state-of-the-art Graph Neural Network (GNN) based graph pooling techniques in performance and demonstrates effectiveness on a diverse set of real-world optimization problems encompassing 3200 graphs. Red-QAOA reduced the node ",
  "content_length": 27437,
  "method": "requests",
  "crawl_time": "2025-12-01 14:13:10"
}