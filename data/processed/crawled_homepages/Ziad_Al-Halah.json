{
  "name": "Ziad Al-Halah",
  "homepage": "https://users.cs.utah.edu/~ziad",
  "status": "success",
  "content": "Home - Ziad Al-Halah Ziad Al-Halah Assistant Professor Kahlert School of Computing The University of Utah 50 Central Campus Dr. Salt Lake City, UT 84112 ziad at cs.utah.edu cs.utah.edu Google Scholar ResearchGate DBLP ORCID Welcome I am an Assistant Professor of Computer Science in the Kahlert School of Computing at the University of Utah. Before that, I was Postdoctoral Fellow at the University of Texas at Austin, working with Prof. Kristen Grauman in the Computer Vision Group. I was a resident researcher with the Artificial Intelligence group at NAVER in South Korea, and a visiting researcher with the Coretx / Computer Vision group at Twitter in the UK and with the Vision group at Disney Research in Pittsburgh. I received my PhD with distinction (summa cum laude) from the Karlsruhe Institute of Technology in Germany, working in the Computer Vision for Human Computer Interaction Laboratory. My research interests are in the areas of computer vision and artificial intelligence. I'm particularly interested in transfer learning, zero-shot learning, multimodal learning, and embodied AI. Latest News ICCV 2025  Two papers accepted, on automatic view selection in videos, and on material-controlled RIR generation. CVPR 2025  [Highlight] Paper on weakly supervised view selection in videos. I'm an Area Chair of CVPR 2025 and NeurIPS 2025. CVPR 2024  Paper on learning spatial features from audio-visual correspondence. Outstanding Reviewer in NeurIPS 2024. I'm an organizer of the Computer Vision for Fashion, Art, and Design (CVFAD) workshop in CVPR 2024. I'm an organizer of the Ethical Considerations in Creative applications of Computer Vision (EC3V) workshop in CVPR 2024. ICML 2023  Paper on efficient video search for episodic memory. Outstanding Reviewer in CVPR 2023. CVPR 2023  Paper on leveraging linguistic narrations as queries to supervise episodic memory search. I'm an Area Chair of ICCV 2023 and AAAI 2023. NeurIPS 2022  Paper on arbitrary RIR prediction from few-shot multi-modal observations. I'm an Area Chair of BMVC 2022 and ACCV 2022. CVPR 2022  Two papers on zero-shot experience learning and object-goal navigation. ICLR 2022  Paper on environment predictive coding for visual navigation. ICCV 2021  Paper on active audio-visual source separation. Outstanding Reviewer in CVPR 2021. I'm an Area Chair of International Conference on Machine Vision Applications, MVA 2021. CVPR 2021  Two papers on semantic audio-visual navigation and dialog-based fashion retrieval. ICLR 2021  Paper on navigation using audio-visual waypoints. Publications How Would It Sound? Material-Controlled Multimodal Acoustic Profile Generation for Indoor Scenes Mahnoor Fatima Saad, Ziad Al-Halah International Conference on Computer Vision (ICCV), Oct. 2025. PDF BibTeX Project Abstract arXiv Supp @inproceedings{saad2025materialrir, author = {Mahnoor Fatima Saad and Ziad Al-Halah}, title = {{How Would It Sound? Material-Controlled Multimodal Acoustic Profile Generation for Indoor Scenes}}, year = {2025}, booktitle = {International Conference on Computer Vision (ICCV)}, month = {Oct.}, doi = {}, arxivId = {2508.02905},} How would the sound in a studio change with a carpeted floor and acoustic tiles on the walls? We introduce the task of material-controlled acoustic profile generation, where, given an indoor scene with specific audio-visual characteristics, the goal is to generate a target acoustic profile based on a user-defined material configuration at inference time. We address this task with a novel encoder-decoder approach that encodes the scene's key properties from an audio-visual observation and generates the target Room Impulse Response (RIR) conditioned on the material specifications provided by the user. Our model enables the generation of diverse RIRs based on various material configurations defined dynamically at inference time. To support this task, we create a new benchmark, the Acoustic Wonderland Dataset, designed for developing and evaluating material-aware RIR prediction methods under diverse and challenging settings. Our results demonstrate that the proposed model effectively encodes material information and generates high-fidelity RIRs, outperforming several baselines and state-of-the-art methods. Switch-a-View: View Selection Learned from Unlabeled In-the-wild Videos Sagnik Majumder, Tushar Nagarajan, Ziad Al-Halah, Kristen Grauman International Conference on Computer Vision (ICCV), Oct. 2025. PDF BibTeX Project Abstract arXiv Supp @inproceedings{majumder2025switchview, author = {Sagnik Majumder and Tushar Nagarajan and Ziad Al-Halah and Kristen Grauman}, title = {{Switch-a-View: View Selection Learned from Unlabeled In-the-wild Videos}}, year = {2025}, booktitle = {International Conference on Computer Vision (ICCV)}, month = {Oct.}, doi = {}, arxivId = {2412.18386},} We introduce Switch-a-View, a model that learns to automatically select the viewpoint to display at each timepoint when creating a how-to video. The key insight of our approach is how to train such a model from unlabeled--but human-edited--video samples. We pose a pretext task that pseudo-labels segments in the training videos for their primary viewpoint (egocentric or exocentric), and then discovers the patterns between the visual and spoken content in a how-to video on the one hand and its view-switch moments on the other hand. Armed with this predictor, our model can be applied to new multi-view video settings for orchestrating which viewpoint should be displayed when, even when such settings come with limited labels. We demonstrate our idea on a variety of real-world videos from HowTo100M and Ego-Exo4D, and rigorously validate its advantages. Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos Sagnik Majumder, Tushar Nagarajan, Ziad Al-Halah, Reina Pradhan, Kristen Grauman IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2025. Highlight. PDF BibTeX Project Abstract arXiv Supp @inproceedings{majumder2025whichview, author = {Sagnik Majumder and Tushar Nagarajan and Ziad Al-Halah and Reina Pradhan and Kristen Grauman}, title = {{Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos}}, year = {2025}, booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {Jun.}, doi = {}, arxivId = {2411.08753},} Given a multi-view video, which viewpoint is most informative for a human observer? Existing methods rely on heuristics or expensive 'best-view' supervision to answer this question, limiting their applicability. We propose a weakly supervised approach that leverages language accompanying an instructional multi-view video as a means to recover its most informative viewpoint(s). Our key hypothesis is that the more accurately an individual view can predict a viewagnostic text summary, the more informative it is. To put this into action, we propose a framework that uses the relative accuracy of view-dependent caption predictions as a proxy for best view pseudo-labels. Then, those pseudo-labels are used to train a view selector, together with an auxiliary camera pose predictor that enhances view-sensitivity. During inference, our model takes as input only a multi-view video--no language or camera poses--and returns the best viewpoint to watch at each timestep. On two challenging datasets comprised of diverse multi-camera setups and how-to activities, our model consistently outperforms state-of-the-art baselines, both with quantitative metrics and human evaluation. Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos Sagnik Majumder, Ziad Al-Halah, Kristen Grauman IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2024. PDF BibTeX Project DOI Abstract arXiv Supp @inproceedings{majumder2024egoav, author = {Sagnik Majumder and Ziad Al-Halah and Kristen Grauman}, title = {{Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos}}, year = {2024}, booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {Jun.}, doi = {10.1109/cvpr52733.2024.02555}, arxivId = {2307.04760},} We propose a self-supervised method for learning representations based on spatial audio-visual correspondences in egocentric videos. Our method uses a masked auto-encoding framework to synthesize masked binaural audio through the synergy of audio and vision, thereby learning useful spatial relationships between the two modalities. We use our pretrained features to tackle two downstream video tasks requiring spatial understanding in social scenarios: active speaker detection and spatial audio denoising. Through extensive experiments, we show that our features are generic enough to improve over multiple state-of-the-art baselines on both tasks on two challenging egocentric video datasets that offer binaural audio, EgoCom and Easy-Com. SpotEM: Efficient Video Search for Episodic Memory Santhosh K. Ramakrishnan, Ziad Al-Halah, Kristen Grauman International Conference on Machine Learning (ICML), Jul. 2023. PDF BibTeX Project Abstract arXiv @inproceedings{ramakrishnan2023soptem, author = {Santhosh K. Ramakrishnan and Ziad Al-Halah and Kristen Grauman}, title = {{SpotEM: Efficient Video Search for Episodic Memory}}, year = {2023}, booktitle = {International Conference on Machine Learning (ICML)}, month = {Jul.}, doi = {}, arxivId = {2306.15850},} The goal in episodic memory (EM) is to search a long egocentric video to answer a natural language query (e.g., ``where did I leave my purse?''). Existing EM methods exhaustively extract expensive fixed-length clip features to look everywhere in the video for the answer, which is infeasible for long wearable camera videos that span hours or even days. We propose SpotEM, an approach to achieve efficiency for a given EM method while maintaining good accuracy. SpotEM consists of three key ideas: a novel c",
  "content_length": 65997,
  "method": "requests",
  "crawl_time": "2025-12-01 14:55:51"
}