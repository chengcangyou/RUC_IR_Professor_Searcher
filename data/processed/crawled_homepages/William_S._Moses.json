{
  "name": "William S. Moses",
  "homepage": "https://wsmoses.com",
  "status": "success",
  "content": "Academic - Life, the Universe, and Everything: The adventures of William (Billy) Moses Papers Presentations Posters William S. Moses Assistant Professor, University of Illinois Urbana-Champaign (UIUC) I'm an assistant professor at UIUC in the Computer Science and Electrical and Computer Engineering departments, and researcher at Google Deepmind/Cloud. Previously, I was a J. Tinsley Oden Faculty Fellow at the University of Texas, Austin. I recieved my PhD, MEng, and SB at MIT in computer science and physics. Before that, I attended Thomas Jefferson High School for Science and Technology (TJHSST) in Northern Virginia. My group has multiple PhD positions available for Fall 2025. If you are interested in compilers or high performance computing, including with applications to machine learning, climate science, databases, security, or biology, please reach out and apply to the Computer Science department at UIUC (I can also advise students from other departments). [email protected] 703-638-2387 UIUC Siebel Center for Computer Science 201 N Goodwin Ave Room 4128, Urbana, IL 61801 Papers The MLIR Transform Dialect. Your compiler is more powerful than you think Lücke, Martin Paul and Zinenko, Oleksandr and Moses, William S. and Steuwer, Michel and Cohen, Albert. CGO’25. To take full advantage of a specific hardware target, performance engineers need to gain control on compilers in order to leverage their domain knowledge about the program and hardware. Yet, modern compilers are poorly controlled, usually by configuring a sequence of coarse-grained monolithic black-box passes, or by means of predefined compiler annotations/pragmas. These can be effective, but often do not let users precisely optimize their varying compute loads. As a consequence, performance engineers have to resort to implementing custom passes for a specific optimization heuristic, requiring compiler engineering expert knowledge. In this paper, we present a technique that provides fine-grained control of general-purpose compilers by introducing the Transform dialect, a controllable IR-based transformation system implemented in MLIR. The Transform dialect empowers performance engineers to optimize their various compute loads by composing and reusing existing - but currently hidden - compiler features without the need to implement new passes or even rebuilding the compiler. We demonstrate in five case studies that the Transform dialect enables precise, safe composition of compiler transformations and allows for straightforward integration with state-of-the-art search methods. @misc{transformcgo, title = {The MLIR Transform Dialect. Your compiler is more powerful than you think}, author = {Lücke, Martin Paul and Zinenko, Oleksandr and Moses, William S. and Steuwer, Michel and Cohen, Albert}, booktitle = {2025 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, year = {2025}, volume = {}, issn = {}, pages = {119-132}, publisher = {ACM}, month = mar, shortname = {CGO'25}, pdf = {https://arxiv.org/pdf/2409.03864} } Input-Gen: Guided Generation of Stateful Inputs for Testing, Tuning, and Training Ivanov, Ivan R. and Meyer, Joachim and Grossman, Aiden and Moses, William S. and Doerfert, Johannes. arXiv. The size and complexity of software applications is increasing at an accelerating pace. Source code repositories (along with their dependencies) require vast amounts of labor to keep them tested, maintained, and up to date. As the discipline now begins to also incorporate automatically generated programs, automation in testing and tuning is required to keep up with the pace – let alone reduce the present level of complexity. While machine learning has been used to understand and generate code in various contexts, machine learning models themselves are trained almost exclusively on static code without inputs, traces, or other execution time information. This lack of training data limits the ability of these models to understand real-world problems in software. In this work we show that inputs, like code, can be generated automatically at scale. Our generated inputs are stateful, and appear to faithfully reproduce the arbitrary data structures and system calls required to rerun a program function. By building our tool within the compiler, it both can be applied to arbitrary programming languages and architectures and can leverage static analysis and transformations for improved performance. Our approach is able to produce valid inputs, including initial memory states, for 90% of the ComPile dataset modules we explored, for a total of 21.4 million executable functions. Further, we find that a single generated input results in an average block coverage of 37%, whereas guided generation of five inputs improves it to 45%. @misc{ivanov2024inputgenguidedgenerationstateful, title = {Input-Gen: Guided Generation of Stateful Inputs for Testing, Tuning, and Training}, author = {Ivanov, Ivan R. and Meyer, Joachim and Grossman, Aiden and Moses, William S. and Doerfert, Johannes}, year = {2024}, eprint = {2406.08843}, archiveprefix = {arXiv}, primaryclass = {cs.SE}, url = {https://arxiv.org/abs/2406.08843}, pdf = {https://arxiv.org/pdf/2406.08843}, shortname = {arXiv} } Retargeting and Respecializing GPU Workloads for Performance Portability Ivanov, Ivan R. and Zinenko, Oleksandr and Domke, Jens and Endo, Toshio and Moses, William S.. CGO’24. The size and complexity of software applications is increasing at an accelerating pace. Source code repositories (along with their dependencies) require vast amounts of labor to keep them tested, maintained, and up to date. As the discipline now begins to also incorporate automatically generated programs, automation in testing and tuning is required to keep up with the pace – let alone reduce the present level of complexity. While machine learning has been used to understand and generate code in various contexts, machine learning models themselves are trained almost exclusively on static code without inputs, traces, or other execution time information. This lack of training data limits the ability of these models to understand real-world problems in software. In this work we show that inputs, like code, can be generated automatically at scale. Our generated inputs are stateful, and appear to faithfully reproduce the arbitrary data structures and system calls required to rerun a program function. By building our tool within the compiler, it both can be applied to arbitrary programming languages and architectures and can leverage static analysis and transformations for improved performance. Our approach is able to produce valid inputs, including initial memory states, for 90% of the ComPile dataset modules we explored, for a total of 21.4 million executable functions. Further, we find that a single generated input results in an average block coverage of 37%, whereas guided generation of five inputs improves it to 45%. @inproceedings{10444828, author = {Ivanov, Ivan R. and Zinenko, Oleksandr and Domke, Jens and Endo, Toshio and Moses, William S.}, booktitle = {2024 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, title = {Retargeting and Respecializing GPU Workloads for Performance Portability}, year = {2024}, volume = {}, issn = {}, pages = {119-132}, doi = {10.1109/CGO57630.2024.10444828}, url = {https://doi.ieeecomputersociety.org/10.1109/CGO57630.2024.10444828}, publisher = {IEEE Computer Society}, address = {Los Alamitos, CA, USA}, month = mar, shortname = {CGO'24}, pdf = {https://c.wsmoses.com/papers/polygeist24.pdf} } ComPile: A Large IR Dataset from Production Sources Grossman, Aiden and Paehler, Ludger and Parasyris, Konstantinos and Ben-Nun, Tal and Hegna, Jacob and Moses, William and Diaz, Jose M Monsalve and Trofin, Mircea and Doerfert, Johannes. arXiv. Code is increasingly becoming a core data modality of modern machine learning research impacting not only the way we write code with conversational agents like OpenAI’s ChatGPT, Google’s Bard, or Anthropic’s Claude, the way we translate code from one language into another, but also the compiler infrastructure underlying the language. While modeling approaches may vary and representations differ, the targeted tasks often remain the same within the individual classes of models. Relying solely on the ability of modern models to extract information from unstructured code does not take advantage of 70 years of programming language and compiler development by not utilizing the structure inherent to programs in the data collection. This detracts from the performance of models working over a tokenized representation of input code and precludes the use of these models in the compiler itself. To work towards the first intermediate representation (IR) based models, we fully utilize the LLVM compiler infrastructure, shared by a number of languages, to generate a 182B token dataset of LLVM IR. We generated this dataset from programming languages built on the shared LLVM infrastructure, including Rust, Swift, Julia, and C/C++, by hooking into LLVM code generation either through the language’s package manager or the compiler directly to extract the dataset of intermediate representations from production grade programs. Statistical analysis proves the utility of our dataset not only for large language model training, but also for the introspection into the code generation process itself with the dataset showing great promise for machine-learned compiler components. @article{grossman2023compile, title = {ComPile: A Large IR Dataset from Production Sources}, author = {Grossman, Aiden and Paehler, Ludger and Parasyris, Konstantinos and Ben-Nun, Tal and Hegna, Jacob and Moses, William and Diaz, Jose M Monsalve and Trofin, Mircea and Doerfert, Johannes}, journal = {arXiv preprint arXiv:2309.15432}, year = {2023}, shortname = {arXiv}, pdf = {https://arxiv.org/pdf/2309.15432.pdf} } The Quantum Tortoise and the Classical Hare: A simple framework for",
  "content_length": 68043,
  "method": "requests",
  "crawl_time": "2025-12-01 14:47:04"
}