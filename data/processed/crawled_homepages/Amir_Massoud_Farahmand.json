{
  "name": "Amir Massoud Farahmand",
  "homepage": "https://academic.sologen.net",
  "status": "success",
  "content": "Amir-massoud Farahmand – Reinforcement/Machine Learning Researcher Skip to content Associate Professor at the Department of Computer and Software Engineering, Polytechnique MontréalMila, Core Academic Member Associate Professor (Status-Only) at the Department of Computer Science, University of Toronto Research Goal: Understanding the computational and statistical mechanisms required to design efficient RL agents that interact with their environment and adaptively improve their long-term performance. Refer to Research and Publications for more information on my research, and to Bio and CV for more information about my academic background. Links Email: {my last name} at cs.toronto.edu Google Scholar CV (November 2024) and Bio Statements: Research (2024): Rethinking Reinforcement Learning Teaching (2024): Nurturing Expertise in ML and RL through Growth Mindset and Deliberate Practice EDI (2024): Empowering Intellectual Potentials: Removing Barriers and Creating Opportunities Prospective students (tldr: I will recruit grad students at Polytechnique Montréal and Mila for September 2025.) Regularization in Reinforcement Learning (PhD Dissertation) SoloGen@Twitter and SoloGenBlog@Twitter (Persian) SoloGen.bsky.social Blog: Anti Memoirs (Persian) [dysfunctional now!] (2024 Fall) I joined Polytechnique Montréal as an associate professor and Mila as a core academic member. (2024 Winter) I got Ontario Early Research Awards (ERA) on Accelerated Reinforcement Learning Algorithms. (2024 Winter) I taught the undergraduate course on Neural Networks and Deep Learning at DCS, U of T. This was my last course at the University of Toronto. (2022-2024) I did not keep the news updated from 2022 Fall until 2024. Many things have happened! Check out our papers instead! (2022 Fall) Tyler Kastner joined my team and Murat Erdogdu’s as a PhD student. Welcome! (2022 Fall) I taught the graduate level Introduction to Machine Learning course at DCS. This is a slightly updated version of the Fall 2021. (2022 Summer) Farnam Mansouri did his MSc on risk-aware RL, and joined University of Waterloo afterwards. Stay tuned for some of his interesting results! (2021 Fall) I taught the graduate level Introduction to Machine Learning course at DCS. I recorded the lectures, which you can find at this YouTube playlist. (2021 Spring & Fall) Allen Bao, an MScAc student, joined my lab in Spring, and in collaboration with AMD worked on Gameplay Test Automation with Reinforcement Learning. He graduated in the Fall. Congratulations! He is currently Senior Software Development Engineer (ML) at AMD. (2021 Summer) Dr. Yangchen Pan defended his PhD! He is my first graduated PhD student and I am very proud of him. He is currently a Departmental Lecturer at the University of Oxford. Congratulations on both achievements! (2021-2023) I have not updated this place between 2021 Spring until 2023 Spring. I retroactively add a few important news above. (2021 Spring) I developed and taught a new course on Reinforcement Learning. All videos can be accessed through this playlist. The course is accompanied by the Lecture Notes in Reinforcement Learning. Continue reading “News” Textbook I wrote an extensive lecture notes on reinforcement learning when I taught my Introduction to Reinforcement Learning course in Spring 2021. The original version was titled Lecture Notes on Reinforcement Learning (LNRL) (2021). I then decided to expand it and make it a complete textbook. Currently (2025), I am in the process of revising it. I will post an updated version every so often: A.M. Farahmand, Foundations of Reinforcement Learning (draft), 2025. Note that I will be releasing chapters after finishing a major revisions on each of them, so some chapters may not be there yet. You can take a look at the LNRL if you want to see the unrevised chapters. The textbook is introductory in the sense that it does not assume prior exposure to reinforcement learning. It is not, however, focused on being a collection of algorithms or only providing high-level intuition. Instead, it tries to build the mathematical intuition behind many important ideas and concepts often encountered in RL. We prove many basic, or sometimes not so basic, results in RL. If you are a university instructor and wish to use slides for your own course, please contact me. Papers This list is regularly updated. You can also check my Google Scholar page. 2025 Amin Rakhsha, Kanika Madan, Tianyu Zhang, A.M. Farahmand, Amir Khasahmadi, “Majority of the Bests: Improving Best-of-N via Bootstrapping,” Accepted at Neural Information Processing Systems (NeurIPS), 2025. (OpenReview) Mete Kemertas, Allan Jepson, and A.M. Farahmand, “Efficient and Accurate Optimal Transport with Mirror Descent and Conjugate Gradients,” Transactions on Machine Learning Research (TMLR), 2025. (PDF; OpenReview; Code) [See also this ICLR 2025 paper.] Avery Ma, Yangchen Pan, A.M. Farahmand, “PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling,” International Conference on Machine Learning (ICML), 2025. (PDF; arXiv) Tyler Kastner, Mark Rowland, Yunhao Tang, Murat Erdogdu, A.M. Farahmand, “Categorical Distributional Reinforcement Learning with Kullback-Leibler Divergence: Convergence and Asymptotics,” International Conference on Machine Learning (ICML), 2025. (PDF) Claas Voelcker, Anastasiia Pedan, Arash Ahmadian, Romina Abachi, Igor Gilitschenski, A.M. Farahmand, “Calibrated Value-Aware Model Learning with Probabilistic Environment Models,” International Conference on Machine Learning (ICML), 2025. (PDF; arXiv) Jongmin Lee, Amin Rakhsha, Ernest K. Ryu, and A.M. Farahmand, “Deflated Dynamics Value Iteration,” Transactions on Machine Learning Research (TMLR), 2025. (PDF; OpenReview; Code) Mete Kemertas, A.M. Farahmand, and Allan Jepson, “A Truncated Newton Method for Optimal Transport,” International Conference on Learning Representations (ICLR), 2025. (PDF; OpenReview; Code) Claas Voelcker, Marcel Hussing, Eric Eaton, A.M. Farahmand, and Igor Gilitschenski, “MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL,” International Conference on Learning Representations (ICLR), 2025. (PDF; OpenReview; arXiv) 2024 Amin Rakhsha, Mete Kemertas, Mohammad Ghavamzadeh, and A.M. Farahmand, “Maximum Entropy Model Correction in Reinforcement Learning,” International Conference on Learning Representations (ICLR), 2024. (PDF; arXiv; OpenReview) Mark Bedaywi, Amin Rakhsha, A.M. Farahmand, “PID Accelerated Temporal Difference Algorithms,” Reinforcement Learning Conference (RLC), 2024. (PDF; arXiv; Code) Claas Voelcker, Tyler Kastner, Igor Gilitschenski, A.M. Farahmand, “When does Self-Prediction help? Understanding Auxiliary Tasks in Reinforcement Learning,” Reinforcement Learning Conference (RLC), 2024. (PDF; arXiv) Marcel Hussing, Claas Voelcker, Igor Gilitschenski, and A.M. Farahmand, Eric Eaton “Dissecting Deep RL with High Update Ratios: Combatting Value Overestimation and Divergence,” Reinforcement Learning Conference (RLC), 2024. (PDF; arXiv) Avery Ma, A.M. Farahmand, Yangchen Pan, Philip Torr, Jindong Gu, “Improving Adversarial Transferability via Model Alignment,” European Conference on Computer Vision (ECCV), 2024. (PDF; arXiv; Code) 2023 Tyler Kastner, Murat A. Erdogdu, and A.M. Farahmand, “Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning,” Neural Information Processing Systems (NeurIPS), 2023. (PDF; OpenReview) Avery Ma, Yangchen Pan, and A.M. Farahmand, “Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods,” Transactions on Machine Learning Research (TMLR), 2023. (PDF; OpenReview; Code) [Featured certificate: ~3.5% of accepted papers] Claas A. Voelcker, Arash Ahmadian, Romina Abachi, Igor Gilitschenski, and A.M. Farahmand, “λ-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces,” 2023. (arXiv) Mete Kemertas, Allan Jepson, and A.M. Farahmand, “Efficient and Accurate Optimal Transport with Mirror Descent and Conjugate Gradients”, 2023. (arXiv) [Published in 2025 in TMLR.] 2022 Amin Rakhsha, Andrew Wang, Mohammad Ghavamzadeh, and A.M. Farahmand, “Operator Splitting Value Iteration,” Neural Information Processing Systems (NeurIPS), 2022. (PDF; OpenReview) Claas A. Voelcker, Victor Liao, Animesh Garg, and A.M. Farahmand, “Value Gradient Weighted Model-Based Reinforcement Learning,” International Conference on Learning Representations (ICLR), 2022. (PDF; OpenReview) Guiliang Liu, Ashutosh Adhikari, A.M. Farahmand, and Pascal Poupart, “Learning Object-Oriented Dynamics for Planning from Text,” International Conference on Learning Representations (ICLR), 2022. (PDF; OpenReview; GitHub) Jincheng Mei, Yangchen Pan, A.M. Farahmand, Martha White, Hengshuai Yao, Mohsen Rohani and, Jun Luo, “Understanding and Mitigating the Limitations of Prioritized Replay,” Conference on Uncertainty in Artificial Intelligence (UAI), 2022. (PDF) 2021 A.M. Farahmand and Mohammad Ghavamzadeh, “PID Accelerated Value Iteration Algorithm,” International Conference on Machine Learning (ICML), 2021. (PDF; Extended Version PDF) Erfan Pirmorad, Faraz Khoshbakhtian, Farnam Mansouri, A.M. Farahmand, “Deep Reinforcement Learning for Online Control of Stochastic Partial Differential Equations,” NeurIPS Workshop on the Symbiosis of Deep Learning and Differential Equations, 2021. (PDF; OpenReview) 2020 Yangchen Pan, Jincheng Mei, A.M. Farahmand, “Frequency-based Search-Control in Dyna,” International Conference on Learning Representations (ICLR), 2020. (PDF) Yangchen Pan, Ehsan Imani, A.M. Farahmand, Martha White, “An Implicit Function Learning Approach for Regression,” Neural Information Processing Systems (NeurIPS), 2020. (PDF on arXiv) Romina Abachi, Mohammad Ghavamzadem, A.M. Farahmand, “Policy-Aware Model Learning for Policy-Gradient Methods,”  2020. (arXiv) Avery Ma, Fartash Faghri, Nicolas Papernot, A.M. Farahmand, “SOAR",
  "content_length": 43172,
  "method": "requests",
  "crawl_time": "2025-12-01 12:55:34"
}