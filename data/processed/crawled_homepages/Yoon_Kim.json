{
  "name": "Yoon Kim",
  "homepage": "https://people.csail.mit.edu/yoonkim",
  "status": "success",
  "content": "Yoon Kim Yoon Kim I am an associate professor at MIT (EECS/CSAIL). I obtained my PhD in computer science from Harvard University, where I was advised by Alexander Rush. yoonkim@mit.edu / CV / Google Scholar Research I work on natural language processing and machine learning. Current interests include: Efficient training and deployment of large-scale models Understanding the limitations and enhancing the capabilities of AI systems Group Postdocs Hadeel Al-Negheimish PhD Students Lucas Torroba Hennigen Han Guo (co-advised with Eric Xing) Ani Nrusimha Abbas Zeitoun Linlu Qiu Zhaofeng Wu Songlin Yang Isha Puri (co-advised with Marzyeh Ghassemi) Oliver Sieberling Former Members Bailin Wang (Postdoc --> RS at Apple) Tiwa Eisape (PhD --> Postdoc at Princeton) Recent Papers [all publications] PaTH Attention: Position Encoding via Accumulating Householder Transformations Songlin Yang, Yikang Shen, Kaiyue Wen, Shawn Tan, Mayank Mishra, Liliang Ren, Rameswar Panda, Yoon Kim arXiv preprint [paper] Log-Linear Attention Han Guo, Songlin Yang, Tarushii Goel, Eric P. Xing, Tri Dao, Yoon Kim arXiv preprint [paper, code] On the Duality between Gradient Transformations and Adapters Lucas Torroba-Hennigen, Hunter Lang, Han Guo, Yoon Kim ICML 2025 [paper] The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities Zhaofeng Wu, Xinyan Velocity Yu, Dani Yogatama, Jiasen Lu, Yoon Kim ICLR 2025 [paper] Parallelizing Linear Transformers with the Delta Rule over Sequence Length Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, Yoon Kim NeurIPS 2024 [paper, slides, blog] Learning to Decode Collaboratively with Multiple Language Models Shannon Zejiang Shen, Hunter Lang, Bailin Wang, Yoon Kim, David Sontag ACL 2024 [paper, code] What Do Language Models Hear? Probing for Auditory Representations in Language Models Jerry Ngo, Yoon Kim ACL 2024 [paper] Gated Linear Attention Transformers with Hardware-Efficient Training Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim ICML 2024 [paper, slides, code] In-Context Language Learning: Architectures and Algorithms Ekin Akyürek, Bailin Wang, Yoon Kim, Jacob Andreas ICML 2024 [paper, code] Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, Yoon Kim NAACL 2024\t[paper, code] LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning Han Guo, Philip Greengard, Eric P. Xing, Yoon Kim ICLR 2024 [paper, code]",
  "content_length": 2620,
  "method": "requests",
  "crawl_time": "2025-12-01 14:52:37"
}