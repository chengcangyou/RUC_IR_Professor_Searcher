{
  "name": "Weiyang Liu",
  "homepage": "https://wyliu.com",
  "status": "success",
  "content": "Weiyang Liu Home Students Papers Contact Weiyang Liu Google Scholar    Github    Twitter The Chinese University of Hong Kong Max Planck Institute for Intelligent Systems About Me I am an assistant professor in Computer Science and Engineering at The Chinese University of Hong Kong, heading the Scalable Principles for Learning and Reasoning Lab (SphereLab). I am also affiliated as a researcher with Max Planck Institute for Intelligent Systems. Previously, I did my postdoc at Max Planck Institute for Intelligent Systems with Bernhard Schölkopf. I have received a Ph.D. in Machine Learning from University of Cambridge, and a Ph.D. in Computer Science from Georgia Tech. My advisors were Adrian Weller, Bernhard Schölkopf and Le Song. I have also spent wonderful time at Google, Nvidia, and MERL. I work primarily on principled modeling of inductive bias in learning algorithms. My research seeks to understand how inductive bias affects generalization, and to develop \"light-yet-sweet\" learning algorithms: (i) light: conceptually simple in methodology and easy to implement in practice, (ii) sweet: having clear intuitions and non-trivial theoretical guarantees. Over the years, I always find myself fascinated by geometric invariance, symmetry, structures and how they can benefit generalization as guiding principles. Recently, I start rethinking inductive bias for foundation models, and develop a deep interest in large language models and generative modeling across visual, textual, and physical domains. My current research focuses on developing principled algorithms for training and adapting foundation models, as in OPT, OFT(v1,v2), BOFT, POET, VML; understanding how LLMs perform reasoning and eliciting it in verifiable scenarios (formal/math/symbolic reasoning), as in MetaMath, SGP-Bench, FormalMATH, BesiegeField. I always believe in two principles in my research: (i) insight must precede application, and (ii) everything should be made as simple as possible, but not simpler. I try to follow certain research values. - Focus on creating novel ideas, not publishing papers - Follow curiosity and passion, not trends - Ideas are not owned, but come with debts to those who came before - Ideas become stronger when shared, discussed and criticized - Life is surprisingly short, so solve problems that interest and excite you most - It is good to be quick, but it is more important to be deep - Think like an amateur, do as an expert - This is not only about how to do research, but also how to live your life Students I take great pleasure to work with a group of highly motivated students. Interested in joining? Make sure to read this first. - Thanks for your interest in joining us! See our group's recent focus. - I am always looking for motivated Postdoc, PhD students and visitors/interns. - Due to the high volume of emails, I apologize if I haven't responded to your email. - Solid math/engineering and good communication skills are necessary. - NO need to email me. Fill out this form and directly apply here (and mention my name). Ph.D. students - Zeju Qiu (with Bernhard Schölkopf) - Jiacheng Chen (with Yu Cheng) - Yamei Chen - He Guo - Yangyi Huang - Siyuan Ma (with Yandong Wen) - Kexuan Shi - Zhouliang Yu - Haoquan Zhang Visiting students - Tim Z. Xiao Alumni - Yamei Chen (2024): research intern - M.S. student at Technical University of Munich - Next: Ph.D. student in my group - Gege Gao (2023 - 2024): research intern - Ph.D. student at University of Tübingen - Zeju Qiu (2022 - 2024): master thesis student - M.S. at Technical University of Munich - Next: Ph.D. student in my group - Longhui Yu (2022 - 2024): research intern - M.S. at Peking University, Ph.D. offers from Caltech, University of Toronto - Next: Researcher at Kimi AI - Zhen Liu (2017 - 2019, 2022 - 2024): research intern - M.S. at Georgia Tech → Ph.D. at Mila & University of Montreal - Next: Assistant Professor at The Chinese University of Hong Kong, Shenzhen Recent Highlight Model Merging with Functional Dual Anchors Kexuan Shi, Yandong Wen, Weiyang Liu Preprint 2025 arXiv | code | project | bib @article{shi2025fda, title={Model Merging with Functional Dual Anchors}, author={Shi, Kexuan and Wen, Yandong and Liu, Weiyang}, journal={arXiv preprint arXiv:2510.21223}, year={2025}} Agentic Design of Compositional Machines Wenqian Zhang, Weiyang Liu, Zhen Liu Preprint 2025 arXiv | code | project | bib @article{besiegefield2025, title={Agentic Design of Compositional Machines}, author={Zhang, Wenqian and Liu, Weiyang and Liu, Zhen}, journal={arXiv preprint arXiv:2510.14980}, year={2025}} SimKO: Simple Pass@K Policy Optimization Ruotian Peng, Yi Ren, Zhouliang Yu, Weiyang Liu, Yandong Wen Preprint 2025 arXiv | code | project | bib @article{yu2025simko, title={SimKO: Simple Pass@K Policy Optimization}, author={Peng, Ruotian and Ren, Yi and Yu, Zhouliang and Liu, Weiyang and Wen, Yandong}, journal={arXiv preprint arXiv:2510.14807}, year={2025}} Reparameterized LLM Training via Orthogonal Equivalence Transformation Zeju Qiu, Simon Buchholz, Tim Z. Xiao, Maximilian Dax, Bernhard Schölkopf, Weiyang Liu* NeurIPS 2025 arXiv | code | project | bib @InProceedings{qiu2025poet, title={Reparameterized LLM Training via Orthogonal Equivalence Transformation}, author={Qiu, Zeju and Buchholz, Simon and Xiao, Tim Z. and Dax, Maximilian and Sch\\\"olkopf, Bernhard and Liu, Weiyang}, booktitle={NeurIPS}, year={2025}} Orthogonal Finetuning Made Scalable Zeju Qiu*, Weiyang Liu*, Adrian Weller, Bernhard Schölkopf EMNLP 2025 arXiv | code | colab | project | bib @InProceedings{qiu2025oftv2, title={Orthogonal Finetuning Made Scalable}, author={Qiu, Zeju and Liu, Weiyang and Weller, Adrian and Sch\\\"olkopf, Bernhard}, booktitle={EMNLP}, year={2025}} FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models Zhouliang Yu, Ruotian Peng, Keyi Ding, Yizhe Li, Zhongyuan Peng, Minghao Liu, Yifan Zhang, Zheng Yuan, Huajian Xin, Wenhao Huang, Yandong Wen, Ge Zhang, Weiyang Liu* Preprint 2025 arXiv | code | project | dataset | bib @article{yu2025formalmath, title={FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models}, author={Yu, Zhouliang and Peng, Ruotian and Ding, Keyi and Li, Yizhe and Peng, Zhongyuan and Liu, Minghao and Zhang, Yifan and Zheng, Yuan and Xin, Huajian and Huang, Wenhao and Wen, Yandong and Liu, Weiyang}, journal={arXiv preprint arXiv:2505.02735}, year={2025}} Publication Show selected / all by date / all by topic This site has been visisted times in total.",
  "content_length": 6538,
  "method": "requests",
  "crawl_time": "2025-12-01 14:46:01"
}