{
  "name": "C. Karen Liu",
  "homepage": "https://tml.stanford.edu/people/karen-liu",
  "status": "success",
  "content": "Karen Liu | The Movement Lab Main content start Professor Karen Liu Karen has a passion for human movement, whether it involves an animated character or a humanoid robot. She directs the Movement Lab at Stanford to pursue Computer Animation and Robotics research in parallel. In her view, animation is about a virtual body controlled by a physical brain (i.e. a human animator) while robotics is about a physical body controlled by a virtual brain. These seemingly opposite research areas share remarkably similar fundamental methodologies that Karen and her team developed, including physics simulation, generative models, imitation learning, reinforcement learning, and various optimal control techniques.Karen’s interests in human movement have remained unchanged since her PhD advisor first showed her a physically simulated character hopping under moon gravity, but her vision has expanded from creating the coolest video game to building predictive human motion models for preventing musculoskeletal injury, studying human athletic and artistic performance, and developing new motion sensors for sport medicine. Her team has pioneered human-centric physics simulation, differentiable physics models, new mocap systems that capture both human actions and egocentric observations, and the largest dataset containing both kinematic and kinetic human motion data.Karen’s passion for human movement has also led her to pursue research in humanoid robots. Starting with bipedal locomotion, Karen and her team have developed various techniques in reinforcement learning, physics simulation, system identification, and sim-to-real transfer, demonstrating that policies trained in simulation can be applied to an inherently unstable bipedal robot. Similarly, Karen’s fascination with human dexterity has evolved from hand animation to dexterous robot hands. Leveraging the similar morphology between human and robotic hands, her team has developed an in-the-wild hand motion capture system that enables learning dexterous robot manipulation from human demonstrations.The fascinating parallel between Computer Animation and Robotics provides the Movement Lab with a unique perspective to explore various directions in Embodied AI, from building foundational models for human motion to creating intelligent digital twins and assistive machines that share autonomy with humans.Karen is also committed to democratizing robotics education. She aims to create a learning experience for beginners that integrates hardware experiments, mathematical foundations, and state-of-the-art AI methodologies. In collaboration with Hands-on Robotics, a non-profit organization, she teaches a well-received undergraduate course at Stanford, \"A Hands-On Introduction to AI-Enabled Robots.\" This course provides lecture materials, open-source code, and open-hardware video instructions to anyone in the world interested in building an AI-enabled quadruped from scratch. Contact karenliu@cs.stanford.edu Location CoDa E356 353 Jane Stanford Way, Stanford, CA 94305 Publications Xu, P., Wu, Z., Wang, R., Sarukkai, V., Fatahalian, K., Karamouzas, I., Zordan, V., & Liu, C. K. (2025). Learning to Ball: Composing Policies for Long-Horizon Basketball Moves. ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2025), 44. https://doi.org/10.1145/3763367 Wu, Z., Li, J., Xu, P., & Liu, C. K. (2025). Human-Object Interaction from Human-Level Instructions. Proceedings of the IEEE/CVF International Conference on Computer Vision. https://hoifhli.github.io/ Ze, Y., Chen, Z., Araújo, J., Cao, Z.- ang, Peng, X. B., Wu, J., & Liu, C. K. (2025). TWIST: Teleoperated Whole-Body Imitation System. Conference on Robot Learning (CoRL). https://yanjieze.com/TWIST/ Chen, S. ., Ye, Y., Cao, Z.-A., Lew, J. ., Xu, P., & Liu, C. K. (2025). Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching. Conference on Robot Learning (CoRL), 2025. https://stanford-tml.github.io/HEAD Chen, S., Wang, C., Nguyen, K., Fei-Fei, L., & Liu, C. K. (2025). ARCap: Collecting High-quality Human Demonstration for Robot Learning with Augmented Reality Feedback. ICRA 2025. https://stanford-tml.github.io/ARCap Truong, T., Piseno, M., Xie, Z., & Liu, C. K. (2024). PDP: Physics-Based Character Animation via Diffusion Policy. SIGGRAPH Asia 2024. https://doi.org/10.1145/3680528.3687683 Wang*, R., Xu*, P., Shi, H., Schumann, E., & Liu, C. K. (2024). FürElise: Capturing and Physically Synthesizing Hand Motions of Piano Performance. SIGGRAPH Asia 2024 Conference Papers, 1-11. https://doi.org/10.1145/3680528.3687703 Li, J., Clegg, A., Mottaghi, R., Wu, J., Puig, X., & Liu, C. K. (2024). Controllable Human-Object Interaction Synthesis. European Conference on Computer Vision (ECCV), 2024. https://lijiaman.github.io/projects/chois/ Wu, A., Wang, R., Chen, S., Eppner, C., & Liu, C. K. (2024). One-Shot Transfer of Long-Horizon Extrinsic Manipulation Through Contact Retargeting. International Conference on Intelligent Robots and Systems (IROS), 2024. https://stanford-tml.github.io/extrinsic-manipulation/ Li, J., Wu, J., & Liu, C. K. (2023). Object Motion Guided Human Motion Synthesis. ACM Transactions of Graphics (SIGGRAPH Asia) 2023. https://lijiaman.github.io/projects/omomo/ Keller, M., Werling, K., Shin, S., Delp, S., Pujades, S., Liu, C., & Black, M. (2023). From Skin to Skeleton: Towards Biomechanically Accurate 3D Digital Humans. ACM Transactions of Graphics (SIGGRAPH Asia) 2023., 42(6). https://doi.org/10.1145/3618381 Chen*, Y., Wang*, C., Fei-Fei, L., & Liu, C. K. (2023). Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation. Conference on Robot Learning (CoRL), 2023. Araujo, J. P., Li, J., Vetrivel, K. ., Agarwal, R., Gopinath, D., Wu, J., Clegg, A., & Liu, C. K. (2023). CIRCLE: Capture In Rich Contextual Environments. Conference on Computer Vision and Pattern Recognition (CVPR), 2023. Wu, A., Guo, M., & Liu, C. K. (2022). Learning Diverse and Physically Feasible Dexterous Grasps with Generative Model and Bilevel Optimization. Conference on Robot Learning (CoRL), 2022. https://proceedings.mlr.press/v205/wu23b.html Back to Top",
  "content_length": 6161,
  "method": "requests",
  "crawl_time": "2025-12-01 13:07:13"
}