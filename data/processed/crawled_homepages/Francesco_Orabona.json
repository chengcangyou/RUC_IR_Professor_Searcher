{
  "name": "Francesco Orabona",
  "homepage": "http://francesco.orabona.com",
  "status": "success",
  "content": "Francesco Orabona Home Research Publications Teaching Software About Me I am looking for 1-2 post-docs and 1-2 PhD students to work on practical and theoretical aspects of online learning, stochastic optimization, and training of LLMs. The ideal candidate has an exceptional mathematical background and is proficient in coding. If you are interested, send me an email with your CV. Please send me your transcript too if your are applying for a PhD position. Due to the volume of emails and my limited time, I might not answer to everyone: please do not take it personally, you were probably not a good match for my lab. I am currently an Associate Professor at KAUST in the Computer, Electrical and Mathematical Sciences and Engineering Division. Previously, I was at Boston University, Stony Brook University, Yahoo Research NY, the Toyota Technological Institute at Chicago, the University of Milan, the IDIAP Research Institute, and the University of Genoa. My current research interest is parameter-free machine learning. In particular I am interested in online learning, batch/stochastic optimization, and statistical learning theory. I manage the OPTIMAL Lab. CV and Google Scholar profile. Twitter: @bremen79 Blog: parameterfree.com New and Selected Papers NEW! A paper on self-directed node classification on graphs by Georgy Sokolov, Maximilian Thiessen, Margarita Akhmejanova, Fabio Vitale, and me has been accepted at ALT 2025. A paper on an equivalence between static and dynamic regret minimization by Andrew Jacobsen and me has been accepted at NeurIPS 2024. A paper on better-than-KL PAC-Bayes bounds by Ilja Kuzborskij, Kwang-Sung Jun, Yulian Wu, Kyoungseok Jang, and me has been accepted at COLT 2024. A paper on training without depth limits using Batch Normalization Without Gradient Explosion by Alexandru Meterez, Amir Joudaki, me, Alexander Immer, Gunnar RÃ¤tsch, and Hadi Daneshmand has been accepted at ICLR 2024. A paper on tight concentrations and confidence sequences from the regret of universal portfolio by me and Kwang-Sung Jun has been accepted at IEEE Transactions on Information Theory. Ashok Cutkosky and I gave a (virtual) tutorial at ICML 2020 on Parameter-Free Online Optimization. I have compiled all my lecture notes of my Introduction to Online Learning class in a monograph. Editorial Activities NEW! Senior Area Chair of NeurIPS 2025. NEW! Senior Area Chair of ICML 2025. NEW! Senior PC Member of COLT 2025. Area Chair of ALT 2025. Co-chair of ALT 2023. Associate Editor of IEEE Trans. Information Theory. Action Editor of JMLR. Recent Talks 6/2/23, Adaptive Optimization Methods, SIAM Conference on Optimization. 5/4/23, ML theory seminar, Princeton. 4/26/23, Microsoft Research Asia Theory Lecture Series. Old articles on the Italian Press \"E adesso Babybot, il robot che impara, aspetta un fratellino (piÃ¹ intelligente di lui)\", Il VenerdÃ¬ di Repubblica, 23 giugno 2006. \"Che bella famiglia sono tutti baby robot\", Il Secolo XIX - 30 luglio 2004. \"Sfide finali per gli artisti del mattoncino\",Corriere della Sera, 17 Giugno 1992. Powered by Free Website Templates",
  "content_length": 3111,
  "method": "requests",
  "crawl_time": "2025-12-01 13:11:01"
}