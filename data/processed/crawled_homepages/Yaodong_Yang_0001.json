{
  "name": "Yaodong Yang 0001",
  "homepage": "https://www.yangyaodong.com",
  "status": "success",
  "content": "About Me | Yaodong Yang top of pageDr. Yaodong Yang (杨耀东)​PKU Alignment & Interaction LabAbout Me ​ ​​​​ Dr. Yaodong Yang is an Assistant Professor (Boya Young Scholar) at the Institute for Artificial Intelligence, Peking University, Director of the AI Safety Centre at BAAI, and Chief Scientist of the PKU–PsiBot Joint Laboratory. His research focuses on experience learning and alignment of AI agents, aiming to advance the trustworthy deployment and real-world alignment of large models, spanning the areas of reinforcement learning, AI alignment, and embodied intelligence. He has published over 200 papers in leading journals and conferences, including Nature Machine Intelligence, Cell Matter, Artificial Intelligence Journal, and IEEE TPAMI, with more than 12,000 Google Scholar citations. Since 2022, he has been ranked as the top scholar in Artificial Intelligence and Machine Learning at Peking University according to CSRankings. Dr. Yang has received numerous honors, including the ACL 2025 Best Paper Award, ICCV 2023 Best Paper Initial List, CoRL 2020 Best System Paper Award, and the AAMAS 2021 Blue Sky Idea Award. He was named to the MIT Technology Review “AI 100 Young Innovators,” received the WAIC 2022 “Yunfan Star Award,” and the ACM SIGAI China Rising Star Award. His work has been featured by CCTV, Xinhua News, the National Natural Science Foundation of China (NSFC), and MIT Technology Review. He serves as an Area Chair for major conferences including ICML, ICLR, NeurIPS, AAAI, IJCAI, AAMAS, and IROS, and as an Associate Editor for Scientific Reports, Transactions on Machine Learning Research, and Neural Networks. Previously, Dr. Yang was an Assistant Professor at King’s College London, a Principal Researcher at Huawei Research U.K., and a Senior Manager at AIG. He received his B.Sc. from the University of Science and Technology of China, M.Sc. from Imperial College London, and Ph.D. from University College London, where he was the university’s sole nominee for the ACM SIGAI Doctoral Dissertation Award. ​​ ​ 杨耀东，北京大学人工智能研究院研究员（博雅学者），北京智源大模型安全中心主任，北大-灵初智能联合实验室首席科学家。国家人社部高层次留学人才、国家优秀青年科学基金（海外）获得者、中国科协青年托举计划入选者。主要研究方向为智能体交互学习与对齐，致力于大模型的可信应用与安全落地，科研领域涵盖强化学习、AI对齐与具身智能。在 Nature Machine Intelligence、Cell Matter、AIJ、TPAMI 等国际顶级期刊和会议发表论文二百余篇，谷歌学术引用逾12000+次，自2022年以来位列CSRanking北大人工智能与机器学习方向学者首位，入选Scopus全球Top2%顶尖科学家。曾获ACL 2025最佳论文奖、ICCV 2023最佳论文奖入围、CoRL 2020最佳系统论文奖、AAMAS 2021最佳前瞻性论文奖，入选麻省理工科技评论“AI 100青年榜”、WAIC 2022“云帆奖·璀璨明星”及ACM SIGAI China新星奖。相关研究成果获中央电视台《焦点访谈》、新华网、国家自然科学基金委官网及《麻省理工科技评论》等多家媒体报道。现任ICML、ICLR、NeurIPS、AAAI、IJCAI、AAMAS、IROS等国际会议领域主席，以及《Scientific Reports》《Transactions on Machine Learning Research》《Neural Networks》等期刊执行编委，主持国家自然科学基金、科技部、北京市科委及多项校企联合实验室科研项目五十余项。曾任伦敦国王学院助理教授、华为英国研究所主任研究员、美国国际集团高级经理。本科毕业于中国科学技术大学，先后在伦敦帝国理工学院获硕士学位、伦敦大学学院获博士学位，并获校唯一提名角逐ACM SIGAI优秀博士论文奖。 ​​ ​ 北大对齐与交互实验室PAIR-Lab的科研方向包括： PAIR-Lab 2026年的博士名额：<0 ​常年招收强化学习实习生/访问学者（带薪） ​ 人工智能对齐（人类反馈强化学习、博弈论、控制论） ICML 2025 Tutorial: 大模型中的对齐 DeepSeek R1模型最新解读 ｜ 视频 OpenAI O1模型最新解读 ｜ 视频 AI对齐全面性综述AI Alignment Survey | 视频解读 AI对齐2024年度进展报告｜短视频 ｜长视频 轻量级对齐新范式：提升GPT-4安全性26% (麻省科技评论) AI安全与伦理 AI Safety & Ethics (CCTV-4 深度国际) AI超级对齐 Super-Alignment (三联生活周刊封面) | ​(播客)​ 大模型安全对齐 Safe-RLHF (ICLR Spotlight)                 (Youtube介绍 | 中文介绍 | 安全对齐数据集) ​​​ 基于强化学习的灵巧双手操作（强化学习、机器人、具身智能） 北大-灵初联合实验室：灵初智能PsiBot 以物体为中心的强化学习灵巧双手操作 Obj-Dex 基于多智能体强化学习的灵巧双手操作 Bi-DexHands NeurIPS 2022 灵巧操作比赛冠军（1/340）| 中国青年报报道 基于多智能体强化学习的具身灵巧双手抛接配合 ​ 多智能体博弈交互（强化学习、多智能体、博弈论） ​超大规模多体强化学习（Nature Machine Intelligence） 一个合作博弈的通用求解框架（TechBeat'23最受欢迎讲者） 一个零和博弈的通用求解框架（TechBeat'22最受欢迎讲者） 博弈多智能体强化学习综述（英文） 大规模对抗博弈理论基础与求解方法（中文） ​​​​​​ 强化学习开源项目（Show me the code, not the story~） JMLR: 安全强化学习 OmniSafe JMLR: 异构智能体强化学习 HARL JMLR: 多智能体合作强化学习 MARLlib JMLR: 多智能体对抗强化学习 MAlib JMLR: 元强化学习 TorchOpt ​ ​recentRecent News​Top Highlights: ​​ 07/2025​ ​​ Our paper wins the ACL'25 Best Paper Award.​ ​ Language Models Resist Alignment: Evidence From Data Compression​ Slide Media（新华社）​ Media（国家自然科学基金委）​ Media（机器之心）​ 04/2025​ ​​ ​I deliver a 3-hour tutorial at ICML 2025 (virtual). ​ \"Alignment Methods on Large Language Models\"​ ​ 12/2024​ ​ Check out our Matter (Cell Press)  paper on applying LLMs for generating carbon nanotubes automatically. ​ Transforming the synthesis of carbon nanotubes with machine learning models and automation ​ 09/2024​ ​ Check out our Nature Machine Intelligence​  paper on Large Scale Multi-agent Networked RL & its applications on pandemics, smart grid and traffic control. ​ Efficient and scalable reinforcement learning for large-scale network control ​ 新华社报道 ​科技日报报道 ​北大新闻网 ​​​​​​​ ​ Latest News: ​ ​ 09/2025​ ​ Eleven papers get accepted at NeurIPS 2025​ ​ Empirical Study on Robustness and Resilience in Cooperative Multi-Agent Reinforcement Learning Learning Principles from Multi-modal Human Preference Spotlight (3%) Safe VLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning Risk-aware Direct Preference Optimization under Nested Risk Measure Social World Model-Augmented Mechanism Design Policy Learning Spotlight (3%) DexFlyWheel: A Scalable and Self-improving Data Generation Framework for Dexterous Manipulation STAR: Efficient Preference-based Reinforcement Learning via Dual Regularization Safe RLHF-V: Safe Reinforcement Learning from Multi-modal Human Feedback Spotlight DB (3%) InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback​​​ PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models World Models Should Prioritize the Unification of Physical and Social Dynamics (Position Paper) ​​ ​ 05/2025 ​ Six papers get accepted at ACL 2025​​ ​ Boosting Policy and Process Reward Models with Monte Carlo Tree Search in Open-Domain QA SafeLawBench: Towards Safe Alignment of Large Language Models Best Paper Award: Language Models Resist Alignment: Evidence From Data Compression Reward Generalization in RLHF: A Topological Perspective Benchmarking Multi-National Value Alignment for Large Language Models BeaverTails v2: Towards Multi-Level Safety Alignment for LLMs with Human Preference​​ ​ ​ 05/2025 ​ Two papers get accepted at ICML 2025​​​ Falcon: Fast visuomotor policy via partial denoising SAE-V: Interpreting Multimodal Models for Enhanced Alignment ​ ​ ​ 01/2025 ​ Five papers get accepted at ICLR 2025​​ In-Context Editing: Learning Knowledge from Self-Induced Distributions Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models Magnetic Mirror Descent Self-play Preference Optimization ​​ ​ ​ 12/2024 ​ Five papers get accepted at AAAI 2025​ Sequence to Sequence Reward Modeling: Improving RLHF by Language Feedback (Oral) Stream Aligner: Efficient Sentence-Level Alignment via Distribution Induction Differentiable Information Enhanced Model-Based Reinforcement Learning (Oral) Towards efficient collaboration via graph modeling in reinforcement learning RAT: Adversarial Attacks on Deep Reinforcement Agents for Targeted Behaviors​​ (Oral) ​​ Two papers get accepted at AAMAS 2025 Mean Field Correlated Imitation Learning EconTwo: A Two-Level Multi-Agent Framework for Dynamic Macroeconomic Modeling with Shock Resilience (Short paper) ​ ​ ​ 10/2024 ​ Checkout my recent talk on \"Can LLM be Aligned ?\" at CNCC 2024. ​ ​ ​ 09/2024 ​ Five papers get accepted at NeurIPS 2024 Achieving Efficient Alignment through Learned Correction (Oral, top 0.5%) ProgressGym: Alignment with a Millennium of Moral Progress (Spotlight) Panacea: Pareto Alignment via Preference Adaptation for LLMs Scalable Constrained Policy Optimization for Safe Multi-agent Reinforcement Learning SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset ​ ​ ​ 09/2024 ​ Check out our Nature Machine Intelligence​ paper on Large Scale Multi-agent RL. ​ Efficient and scalable reinforcement learning for large-scale network control ​ 新华社报道 ​科技日报报道 ​北大新闻网 ​ ​​​ ​ 08/2024 ​​ Two papers accepted at CoRL 2024 Neural Attention Field: Emerging Point Relevance in 3D Scenes for One-Shot Dexterous Grasping ​Object-Centric Dexterous Manipulation from Human Motion Data ​ ​​​ ​ 05/2024 ​ Valse 2024年度进展报告:从偏好对齐到价值对齐与超对齐 ​中文视频 ​ ​ ​ 05/2024 ​ Three papers get accepted at ICML 2024 SINSIGHT: End-to-End Neuro-Symbolic Visual Reinforcement Learning with Language Explanations Safe Reinforcement Learning using Finite-Horizon Gradient-based Estimation Planning with Theory of Mind for Few-Shot Adaptation in Mixed-motive Environments ​ ​ ​ ​ 03/2024​ ​ We, alogn with Yoshua Bengio, Stuart Russell, Geff Hinton and Chinese decision makers signed Beijing Declaration on AI Safety. ​中文报道 ​ ​ ​ ​ 01/2024​ ​ Five papers get accepted at ICLR 2024 & one paper on TPAMI. ​​ Spotlight (5%) CivRealm: A Learning and Reasoning Odyssey for Decision-Making Agents Spotlight (5%) Maximum Entropy Heterogeneous-Agent Reinforcement Learning Spotlight (5%) Safe RLHF: Safe Reinforcement Learning from Human Feedback SafeDreamer: Safe Reinforcement Learning with World Models Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game PAMI  ASP: Learn a Universal Neural Solver ​ ​ ​ ​ 12/2023 ​ Three papers get accepted at AAAI 2024. ​ STAS: Spatial-Temporal Return Decomposition for Multi-agent Reinforcement Learning Oral (7%) ProAgent: Building Proactive Cooperative AI with Large Language Models A Perspective of Q-value Estimation on Offline-to-Online Reinforcement Learning ​ ​ ​ 12/2023 ​ Two top journals get accepted! ​ PAMI Bi-DexHands: Towards Human-Level Bimanual Dexterous Manipulation JMLR Heterogeneous-Agent Reinforcement Learning ​ ​ 11/2023 ​ We release AI Alignment Survey and Alignment Resource Website. ​ ​ ​ 10/2023 ​ Our paper won the best paper initial list (17/8260) at ICCV 2023! ​ UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-awar",
  "content_length": 20765,
  "method": "requests",
  "crawl_time": "2025-12-01 14:50:34"
}