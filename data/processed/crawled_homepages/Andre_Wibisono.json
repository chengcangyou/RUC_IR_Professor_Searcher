{
  "name": "Andre Wibisono",
  "homepage": "http://www.cs.yale.edu/homes/wibisono",
  "status": "success",
  "content": "Andre Wibisono Andre Wibisono YALE UNIVERSITY andre.wibisono [at] yale [dot] edu I am an assistant professor in the Department of Computer Science at Yale University, with a secondary appointment in the Department of Statistics & Data Science. My research interests are in the design and analysis of algorithms for machine learning, in particular for problems in optimization, sampling, and game theory. I received my BS degrees in Mathematics and in Computer Science from MIT. I received my MEng in Computer Science from MIT, advised by Tomaso Poggio. I received by MA in Statistics from UC Berkeley, and my PhD in Computer Science from UC Berkeley, advised by Michael I. Jordan. Before joining Yale in 2021, I have done postdoctoral research at the University of Wisconsin-Madison, working with Po-Ling Loh and Varun Jog, and at the Georgia Institute of Technology, working with Jacob Abernethy and Santosh Vempala. TEACHING • CPSC 486/586: Probabilistic Machine Learning (Spring 2023-2025) • CPSC/ECON 365: Algorithms (Fall 2023-2024, Spring 2022) • CPSC 481/581: Introduction to Machine Learning (Fall 2021) • CPSC 661: Sampling Algorithms in Machine Learning (Spring 2021) RESEARCH GROUP • Siddharth Mitra • Kaylee (Yingxi) Yang • Jane H. Lee • Qiang Fu • Peter (Xiuyuan) Wang RESEARCH ALUMNI • Jun-Kun Wang (postdoc 2021-2023, now at UCSD) • Jiaming Liang (postdoc 2022-2023, now at University of Rochester) RESEARCH For complete publication list, please see Google Scholar • Hamiltonian Descent Algorithms for Optimization: Accelerated Rates via Randomized Integration Time Qiang Fu Andre Wibisono arXiv preprint arXiv:2505.12553 • Fast and Furious Symmetric Learning in Zero-Sum Games: Gradient Descent as Fictitious Play John Lazarsfeld, Georgios Piliouras, Ryann Sim, Andre Wibisono COLT (Conference on Learning Theory) 2025 • Mixing Time of the Proximal Sampler in Relative Fisher Information via Strong Data Processing Inequality Andre Wibisono COLT (Conference on Learning Theory) 2025 • On the Convergence of Min-Max Langevin Dynamics and Algorithm Yang Cai, Siddharth Mitra, Xiuyuan Wang, Andre Wibisono COLT (Conference on Learning Theory) 2025 • Characterizing Dependence of Samples along the Langevin Dynamics and Algorithms via Contraction of Phi-Mutual Information Jiaming Liang, Siddharth Mitra, Andre Wibisono COLT (Conference on Learning Theory) 2025 • High-accuracy sampling from constrained spaces with the Metropolis-adjusted Preconditioned Langevin Algorithm Vishwak Srinivasan, Andre Wibisono, Ashia Wilson ALT (Algorithmic Learning Theory) 2025 • Fast Convergence of Phi-Divergence Along the Unadjusted Langevin Algorithm and Proximal Sampler Siddharth Mitra, Andre Wibisono ALT (Algorithmic Learning Theory) 2025 • A symplectic analysis of alternating mirror descent Jonas Katona, Xiuyuan Wang, Andre Wibisono arXiv preprint arXiv:2405.03472, 2024 • Optimal score estimation via empirical Bayes smoothing Andre Wibisono, Yihong Wu, Kaylee Yingxi Yang COLT (Conference on Learning Theory) 2024 • Fast sampling from constrained spaces using the Metropolis-adjusted Mirror Langevin Algorithm Vishwak Srinivasan, Andre Wibisono, Ashia Wilson COLT (Conference on Learning Theory) 2024 • Extragradient Type Methods for Riemannian Variational Inequality Problems Zihao Hu, Guanghui Wang, Xi Wang, Andre Wibisono, Jacob Abernethy, Molei Tao AISTATS (Artificial Intelligence and Statistics) 2024 • Learning Exponential Families from Truncated Samples Jane Lee, Andre Wibisono, Manolis Zampetakis NeurIPS (Neural Information Processing Systems) 2023 • On a Class of Gibbs Sampling over Networks Bo Yuan, Jiaojiao Fan, Jiaming Liang, Andre Wibisono, Yongxin Chen COLT (Conference on Learning Theory) 2023 • Towards Understanding GD with Hard and Conjugate Pseudo-labels for Test-Time Adaptation Jun-Kun Wang, Andre Wibisono ICLR (International Conference on Learning Representations) 2023 • Accelerating Hamiltonian Monte Carlo via Chebyshev Integration Time Jun-Kun Wang, Andre Wibisono ICLR (International Conference on Learning Representations) 2023 • Continuized Acceleration for Quasar Convex Functions in Non-Convex Optimization Jun-Kun Wang, Andre Wibisono ICLR (International Conference on Learning Representations) 2023 • Convergence in KL Divergence of the Inexact Langevin Algorithm with Application to Score-based Generative Models Kaylee Yingxi Yang, Andre Wibisono arXiv preprint arXiv:2211.01512, 2022 • Alternating Mirror Descent for Constrained Min-Max Games Andre Wibisono, Molei Tao, Georgios Piliouras NeurIPS (Neural Information Processing Systems) 2022 • Provable Acceleration of Heavy Ball beyond Quadratics for a Class of Polyak-Lojasiewicz Functions when the Non-Convexity is Averaged-Out Jun-Kun Wang, Chi-Heng Lin, Andre Wibisono, Bin Hu ICML (International Conference on Machine Learning) 2022 • Improved analysis for a proximal algorithm for sampling Yongxin Chen, Sinho Chewi, Adil Salim, Andre Wibisono COLT (Conference on Learning Theory) 2022 • The Mirror Langevin Algorithm Converges with Vanishing Bias Ruilin Li, Molei Tao, Santosh S. Vempala, Andre Wibisono ALT (Algorithmic Learning Theory) 2022 • Last-iterate convergence rates for min-max optimization Jacob Abernethy, Kevin Lai, and Andre Wibisono ALT (Algorithmic Learning Theory) 2021 • Fast Convergence of Fictitious Play for Diagonal Payoff Matrices Jacob Abernethy, Kevin Lai, and Andre Wibisono SODA (Symposium on Discrete Algorithms) 2021 • Proximal Langevin Algorithm: Rapid convergence under isoperimetry Andre Wibisono arXiv preprint arXiv:1911.01469, 2019 • Rapid convergence of the Unadjusted Langevin Algorithm: Isoperimetry suffices Santosh Vempala and Andre Wibisono NeurIPS (Neural Information Processing System) 2019 arXiv version | poster | expanded journal version in GAFA • Accelerating Rescaled Gradient Descent: Fast optimization of smooth functions Ashia Wilson, Lester Mackey, and Andre Wibisono NeurIPS (Neural Information Processing System) 2019 • Convexity of mutual information along the Ornstein-Uhlenbeck flow Andre Wibisono and Varun Jog ISITA (International Symposium on Information Theory and Applications) 2018 • Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem Andre Wibisono COLT (Conference on Learning Theory) 2018 • Convexity of mutual information along the heat flow Andre Wibisono and Varun Jog ISIT (International Symposium on Information Theory) 2018 • Information and estimation in Fokker-Planck channels Andre Wibisono, Varun Jog, and Po-Ling Loh ISIT (International Symposium on Information Theory) 2017 • A variational perspective on accelerated methods in optimization Andre Wibisono, Ashia Wilson, and Michael Jordan Proceedings of the National Academy of Sciences, 133, E7351--E7358, 2016. [arXiv version] • Optimal rates for zero-order convex optimization: the power of two function evaluations John Duchi, Michael Jordan, Martin Wainwright, and Andre Wibisono IEEE Transactions on Information Theory, 61(5): 2788--2806, May 2015 • A Hadamard-type lower bound for symmetric diagonally dominant positive matrices Christopher Hillar and Andre Wibisono Linear Algebra and Applications, 472: 135--141, 2015 • Convexity of reweighted Kikuchi approximation Po-Ling Loh and Andre Wibisono NIPS (Neural Information Processing System) 2014 • How to hedge an option against an adversary: Black-Scholes pricing is minimax optimal Jake Abernethy, Peter Bartlett, Rafael Frongillo, and Andre Wibisono NIPS (Neural Information Processing System) 2013 • Streaming variational Bayes Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia Wilson, and Michael Jordan NIPS (Neural Information Processing System) 2013 • Maximum entropy distributions on graphs Christopher Hillar and Andre Wibisono arXiv preprint arXiv:1301.3321, 2013 • Inverses of symmetric, diagonally dominant positive matrices and applications Christopher Hillar, Shaowei Lin, and Andre Wibisono arXiv preprint arXiv:1203.6812, 2013 • Finite sample convergence rates of zero-order stochastic optimization methods John Duchi, Michael Jordan, Martin Wainwright, and Andre Wibisono NIPS (Neural Information Processing System) 2012 • Minimax option pricing meets Black-Scholes in the limit Jacob Abernethy, Rafael Frongillo, and Andre Wibisono STOC (Symposium on the Theory of Computing) 2012 THESES • Variational and Dynamical Perspectives on Learning and Optimization PhD in Computer Science, University of California, Berkeley, May 2016 • Maximum Entropy Distributions on Graphs MA in Statistics, University of California, Berkeley, May 2013 • Generalization and Properties of the Neural Response MEng in Computer Science, Massachusetts Institute of Technology, June 2010",
  "content_length": 8742,
  "method": "requests",
  "crawl_time": "2025-12-01 12:56:40"
}