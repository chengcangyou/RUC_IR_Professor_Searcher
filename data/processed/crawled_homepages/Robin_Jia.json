{
  "name": "Robin Jia",
  "homepage": "https://robinjia.github.io",
  "status": "success",
  "content": "Robin Jia Robin Jia Email: robinjia at usc dot edu Office: GCS 405E Curriculum vitae I am an assistant professor in the Thomas Lord Department of Computer Science at the University of Southern California, where I lead the AI, Language, Learning, Generalization, and Robustness (Allegro) Lab. My research seeks to understand modern deep learning systems for NLP and ensure that they are reliable. Some research questions I think about include: How can we scientifically understand large language models? Our scientific understanding of LLMs lags far behind our ability to engineer them. To bridge this gap, we study mechanisms behind LLM capabilities such as in-context learning (NeurIPS 2024, EMNLP 2024), data memorization (NAACL 2024), numerical reasoning (NeurIPS 2024), factual recall (arXiv 2025), and retracting wrong answers (arXiv 2025). We strive to convert our improved understanding into actionable insights, e.g., by improving the mathematical abilities of Transformers (arXiv 2025) or the reliability of in-context learning (EMNLP 2024). How can technical research shed light on legal and policy issues surrounding AI? The widespread adoption of LLMs begets many legal and policy questions. We have collaborated with legal scholars to propose a fair learning framework for analyzing copyright claims against LLMs (FAccT 2025) and operationalize underspecified requirements in the EU’s Digital Services Act (AIES 2024). We have also developed data watermarking techniques that enable copyright holders to determine if their data was used to pre-train a closed-source LLM (ACL Findings 2024, ACL Findings 2025). How should we benchmark LLMs? I have long advocated for benchmarking robustness (EMNLP 2017) and uncertainty (ACL 2020) of NLP systems. My work at USC has benchmarked generalization to long-tail examples (EACL Findings 2023) and calibration of LLMs (EMNLP Findings 2023). Most recently, we collaborated with oncologists to benchmark LLMs on patient questions about cancer, finding that they fail to push back against false presuppositions (arxiv 2025). How can we leverage LLMs when faced with complex reasoning tasks? We have developed methods that combine LLMs with symbolic solvers to solve complex long-term planning tasks (NAACL 2025, arXiv 2025), and with semantic code analysis engines to help software engineers find and fix bugs in large codebases. We have also improved smaller models on complex question answering tasks by training them to generate reasoning chains (EMNLP 2023). I received my Ph.D. in Computer Science from Stanford University, where I was advised by Percy Liang. After that, I spent one year as a visiting researcher at Facebook AI Research, working with Luke Zettlemoyer and Douwe Kiela. For prospective Ph.D. students I am recruiting Ph.D. students for Fall 2026. If you are interested in working with me, please apply to the USC computer science department and list me as a potential advisor. This is the best way to ensure that I will read your application. This cycle, I am especially looking for students interested in (1) Studying policy-relevant concerns related to LLMs (e.g., copyright issues, LLM sycophancy), (2) Leading interdisciplinary collaborations on applying LLMs (e.g., in software engineering or medicine), and (3) Mechanistic interpretability. For USC undergraduate or master's students interested in research If you are an undergraduate or master’s student at USC and are interested in doing research with me, please send me an email with the following: A description of why you’re interested in doing research. A summary of any experience you think may be relevant, including but not limited to coursework, previous projects, volunteer work, etc. A copy of your undergraduate and graduate (if applicable) transcripts. (Optional) Your CV. For other undergraduate or master's students interested in research Unfortunately, I do not have the bandwidth to advise undergraduate or master’s students from other universities at this time. One exception is the Viterbi Summer Undergraduate Research Experience (SURE) program. You are welcome to apply to this program and list me as a potential advisor. News and Upcoming Events October 24, 2025: We have released Hubble, our model suite for studying LLM memorization of pre-training data! We are grateful to NAIRR and NVIDIA for providing 200k GPU-hours for this project. July 28 - August 1, 2025: I attended ACL 2025 in Vienna, where I co-organized the L2M2 Workshop. July 1, 2025: I am excited to receive an inaugural Google ML and Systems Junior Faculty Award! June 5, 2025: I gave an invited talk at UChicago. June 2, 2025: I gave an invited talk at the Northwestern CS Seminar. May 9, 2025: I gave an invited talk at the Cornell AI Seminar. May 1-3, 2025: I attended NAACL 2025 in Albuquerque. December 13, 2024: I am excited to receive a joint gift award from the USC - Capital One Center for Responsible AI and Decision Making in Finance (CREDIF) with Sai Praneeth Karimireddy for research on privacy-preserving synthetic data generation! December 3, 2024: I gave an invited talk at the CMU AI Seminar (slides). November 13-16, 2024: I attended EMNLP 2024 in Miami and was a panelist at the GenBench Workshop. November 12, 2024: I gave an invited talk at the Workshop on Domain Adaptation and Related Areas at the Simons Institute (slides). November 5, 2024: I gave an invited talk at UCLA (slides). October 24, 2024: I gave an invited talk at UC Berkeley (slides). October 22-23, 2024: I attended AIES 2024 in San Jose. October 10, 2024: I was a panelist at the DSAA 2024 Panel on Data Science in the Age of GenAI, and gave an invited talk at UC San Diego on the same day (slides). August 14, 2024: I am excited to receive a gift award from the USC-Amazon Center on Secure and Trusted Machine Learning for my research on improving factuality in language models! July 18, 2024: I gave an invited talk at the Stanford NLP Seminar (slides). July 11, 2024: I am excited to receive a joint NSF Grant with Jordan Boyd-Graber, Swabha Swayamdipta, John Lalor, and Alvin Grissom II on creating personalized, robust, and diversity-aware language models! May 11, 2024: I gave a keynote talk at the Workshop on Secure and Trustworthy Large Language Models (SET-LLM) at ICLR 2024. November 20, 2023: I spoke with Will Orr and Kate Crawford’s Knowing Machines podcast about the process of creating AI datasets. Quotes from this interview were also used in two of Orr and Crawford’s publications titled The social construction of datasets: On the practices, processes, and challenges of dataset creation for machine learning and Building Better Datasets: Seven Recommendations for Responsible Design from Dataset Creators. April 19, 2023: I am excited to receive a Google Research Scholar award for my research on understanding in-context learning! March 14, 2023: I am excited to receive a Cisco Research award for my research on estimating capabilities of LLMs! Students Ph.D. Students Johnny Wei Ameya Godbole Wang (Bill) Zhu (joint with Jesse Thomason) Ting-Yun (Charlotte) Chang (joint with Jesse Thomason) Deqing Fu (joint with Vatsal Sharan) Tianyi Zhou (joint with Vatsal Sharan) Yuqing Yang Muru Zhang (joint with Swabha Swayamdipta) Undergraduate and Masters Students Gustavo Lucas Carvalho Xiaoyuan Zhu (with Willie Neiswanger) Nitya Kashyap Miaosen Chai Qiutong (Tony) Yi Zitong (Cynthia) Huang Lab Alumni Ryan Wang: Former USC undergraduate, now a Ph.D. student at UC Berkeley Lorena Yan: Former USC undergraduate, now a Ph.D. student at Columbia Qilin Ye: Former USC undergraduate, now a M.S. student at Duke Harvey Yiyun Fu: Former USC undergraduate, now a Ph.D. student at UChicago Publications Promote, Suppress, Iterate: How Language Models Answer One-to-Many Factual Queries. Tianyi Lorena Yan and Robin Jia. Empirical Methods in Natural Language Processing (EMNLP), 2025. Rethinking Backdoor Detection Evaluation for Language Models. Jun Yan, Wenjie Jacky Mo, Xiang Ren, and Robin Jia. Empirical Methods in Natural Language Processing (EMNLP), 2025. Why Do Some Inputs Break Low-Bit LLM Quantization? Ting-Yun Chang, Muru Zhang, Jesse Thomason, and Robin Jia. Empirical Methods in Natural Language Processing (EMNLP), 2025. Teaching Models to Understand (but not Generate) High-risk Data. Ryan Wang, Matthew Finlayson, Luca Soldaini, Swabha Swayamdipta, and Robin Jia. Conference on Language Modeling (COLM), 2025. (github) LLM Unlearning Without an Expert Curated Dataset. Xiaoyuan Zhu, Muru Zhang, Ollie Liu, Robin Jia, and Willie Neiswanger. Conference on Language Modeling (COLM), 2025. (github) Verify with Caution: The Pitfalls of Relying on Imperfect Factuality Metrics. Ameya Godbole and Robin Jia. Findings of ACL, 2025. (github) (acl anthology) (bib) Mechanistic Interpretability of Emotion Inference in Large Language Models. Ala N. Tak*, Amin Banayeeanzade*, Anahita Bolourani, Mina Kian, Robin Jia, and Jonathan Gratch. Findings of ACL, 2025. (github) (acl anthology) (bib) Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge. Xinyue Cui, Johnny Tian-Zheng Wei, Swabha Swayamdipta, and Robin Jia. Findings of ACL, 2025. (github) (acl anthology) (bib) Interrogating LLM Design under a Fair Learning Doctrine. Johnny Tian-Zheng Wei*, Maggie Wang*, Ameya Godbole, Jonathan H. Choi, and Robin Jia. ACM Conference on Fairness, Accountability, and Transparency (FAccT), 2025. (acm) Language Models can Infer Action Semantics for Classical Planners from Environment Feedback. Wang Zhu, Ishika Singh, Robin Jia, and Jesse Thomason. North American Association for Computational Linguistics (NAACL), 2025. (github) (acl anthology) (bib) TLDR: Token-Level Detective Reward Model for Large Vision Language Models. Deqing Fu, Tong Xiao, Rui Wang, Wang Zhu, Pengchuan Zhang, Guan Pang, Robin Jia, and Lawrence Chen. International Conference on Learning Representations (ICLR), 2025. Pre-trained Large Language Models Use Fourier Features to Co",
  "content_length": 28555,
  "method": "requests",
  "crawl_time": "2025-12-01 14:19:35"
}