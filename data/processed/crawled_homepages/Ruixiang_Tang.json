{
  "name": "Ruixiang Tang",
  "homepage": "https://www.ruixiangtang.net",
  "status": "success",
  "content": "Ruixiang (Ryan) TangSearch this siteEmbedded FilesSkip to main contentSkip to navigationRuixiang (Ryan) TangRuixiang (Ryan) TangDepartment of Computer ScienceRutgers-New BrunswickAddress: 110 Frelinghuysen Rd, Hill Center 416, Piscataway, NJ 08854Email: ruixiang.tang@rutgers.eduAbout MeI am a Tenure-Track Assistant Professor in the Department of Computer Science at Rutgers Universityâ€“New Brunswick. I received my Ph.D. in Computer Science from Rice University, where I was advised by Dr. Xia Hu. I began my undergraduate studies in Biology at Tsinghua University, conducting research under the guidance of Dr. Xu Tan, and later transferred to the Department of Automation, completing my B.S. under the supervision of Dr. Jiwen Lu.Our research focus is in the realm of Trustworthy Machine Learning, a critical area that demands the infusion of trust throughout the machine learning lifecycleâ€”from data acquisition and model development to deployment and user interaction. Within this overarching theme, I specialize in issues related to robustness and explainability. Additionally, I collaborate closely with biologists and health informaticians to apply machine learning in addressing critical challenges in biology and medicine.The Large Foundation Models (e.g., Large Language Models, Large Vision-Language Models, and Vision-Language Action Models) bring new and complex challenges to the trustworthiness of machine learning. Our long-term objective is to develop learning algorithms that enhance the reasoning capabilities of these models, making them more reliable, transparent, and resistant to misuse. By advancing the underlying reasoning mechanisms, we aim to strengthen their alignment with human intent and ensure their responsible use in real-world applications.ðŸ”¥ Iâ€™m always looking for outstanding PhD students and research interns. If youâ€™re interested, please email me with your CV, transcript, a summary of your research experience, and highlight publications. I will carefully review every application.Research Overview11/2025: Three papers accepted to AAAI 2026 (two orals, one poster). Our work advances multimodal reasoning, improves reasoning efficiency, and explores counterfactual reasoning in LLMs.10/2025: Congratulations to Zeru Shi for being selected as a Rutgers Climate and Energy Institute (RCEI) Fellowship!10/2025: One paper accepted by AACL 2025. We provide a dataset, taxonomy, and baselines for analysis of rational persuasion and manipulation.8/2025: Three paper (one main two findings) accepted by EMNLP 2025. We explored multimodal in-context learning, LoRA security, and an LLM agent for text style transfer.7/2025: One paper accepted by Bioinformatics Advances. We explored a LLM-based framework that improves rare disease gene prioritization through phenotype-driven classification.7/2025: One paper accepted by COLM 2025: we explored replacing multimodal examples with learnable vectors to improve in-context learning in Large Vision-Language Models.6/2025: Two papers accepted by ICCV 2025: we explored the robustness of Vision-Language-Action model-based robotics, and addressed hallucination in Large Vision-Language Models.5/2025: Invited to serve as an Associate Editor for Smart Health.5/2025: Congratulations to Leroy Souz for being named a Paul Robeson Centennial Scholar and for reaching the semifinals of the Rice Business Plan Competition.5/2025: Two papers accepted by ACL 2025 Main, we explored using explainability to detect backdoors in LLMs and disentangle reasoning from memory in LLMs.5/2025: One paper accepted by ICML 2025, we show that Massive Q/K values are key to contextual understanding in LLMs.4/2025: Our paper \"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond\" is now the all-time most-read article of ACM Transactions on Knowledge Discovery from Data (TKDD)!2/2025: Receive 2,000 $ API credit from OpenAI to support our research on detecting and mitigating hallucination in VLMs.Â 2/2025: ðŸ”¥ Check out our recent paper exploring the robustness of VLA-based robotics. (demo link).2/2025: One paper accepted by SIGKDD Explorations, we proposed to mitigate shortcuts in natural language understanding tasks.1/2025: One paper accepted by ICLR 2025, we proposed MRT to efficiently edit multimodal representations.1/2025: Our tutorial about \"Understanding Large-Scale Machine Learning Robustness under Paradigm Shift\" is accepted by SDM 2025. See you in Virginia!09/2024: Three papers have been accepted for EMNLP 2024, focusing on issues related to LLM intellectual property protection, bias mitigation, and building trustworthy agents.07/2024: Our paper, Mitigating Relational Bias on Knowledge Graphs, has been accepted by TKDD.04/2024: ðŸ”¥ Our paper, The Science of Detecting LLM-Generated Text, has been accepted for a cover paper in the April 2024 issue of Communications of the ACM (CACM)!04/2024: Our research about AI for watermarking has been awarded the annual Hershel M. Rich Invention Award, congratulations to the team!03/2024: One paper has been accepted by the NAACL 2024. We proposed a key prompt protection mechanism to safeguard large language models.02/2024: One paper has been accepted by the Journal of Biomedical Informatics. We proposed a soft prompt calibration method to mitigate the variance of large language model output in the radiology report summarization task.01/2024: Our paper, Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond, has been accepted by TKDD.10/2023: Our paper about using LLMs for patient-trial matching was selected as the AMIA 2023 Best Student Paper and KDDM Student Innovation Award.10/2023: Our paper about building knowledge refinement and retrieval systems for interdisciplinarity in biomedical research has been selected as CIKM 2023 Best Demo Paper Honorable Mention.10/2023: One paper has been accepted by EMNLP 2023. We introduce a membership inference attack aimed at Large Language Models to analyze associated privacy risks.09/2023: Two papers have been accepted by NeurIPS 2023. We proposed a honeypot mechanism to defend against backdoor attacks in language models.08/2023: One paper has been accepted by ECML-PKDD 2023. We proposed a serial key protection mechanism for safeguarding DNN models.08/2023: Two papers have been accepted by CIKM 2023. We proposed a transferable watermark for defending against model extraction attacks.07/2023: Three Papers have been accepted by AMIA 2023 Annual Symposium. We investigated methods for harnessing the capabilities of LLMs in several healthcare tasks, such as Named Entity Recognition, Relation Extraction, and Patient-Trial Matching.Â 05/2023: One paper has been accepted by ACL 2023. We reveal that LLMs are \"lazy learners\" that tend to exploit shortcuts in prompts for downstream tasks.Â 04/2023: One paper has been accepted by SIGKDD Exploration 2023. We proposed a clean-label backdoor-based watermarking framework for safeguarding training datasets.Â 03/2023: One paper has been accepted by ICHI 2023. We proposed a deep ensemble framework for improving phenotype prediction from multi-modal data.Â Â Â Â Â Â Â Â BenignÂ  Â  Â  Â  Â  Â  Â  Â  Manipulatedcsweb.rice.edu/news/rice-cs-xia-ben-hu-investigates-llms-and-likely-applicationsTrojanNet â€“ a simple yet effective attack on machine learning modelsServicesArea Chair/Program Committee Member / Reviewer:Â Area Chair: ACL ARRConference: ICML, NeurIPS, ICLR, ARR, IJCAI, AAAI, SDM, CIKM, etc.Journal: PNAS, Nature Human Behavior, npj Digital Medicine, TIPAMI, TKDE, JAMIA,Â  SNAM, etc.Editorial Board:Â Associate Editor, Smart HealthRecognition:Top Reviewer of NeurIPS 2023.Google SitesReport abusePage detailsPage updated Google SitesReport abuse",
  "content_length": 7704,
  "method": "requests",
  "crawl_time": "2025-12-01 14:21:27"
}