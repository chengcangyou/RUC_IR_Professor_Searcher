{
  "name": "Tommi S. Jaakkola",
  "homepage": "https://people.csail.mit.edu/tommi",
  "status": "success",
  "content": "Tommi Jaakkola Tommi S. Jaakkola, Ph.D. Thomas Siebel Professor of Electrical Engineering and Computer Science and the Institute for Data, Systems, and Society MIT Computer Science and Artificial Intelligence Laboratory Stata Center, Bldg 32-G470 Cambridge, MA 02139 tommi at csail dot mit dot edu [home] [papers] [research] [people] Accessibility Research synopsis (projects) Our research advances how machines can learn, predict or control, and do so at scale in an efficient, principled, and interpretable manner. Our research in machine learning extends from foundational theory to modern applications, focusing especially on statistical inference and estimation tasks that lie at the heart of complex learning problems. We design new methods, theory and algorithms so as to automate the use and generation of semi-structured data such as natural language text, images, molecules, or strategies. We apply and develop our algorithms to solve multi-faceted recommender, retrieval, or inferential tasks (e.g., biomedical), design and optimize molecules or reactions for the purpose of drug design, and to model strategic, game theoretic interactions. People (more people) Julia Balla(c), Abhi Gupta, Cathy Cai(c), MinGyu Choi(c), Cameron Diao(c), Felix Faltings(c), Peter Holderrieth, Bowen Jing(c), Jeet Mohapatra, Amit Schechter, Ron Shprints, Hannes Stärk(c), Shangyuan Tong, Chenyu Wang, Maurice Weiler*, Cai Zhou(c) (* = postdoc, c = co-advised, v = visiting) New release: BoltzGen We introduce an all-atom generative model -- BoltzGen -- for designing proteins and peptides across all modalities to bind a wide range of biomolecular targets. BoltzGen builds strong structural reasoning capabilities about target-binder interactions into its generative design process and is controlled by a flexible design specification language. We experimentally validate these capabilities in a total of eight diverse wetlab design campaigns. Model weights, code for data, inference and training are released under the MIT license. H. Stärk, F. Faltings, M. Choi, Y. Xie, E. Hur, T. O Donnell, A. Bushuiev, T. Ucar, S. Passaro, W. Mao, M. Reveiz, R. Bushuiev, T. Pluskal, Josef Sivic, Karsten Kreis, A. Vahdat, S. Ray, J. Goldstein, A. Savinov, J. Hambalek, A. Gupta, D. Taquiri-Diaz, Y. Zhang, A. K. Hatstat, A. Arada, N. H. Kim, E. Tackie-Yarboi, D. Boselli, L. Schnaider, C. C. Liu, G.-W. Li, D. Hnisz, D. M. Sabatini, W. F. DeGrado, J. Wohlwend, G. Corso, R. Barzilay and T. Jaakkola. BoltzGen: Toward Universal Binder Design. Preprint. [bioRxiv],  [GitHub] Recent papers ( more papers, Google scholar, preprints on arXiv, preprints on bioRxiv ) C. Wang, C. Zhou, S. Gupta, Z. Lin, S. Jegelka, S. Bates, and T. Jaakkola. Learning diffusion models with flexible representation guidance. In Neural Information Processing Systems (NeurIPS), 2025. [link] C. Zhou, C. Wang, D. Zhang, S. Tong, Y. Wang, S. Bates, and T. Jaakkola. Next semantic scale prediction via hierarchical diffusion language models. In Neural Information Processing Systems (NeurIPS), 2025. M. Wu, C. Zhou, S. Bates, and T. Jaakkola. Thought calibration: Efficient and confident test-time scaling. In Empirical Methods in Natural Language Processing (EMNLP), 2025. [link] P. Holderrieth, M. Albergo, and T. Jaakkola. Leaps: A discrete neural sampler via locally equivariant networks. In International Conference on Machine Learning (ICML), 2025. [link] M. Wu, U. Padia, S. H. Murphy, R. Barzilay, and T. Jaakkola. Identifying biological perturbation targets through causal differential networks. In International Conference on Machine Learning (ICML), 2025. J. Mohapatra, N. Dehmamy, C. Both, S. Das, and T. Jaakkola. Symmetry-driven discovery of dynamical variables in molecular simulations. In International Conference on Machine Learning (ICML), 2025. P. Holderrieth, M. Havasi, J. Yim, N. Shaul, I. Gat, T. Jaakkola, B. Karrer, R. T. Q. Chen, and Y. Lipman. Generator matching: Generative modeling with arbitrary markov processes. In The 13th International Conference on Learning Representations (ICLR), 2025. [link] G. Corso, V. Ram Somnath, N. Getz, R. Barzilay, T. Jaakkola, and A. Krause. Composing unbalanced flows for flexible docking and relaxation. In The 13th International Conference on Learning Representations (ICLR), 2025. [link] H. Stärk, B. Jing, T. Geffner, J. Yim, T. Jaakkola, A. Vahdat, and K. Kreis. Protcomposer: Compositional protein structure generation with 3d ellipsoids. In The 13th International Conference on Learning Representations (ICLR), 2025. [link] C. Wang, S. Gupta, X. Zhang, S. Tonekaboni, S. Jegelka, T. Jaakkola, and C. Uhler. An information criterion for controlled disentanglement of multimodal data. In The 13th International Conference on Learning Representations (ICLR), 2025. [link] M. Karimi, S. Banerjee, T. Jaakkola, B. Dubrov, S. Shang, and R. Benson. Data distillation for extrapolative protein design through exact preference optimization. In The 13th International Conference on Learning Representations (ICLR), 2025. [link] C. Wang, M. Uehara, Y. He, A. Wang, T. Biancalani, A. Lal, T. Jaakkola, S. Levine, Hanchen, and A. Regev. Fine-tuning discrete diffusion models via reward optimization with applications to dna and protein design. In The 13th International Conference on Learning Representations (ICLR), 2025. [link] Y. Liu, S. Chang, T. Jaakkola, and Y. Zhang. Fictitious synthetic data can improve llm factuality via prerequisite learning. In The 13th International Conference on Learning Representations (ICLR), 2025. [link] S. Liu, J. Nam, A. Campbell, H. Stärk, Y. Xu, T. Jaakkola, and R. Gomez-Bombarelli. Think while you generate: Discrete diffusion with planned denoising. In The 13th International Conference on Learning Representations (ICLR), 2025. [link] P. Holderrieth, Y. Xu, and T. Jaakkola. Hamiltonian score matching and generative flows. In Neural Information Processing Systems (NeurIPS), 2024. [link] S. Gupta, C. Wang, Y. Wang, T. Jaakkola, and S. Jegelka. Symmetries in-context: Universal self-supervised learning through contextual world models. In Neural Information Processing Systems (NeurIPS), 2024. [link] X. Fu, A. S. Rosen, K. Bystrom, R. Wang, A. Musaelian, B. Kozinsky, T. Smidt, and T. Jaakkola. A recipe for charge density prediction. In Neural Information Processing Systems (NeurIPS), 2024. [link] N. Dehmamy, C. Both, J. Mohapatra, S. Das, and T. Jaakkola. Neural network reparametrization for accelerated optimization in molecular simulations. In Neural Information Processing Systems (NeurIPS), 2024. [link] B. Jing, H. Stärk, T. Jaakkola, and B. Berger. Generative modeling of molecular dynamics trajectories. In Neural Information Processing Systems (NeurIPS), 2024. [link] B. Jing, B. Berger, and T. Jaakkola. Alphafold meets flow matching for generating protein ensembles. In International Conference on Machine Learning (ICML), 2024. [link] A. Campbell, J. Yim, R. Barzilay, T. Rainforth, and T. Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. In International Conference on Machine Learning (ICML), 2024. [link] Y. Xu, G. Corso, T. Jaakkola, A. Vahdat, and K. Kreis. Disco-diff: Enhancing continuous diffusion models with discrete latents. In International Conference on Machine Learning (ICML), 2024. [link] H. Stärk, B. Jing, R. Barzilay, and T. Jaakkola. Harmonic self-conditioned flow matching for joint multi-ligand docking and binding site design. In International Conference on Machine Learning (ICML), 2024. [link] H. Stärk, B. Jing, C. Wang, G. Corso, B. Berger, R. Barzilay, and T. Jaakkola. Dirichlet flow matching with applications to dna sequence design. In International Conference on Machine Learning (ICML), 2024. [link] J. Yim, H. Stärk, G. Corso, B. Jing, R. Barzilay, and T. Jaakkola. Diffusion models in protein structure and docking. WIREs Computational Molecular Science, 14(2):e1711, 2024. [link] R. Okabe, A. Chotrattanapituk, A. Boonkird, N. Andrejevic, X. Fu, T. S. Jaakkola, Q. Song, T. Nguyen, N. Drucker, S. Mu, Y. Wang, B. Liao, Y. Cheng, and M. Li. Virtual node graph neural network for full phonon prediction. Nature Computational Science, 4(7), 2024. [link] Y. Liu, Y. Zhang, T. Jaakkola, and S. Chang. Correcting diffusion generation through resampling. In Computer Vision and Pattern Recognition (CVPR), 2024. [link] G. Corso, H. Stark, S. Jegelka, T. Jaakkola, and R. Barzilay. Graph neural networks. Nature Reviews Methods Primers, 4(17), 2024. [link] X. Fu, T. Xie, A. S. Rosen, T. Jaakkola, and J. A. Smith. Mofdiff: Coarse-grained diffusion for metal-organic framework design. In The 12th International Conference on Learning Representations (ICLR), 2024. [link] G. Corso, Y. Xu, V. De Bortoli, R. Barzilay, and T. Jaakkola. Particle guidance: non-i.i.d. diverse sampling with diffusion models. In The 12th International Conference on Learning Representations (ICLR), 2024. [link] G. Corso, A. Deng, N. Polizzi, R. Barzilay, and T. Jaakkola. Deep confident steps to new pockets: Strategies for docking generalization. In The 12th International Conference on Learning Representations (ICLR), 2024. [link] C. Wang, S. Gupta, C. Uhler, and T. Jaakkola. Removing biases from molecular representations via information maximization. In The 12th International Conference on Learning Representations (ICLR), 2024. [link] B. Jing, T. Jaakkola, and B. Berger. Learning scalar fields for molecular docking with fast fourier transforms. In The 12th International Conference on Learning Representations (ICLR), 2024. [link] A. Kirjner, J. Yim, R. Samusevich, S. Bracha, T. Jaakkola, R. Barzilay, and I. R. Fiete. Improving protein optimization with smoothed fitness landscapes. In The 12th International Conference on Learning Representations (ICLR), 2024. [link] V. Quach, A. Fisch, T. Schuster, A. Yala, J. H. Sohn, T. Jaakkola, and R. Barzilay. Conformal language modeling. In The 12th International Conference on Learning Repre",
  "content_length": 35545,
  "method": "requests",
  "crawl_time": "2025-12-01 14:39:59"
}