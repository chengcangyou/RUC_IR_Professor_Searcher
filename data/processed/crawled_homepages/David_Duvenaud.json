{
  "name": "David Duvenaud",
  "homepage": "http://www.cs.toronto.edu/~duvenaud",
  "status": "success",
  "content": "David Duvenaud David Kristjanson Duvenaud I'm an associate professor at the University of Toronto. My research focuses on AGI governance, evaluation, and mitigating catastrophic risks from future systems. I recently finished an extended sabbatical on the Alignment Science team at Anthropic. My past research focused on constructing deep probabilistic models to help predict, explain and design things. Some past highlights: Neural ODEs, a kind of continuous-depth neural network, Automatic chemical design using generative models, Gradient-based hyperparameter tuning, Structured latent-variable models for modeling video, and Convolutional networks on graphs. Before Toronto, I was a postdoc in the Harvard Intelligent Probabilistic Systems group with Ryan Adams. I did my Ph.D. at the University of Cambridge, where my advisors were Carl Rasmussen and Zoubin Ghahramani. My M.Sc. advisor was Kevin Murphy at the University of British Columbia. I'm also a founding member of the Vector Institute, a Sloan Research Fellow, and hold a Schwartz Reisman Chair in Technology and Society. Curriculum Vitae E-mail: duvenaud@cs.toronto.edu Teaching: STA414: Statistical Methods for Machine Learning II CSC2515: Machine Learning JSC370/470: Advanced Data Science CSC412: Probabilistic Learning and Reasoning CSC2547: Learning to Search STA4273: Learning Discrete Latent Structure CSC2541: Differentiable Inference and Generative Models Current students and postdocs: James Requeima Jesse Bettencourt Raymond Douglas How to join my group Give me anonymous feedback Selected papers Op-ed: Can we stop AI making humans obsolete? AI and robots might soon make humans obsolete â as both producers and consumers â leaving us in a vulnerable and unstable position, with little power over our lives and the future. David Duvenaud The Guardian, May 2025. article Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development AI risk scenarios usually portray a relatively sudden loss of human control to AIs, outmaneuvering individual humans and human institutions, due to a sudden increase in AI capabilities, or a coordinated betrayal. However, we argue that even an incremental increase in AI capabilities, without any coordinated power-seeking, poses a substantial risk of eventual human disempowerment. This loss of human influence will be centrally driven by having more competitive machine alternatives to humans in almost all societal functions, such as economic labor, decision making, artistic creation, and even companionship. Jan Kulveit, Raymond Douglas, Nora Ammann, Deger Turan, David Krueger, David Duvenaud Technical Report, 2025. arXiv | website version | coverage by Zvi Mowshowitz | 80k problem profile | slides | bibtex Sabotage Evaluations for Frontier Models Sufficiently capable models could subvert human oversight and decision-making in important contexts. For example, in the context of AI development, models could covertly sabotage efforts to evaluate their own dangerous capabilities, to monitor their behavior, or to make decisions about their deployment. We refer to this family of abilities as sabotage capabilities. We develop a set of related threat models and evaluations. These evaluations are designed to provide evidence that a given model, operating under a given set of mitigations, could not successfully sabotage a frontier model developer or other large organizationâs activities in any of these ways. We demonstrate these evaluations on Claude. Our results suggest that minimal mitigations are currently sufficient to address sabotage risks, but that more realistic evaluations and stronger mitigations seem likely to be necessary soon as capabilities improve. Finally, we discuss the advantages of mitigation-aware capability evaluations, and of simulating large-scale deployments using small-scale statistics. Joe Benton, Misha Wagner, Eric Christiansen, Cem Anil, Ethan Perez, Jai Srivastav, Esin Durmus, Deep Ganguli, Shauna Kravec, Buck Shlegeris, Jared Kaplan, Holden Karnofsky, Evan Hubinger, Roger Grosse, Samuel R. Bowman, David Duvenaud Technical Report, 2024. paper | blog post | short video | bibtex LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language It's hard to incorporate knowledge and beliefs into predictive models. We'd like to build a regression model that can process numerical data and make probabilistic predictions at arbitrary locations, guided by natural language. We start by exploring strategies for eliciting explicit, coherent numerical predictive distributions from LLMs. We examine these joint predictive distributions, which we call LLM Processes, over arbitrarily-many quantities in settings such as forecasting, multi-dimensional regression, black-box optimization, and image modeling. We investigate the practical details of prompting to elicit coherent predictive distributions, and demonstrate their effectiveness at regression. Finally, we demonstrate the ability to usefully incorporate text into numerical predictions. This lets us begin to explore the rich, grounded hypothesis space that LLMs implicitly encode. James Requeima*, John Bronskill*, Dami Choi, David Duvenaud, Richard E. Turner , Neural Information Processing Systems, 2024. paper | slides | bibtex | twitter thread Many-shot Jailbreaking We investigate a family of simple long-context attacks on large language models: prompting with hundreds of demonstrations of undesirable behavior. This is newly feasible with the larger context windows recently deployed by Anthropic, OpenAI and Google DeepMind. We find that in diverse, realistic circumstances, the effectiveness of this attack follows a power law, up to hundreds of shots. We demonstrate the success of this attack on the most widely used state-of-the-art closed-weight models, and across various tasks. Our results suggest very long contexts present a rich new attack surface for LLMs. Cem Anil, Esin Durmus, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Nina Rimsky, Meg Tong, Jesse Mu, Daniel Ford, Francesco Mosconi, ..., Jared Kaplan, Deep Ganguli, Samuel R. Bowman, Ethan Perez, Roger Grosse, David Duvenaud Neural Information Processing Systems, 2024. paper | blog post | bibtex Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs We propose a general strategy for teaching a model to quantify its own epistemic uncertainty: train it to predict pairs of independent responses drawn from the true conditional distribution, allow it to \"cheat\" by observing one response while predicting the other, then measure how much it cheats. Remarkably, we prove that being good at cheating is equivalent to being second-order calibrated, a principled extension of ordinary calibration that allows us to construct provably-correct frequentist confidence intervals for p(Y|X) and detect incorrect responses with high probability. We demonstrate empirically that our approach accurately estimates how much models don't know across a variety of tasks. Daniel D. Johnson, Daniel Tarlow, David Duvenaud, Chris Maddison International Conference on Machine Learning, 2024. paper | code | bibtex Towards Understanding Sycophancy in Language Models Human feedback encourages model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure makes use of human feedback, and the role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses. Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, Ethan Perez International Conference on Learning Representations, 2024. paper | code Sorting Out Quantum Monte Carlo Molecular modeling at the quantum level requires choosing a parameterization of the wavefunction that both respects the required particle symmetries, and is scalable to systems of many particles. For the simulation of fermions, valid parameterizations must be antisymmetric with respect to the exchange of particles. Typically, antisymmetry is enforced by leveraging the anti-symmetry of determinants with respect to the exchange of matrix rows, but this involves computing a full determinant each time the wavefunction is evaluated. Instead, we introduce a new antisymmetrization layer derived from sorting, the sortlet, which scales as O(NlogN) with regards to the number of particles -- in contrast to O(N3) for the determinant. We show numerically that applying this anti-symmeterization layer on top of an attention based neural-network backbone yields a flexible wavefunction parameterization capable of reaching chemical accuracy when approximating the ground state of first-row atoms and small molecules. Luca Thiede, Jack Richter-Powell, AlÃ¡n Aspuru-Guzik, David Duvenaud In Submission, 2023. paper | code Tools for Verifying Neural Models' Training Data What could a ânuclear inspectorâ for large neural models check if they had access to the training checkpoints? We propose a simple protocol for verifying claims about very large SGD training runs. Specifically, we introduce a protocol that allows a model trainer to convince a verifier of the training data that produced a set of model weights. We explore efficient verification strategies for Proof-of-Training-Data that are compatible with most current large-model training procedures. We show experimentally that our verification procedures can catch a wide variety of attacks. Dami Choi, Yonadav S",
  "content_length": 42674,
  "method": "requests",
  "crawl_time": "2025-12-01 12:58:50"
}