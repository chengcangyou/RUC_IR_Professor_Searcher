{
  "name": "Mikhail Kapralov",
  "homepage": "http://theory.epfl.ch/kapralov",
  "status": "success",
  "content": "Michael Kapralov Michael Kapralov I am an Associate Professor in the School of Computer and Communication Sciences at EPFL, and part of the Theory Group at EPFL. Email: michael dot kapralov at epfl dot ch Phone: +41 21 69 38143 Address: EPFL IC Building INJ (INJ113) Station 14 CH-1015 Lausanne Switzerland Interested in working with me? Apply to our PhD program and mention my name on your application. For an undergraduate summer internship, apply to the Summer@EPFL program. Multiple postdoctoral positions available in the Theory Group: apply here. Research: I am broadly interested in theoretical computer science, with an emphasis on theoretical foundations of big data analysis. Most of my algorithmic work is in sublinear algorithms, where specific directions include streaming, sketching, sparse recovery and Fourier sampling. Bio: I completed my PhD at Stanford iCME, where I was advised by Ashish Goel. After graduating from Stanford I spent two years as a postdoc at the Theory of Computation Group at MIT CSAIL, working with Piotr Indyk, and then a year at the IBM T. J. Watson Research Center as a Herman Goldstine Postdoctoral Fellow. I am grateful to the European Research Council for their generous support (ERC Starting Grant SUBLINEAR, 2018-2023). We are organizing the Turing Course, which introduces high school students to some of the gems of computer science. Reading group on the Foundations of Deep Learning Theory Coffee EPFL Theory Seminar Swiss Winter School on Theoretical Computer Science (application deadline: October 24th). I am very fortunate to be advising/have advised/co-advised several amazing PhD students and postdocs: Ekaterina Kochetkova Guy Weissenberg Davide Mazzali Weronika Wrzos-Kaminska Kshiteej Sheth Mikhail Makarov Grzegorz Gluch (postdoc at UC Berkeley) Yu Chen (Assistant Professor at the National University of Singapore) Akash Kumar (Assistant Professor at IIT Bombay) Marek Elias (Assistant Professor at Bocconi) Jakab Tardos (Google Zurich) Navid Nouri (BlockTech B.V.) Ashish Chiplunkar (Assistant Professor at IIT Delhi) Aida Mousavifar (Google Zurich) Ameya Velingker (Senior Research Scientist at Google Mountain View) Amir Zandieh (Research Scientist at Google Zurich) Recent papers: Recovering Communities in Structured Random Graphs, to appear in ITCS 2026 M. Kapralov, L. Trevisan, W. Wrzos-Kaminska Spectral Clustering with Side Information, to appear in SODA 2026 H. Fichtenberger, M. Kapralov, E. Kochetkova, S. Lattanzi, D. Mazzali, W. Wrzos-Kaminska Sublinear Time Low-Rank Approximation of Hankel Matrices, to appear in SODA 2026 M. Kapralov, C. Musco and K. Sheth Spectral clustering in birthday paradox time, to appear in SODA 2026 M. Kapralov, E. Kochetkova and W. Wrzos-Kaminska Streaming Attention Approximation via Discrepancy Theory, to appear in NeurIPS 2025 (Spotlight Paper) E. Kochetkova, K. Sheth, I. Han, A. Zandieh, M. Kapralov Generalized Flow in Nearly-linear Time on Moderately Dense Graphs, to appear in FOCS 2025 S. Jiang, M. Kapralov, L. Er Lu and A. Sidford Approximating Dasgupta Cost in Sublinear Time from a Few Random Seeds, ICALP 2025 M.Kapralov, A. Kumar, S. Lattanzi, A. Mousavifar, W. Wrzos-Kaminska On the Adversarial Robustness of Locality-Sensitive Hashing in Hamming Space, PODS 2025 M. Kapralov, M. Makarov, C. Sohler Improved Algorithms for Kernel Matrix-Vector Multiplication, ICLR 2025 Best Paper at ICML 2024 workshop on Long Context Foundation Models P. Indyk, M. Kapralov, K. Sheth, T. Wagner On the Robustness of Spectral Algorithms for Semirandom Stochastic Block Models, NeurIPS 2024 A. Bhaskara, A. Jha, M. Kapralov, N. Manoj, D. Mazzali and W. Wrzos-Kaminska On the Streaming Complexity of Expander Decomposition, ICALP 2024 Y. Chen, M. Kapralov, M. Makarov, D. Mazzali Streaming Algorithms for Connectivity Augmentation, ICALP 2024 C. Jin, M. Kapralov, S. Mahabadi, A. Vakilian A Quasi-Monte Carlo Data Structure for Smooth Kernel Evaluation, SODA 2024 M. Charikar, M. Kapralov, E. Waingarten On Constructing Spanners from Random Gaussian Projections, RANDOM 2023 S. Assadi, M. Kapralov, H. Yu Expander Decomposition in Dynamic Streams, ITCS 2023 A. Filtser, M. Kapralov, M. Makarov Toeplitz Low-Rank Approximation with Sublinear Query Complexity, SODA 2023 M. Kapralov, H. Lawrence, M. Makarov, C. Musco and K. Sheth Learning Hierarchical Cluster Structure of Graphs in Sublinear Time, SODA 2023 M. Kapralov, A. Kumar, S. Lattanzi and A. Mousavifar Traversing the FFT Computation Tree for Dimension-Independent Sparse Fourier Transforms, SODA 2023 K. Bringmann, M. Kapralov, M. Makarov, V. Nakos, A. Yagudin and A. Zandieh Motif Cut Sparsifiers, FOCS 2022 M. Kapralov, M. Makarov, S. Silwal, C. Sohler and J. Tardos Factorial Lower Bounds for (Almost) Random Order Streams, FOCS 2022 A. Chiplunkar, J. Kallaugher, M. Kapralov and E. Price Practice of Streaming Processing of Dynamic Graphs: Concepts, Models, and Systems, to appear in IEEE Transactions on Parallel and Distributed Systems M. Besta, M. Fischer, V. Kalavri, M. Kapralov, T. Hoefler Noisy Boolean Hidden Matching with Applications, ITCS 2022 M. Kapralov, A. Musipatla, J. Tardos, D. Woodruff, S. Zhou Simulating Random Walks in Random Streams, SODA 2022 J. Kallaugher, M. Kapralov and E. Price Efficient and Local Parallel Random Walks, NeurIPS 2021 M. Kapralov, S. Lattanzi, N. Nouri and J. Tardos Spectral Hypergraph Sparsifiers of Nearly Linear Size, FOCS 2021 M. Kapralov, R. Krauthgamer, J. Tardos and Y. Yoshida Towards Tight Bounds for Spectral Sparsification of Hypergraphs, STOC 2021 M. Kapralov, R. Krauthgamer, J. Tardos and Y. Yoshida Communication Efficient Coresets for Maximum Matching, SOSA 2021 M. Kapralov, G. Maystre and J. Tardos Space Lower Bounds for Approximating Maximum Matching in the Edge Arrival Model, SODA 2021 M. Kapralov Graph Spanners by Sketching in Dynamic Streams and the Simultaneous Communication Model, SODA 2021 A. Filtser, M. Kapralov and N. Nouri Spectral Clustering Oracles in Sublinear Time, SODA 2021 G. Gluch, M. Kapralov, S. Lattanzi, A. Mousavifar and C. Sohler Kernel Density Estimation through Density Constrained Near Neighbor Search, FOCS 2020 M. Charikar, M. Kapralov, N. Nouri and P. Siminelakis Scaling up Kernel Ridge Regression via Locality Sensitive Hashing, AISTATS 2020 M. Kapralov, N. Nouri, I. Razenshteyn, A. Velingker and A. Zandieh Space Efficient Approximation to Maximum Matching Size from Uniform Edge Samples, SODA 2020 M. Kapralov, S. Mitrovic, A. Norouzi-Fard and J. Tardos Differentially Private Release of Synthetic Graphs, SODA 2020 M. Elias, M. Kapralov, J. Kulkarni and Y. T. Lee Oblivious Sketching of High-Degree Polynomial Kernels, SODA 2020 T. D. Ahle, M. Kapralov, J. B. T. Knudsen, R. Pagh, A. Velingker, D. Woodruff and A. Zandieh (Merged version of this paper and this paper) Fast and Space Efficient Spectral Sparsification in Dynamic Streams, SODA 2020 M. Kapralov, A. Mousavifar, C. Musco, C. Musco, N. Nouri, A. Sidford and J. Tardos (Merged version of this paper and this paper) Efficiently Learning Fourier Sparse Set Functions, NeurIPS 2019 (spotlight presentation) A. Amrollahi, A. Zandieh, M. Kapralov, A. Krause Online Matching with General Arrivals, FOCS 2019 B. Gamlath, M. Kapralov, A. Maggiori, O. Svensson, D. Wajc An Optimal Space Lower Bound for Approximating MAX-CUT, STOC 2019 M. Kapralov, D. Krachun A Universal Sampling Method for Reconstructing Signals with Simple Fourier Transforms, STOC 2019 H. Avron, M. Kapralov, C. Musco, C. Musco, A. Velingker and A. Zandieh A Simple Sublinear-Time Algorithm for Counting Arbitrary Subgraphs via Edge Sampling, ITCS 2019 S. Assadi, M. Kapralov, S. Khanna Dimension-independent Sparse Fourier transform, SODA 2019 M. Kapralov, A. Velingker, A. Zandieh Testing Graph Clusterability: Algorithms and Lower Bounds, FOCS 2018 A. Chiplunkar, M. Kapralov, S. Khanna, A. Mousavifar and Y. Peres [Abstract] We consider the problem of testing graph cluster structure: given access to a graph $G=(V, E)$, can we quickly determine whether the graph can be partitioned into a few clusters with good inner conductance, or is far from any such graph? This is a generalization of the well-studied problem of testing graph expansion, where one wants to distinguish between the graph having good expansion (i.e. being a good single cluster) and the graph having a sparse cut (i.e. being a union of at least two clusters). A recent work of Czumaj, Peng, and Sohler (STOC'15) gave an ingenious sublinear time algorithm for testing $k$-clusterability in time $\\tilde{O}(n^{1/2} \\poly(k))$: their algorithm implicitly embeds a random sample of vertices of the graph into Euclidean space, and then clusters the samples based on estimates of Euclidean distances between the points. This yields a very efficient testing algorithm, but only works if the cluster structure is very strong: it is necessary to assume that the gap between conductances of accepted and rejected graphs is at least logarithmic in the size of the graph $G$. In this paper we show how one can leverage more refined geometric information, namely angles as opposed to distances, to obtain a sublinear time tester that works even when the gap is a sufficiently large constant. Our tester is based on the singular value decomposition of a natural matrix derived from random walk transition probabilities from a small sample of seed nodes. We complement our algorithm with a matching lower bound on the query complexity of testing clusterability. Our lower bound is based on a novel property testing problem, which we analyze using Fourier analytic tools. As a byproduct of our techniques, we also achieve new lower bounds for the problem of approximating MAX-CUT value in sublinear time. The Sketching Complexity of Graph and Hypergraph Counting, FOCS 2018 J. Kallaugher, M. Kapralov and E. Price [Abstract] Subgraph counting is a fundamental primitive in graph processing, with applications in social network analysis (e.g.,\\ ",
  "content_length": 62905,
  "method": "requests",
  "crawl_time": "2025-12-01 13:59:54"
}