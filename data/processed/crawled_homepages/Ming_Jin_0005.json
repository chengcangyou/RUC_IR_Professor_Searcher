{
  "name": "Ming Jin 0005",
  "homepage": "https://mingjin.dev",
  "status": "success",
  "content": "Ming Jin's Homepage Ming Jin Assistant Professor (Lecturer) @ Griffith University Office: 1.14_N44, 170 Kessels Rd, Nathan QLD 4111, Australia Email: mingjinedu AT gmail DOT com [Google Scholar] [Github] [Linkedin] [University Profile] About Me I am currently an assistant professor at School of Information and Communication Technology (ICT), Griffith University. Prior to this, I obtained my Ph.D. degree in the Faculty of Information Technology at Monash University in 2024. I specialize in time series analysis and spatio-temporal data mining, with a good track record of publishing high-impact papers in top-ranked venues, including NeurIPS, ICLR, ICML, KDD, and TPAMI, among others. My research works were selected as Most Influential Papers, with some having become widely used baselines, gaining substantial recognition in the open-source community. I also server as an associate editor for Neurocomputing (JCR Q1 IF 6.5) and actively contributes as an area chair or senior program committee member for prestigious international conferences. I am dedicated to conducting high-impact research and open for collaborations. My research interests are broadly in (1) time series analytics, (2) spatio-temporal data mining, and (3) multimodal learning with a special focus on temporal settings (e.g., physical AI on time series and spatio-temporal data) in solving both fundamental and real-world problems. ðŸ“Œ Opening Ph.D./intern positions. I am looking for (1) well-matched (w.r.t. research interests), (2) qualified, and (3) highly self-motivated candidates to work with me. If you are interested, please fill this Google Form and I will be in touch. Scholarships may be available in 2026/2027. Due to the overwhelming volume of emails I receive daily, I kindly apologize in advance for not being able to respond to each one individually. Research Interests I work in the field of time series and spatio-temporal data mining, primarily investigating the development and application of advanced machine learning techniques to understand complex patterns in temporal data. I focus on the following research topics: Time Series Intelligence (e.g., pattern discovery, foundation and reasoning models) Spatio-temporal Data Mining (e.g., learning on evolving graphs and AI for transportation/climate science) Multimodal AI (e.g., time series language models and agentic AI for healthcare applications) Trustworthy AI (e.g., model transparency and robustness) Recent News [11/2025] Our paper FaST: Efficient and Effective Long-Horizon Forecasting for Large-Scale Spatial-Temporal Graphs via Mixture-of-Experts has been accepted by KDD 2026 (CORE A*) [11/2025] Our paper TimeDistill: Efficient Long-Term Time Series Forecasting with MLP via Cross-Architecture Distillation has been accepted by KDD 2026 (CORE A*) [11/2025] The 2nd International Workshop on Spatio-Temporal Data Mining from the Web has been accepted by WWW 2026 [10/2025] A Survey on Diffusion Models for Time Series and Spatio-Temporal Data has been accepted by ACM Computing Survey [09/2025] Our paper ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models has been accepted by NeurIPS 2025 (CORE A*) [09/2025] Our benchmark paper of Self-Correction in LLMs has been accepted by NeurIPS 2025 (CORE A*) [09/2025] Our paper Multi-Scale Fine-tuning for Encoder-based Time Series Foundation Models has been accepted by NeurIPS 2025 (CORE A*) [09/2025] Our paper Continuous State Space Modeling on Dynamic Graphs has been accepted by NeurIPS 2025 (CORE A*) [09/2025] I have secured NVIDIA Academic Grant on developing next-gen time series analytical engines [09/2025] I will serve as an Area Chair for ICLR 2026 and ICASSP 2026 [09/2025] Our tutorial on Foundation Models for Time Series Analysis has been accepted by AAAI 2026 (CORE A*) [08/2025] Our paper Test-time GNN Model Evaluation on Dynamic Graphs has been accepted by ICDM 2025 (CORE A*) [08/2025] Our applied research on salinity prediction using sparse drifter data has been accepted by CIKM 2025 (CORE A) [07/2025] Our applied research Large lithium-ion battery model for secure shared electric bike battery in smart cities has been accepted by Nature Communications (IF 15.7) [07/2025] Our applied research on supply chain forecasting with time series foundation model has been accepted by ITSC 2025 [06/2025] Honored to serve on the Program Committee of IEEE CIS Task Force on AI for Time Series and Spatio-Temporal Data [05/2025] Our paper Time-MQA: Time Series Multi-Task Question Answering with Context Enhancement has been accepted by ACL 2025 (CORE A*) [05/2025] Our paper Flow-based Reconcile Transformer for Hierarchical Time Series has been accepted by KDD 2025 (CORE A*) [05/2025] Our tutorial on Foundation Models for Spatio-Temporal Data Science has been accepted by KDD 2025 (CORE A*) [05/2025] Our paper Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time Series Forecasting has been accepted by ICML 2025 (CORE A*) [04/2025] Our paper T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models has been accepted by IJCAI 2025 (CORE A*) [04/2025] Honored to deliver a lecture-style tutorial on \"Time Series Analysis in the Web\" at WWW 2025 (CORE A*) [02/2025] Our paper Towards Expressive Spectral-Temporal Graph Neural Networks for Time Series Forecasting has been accepted by IEEE TPAMI (IF 20.8) [02/2025] Honored to deliver a lecture-style tutorial on \"Time Series Foundation Models\" at AAAI 2025 (CORE A*) [01/2025] Our paper TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis has been accepted by ICLR 2025 (CORE A*) as an Oral (Top 1.8%) ðŸŽŠðŸ¥³ [01/2025] Our paper Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts has been accepted by ICLR 2025 (CORE A*) as a Spotlight (Top 5%) ðŸŽŠðŸ¥³ [01/2025] Our paper Towards Neural Scaling Laws for Time Series Foundation Models has been accepted by ICLR 2025 (CORE A*) [12/2024] We are organizing AI4TS-25 Workshop at TheWebConf 2025 (CORE A*) and now calling for fast-track submissions [12/2024] We are organizing WebST-25 Workshop at TheWebConf 2025 (CORE A*) and now calling for fast-track submissions [12/2024] Our tutorial on Time Series Analysis in the Web: Recent Advances and Future Trends has been accepted by TheWebConf 2025 (CORE A*) [10/2024] Our paper Scaling to Billion Parameters for Time Series Foundation Models with Mixture of Experts has been accepted by NeurIPS 2024 TSALM Workshop [10/2024] I will give another talk at MLBoost about \"Time Series Foundation Models\" [YouTube Live] Selected Publications [Full List] (^Co-first author; *Corresponding author) ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models Bosong Huang, Ming Jin*, Yuxuan Liang, Johan Barthelemy, Debo Cheng, Qingsong Wen, Chenghao Liu, Shirui Pan* Annual Conference on Neural Information Processing Systems (NeurIPS), 2025. [Paper] [GitHub] T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models Yunfeng Ge, Jiawei Li, Yiji Zhao, Haomin Wen, Zhao Li, Meikang Qiu, Hongyan Li, Ming Jin*, Shirui Pan* International Joint Conference on Artificial Intelligence (IJCAI), 2025. [Paper] [GitHub] Time-MQA: Time Series Multi-Task Question Answering with Context Enhancement Yaxuan Kong^, Yiyuan Yang^, Yoontae Hwang, Wenjie Du, Stefan Zohren, Zhangyang Wang, Ming Jin*, Qingsong Wen* Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL), 2025. [Paper] [Hugging Face] Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts Xiaoming Shi^, Shiyu Wang^, Yuqi Nie^, Dianqi Li, Zhou Ye, Qingsong Wen, Ming Jin* International Conference on Learning Representations (ICLR Spotlight), 2025. [Paper] [GitHub] [Hugging Face] TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis Shiyu Wang^, Jiawei Li^, Xiaoming Shi, Zhou Ye, Baichuan Mo, Wenze Lin, Shengtong Ju, Zhixuan Chu*, Ming Jin* International Conference on Learning Representations (ICLR Oral), 2025. [Paper] [GitHub] Towards Neural Scaling Laws for Time Series Foundation Models Qingren Yao, Chao-Han Huck Yang, Renhe Jiang, Yuxuan Liang, Ming Jin*, Shirui Pan* International Conference on Learning Representations (ICLR), 2025. [Paper] [GitHub] A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection Ming Jin^, Huan Yee Koh^, Qingsong Wen, Daniele Zambon, Cesare Alippi, Geoffrey I Webb, Irwin King, Shirui Pan IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2024. [Paper] [GitHub] Time-LLM: Time Series Forecasting by Reprogramming Large Language Models Ming Jin^, Shiyu Wang^, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, Qingsong Wen International Conference on Learning Representations (ICLR), 2024. [Paper] [GitHub] Towards Expressive Spectral-Temporal Graph Neural Networks for Time Series Forecasting Ming Jin^, Guangsi Shi^, Yuan-Fang Li, Bo Xiong, Tian Zhou, Flora Salim, Liang Zhao, Lingfei Wu, Qingsong Wen, Shirui Pan IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2025. [Paper] [GitHub] Position: What Can Large Language Models Tell Us about Time Series Analysis Ming Jin^, Yifan Zhang^, Wei Chen^, Kexin Zhang, Yuxuan Liang, Bin Yang, Jindong Wang, Shirui Pan, Qingsong Wen International Conference on Machine Learning (ICML), 2024. [Paper] Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang, James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, Shirui Pan, Vincent S Tseng, Yu Zheng, Lei Chen, Hui Xiong arXiv preprint, 2023. [Paper] [GitHub] Neural Temporal Walks: Motif-Aware Representation Learning on Continuous-Time Dynamic Graphs Ming Jin, Yuan-Fang Li, Shirui Pan Annual Conference on Neural Information Processing Syste",
  "content_length": 14515,
  "method": "requests",
  "crawl_time": "2025-12-01 14:00:15"
}