{
  "name": "Nihar Bhadresh Shah",
  "homepage": "http://www.cs.cmu.edu/~nihars",
  "status": "success",
  "content": "Nihar B. Shah - CMU I am an associate professor at Carnegie Mellon University, with joint appointments in the Machine Learning and Computer Science departments. My group's research is centered around the science of evaluation and the evaluation of science. We develop algorithms with strong theoretical guarantees, as well as conduct large-scale controlled experiments for evidence-based policy design and real-world deployments. Our research uses human-AI collaboration to address fundamental questions about scientific work: Is the research correct? Is it high-quality? Is it fundable, or even authentic? These questions are critical because inaccurate or flawed scientific findings can disrupt subsequent research, resulting in significant setbacks to scientific advancement. Additionally, problems in research evaluation affect billions of dollars in funding decisions, have directly contributed to fatalities in areas such as biomedicine, and impose substantial stress on students whose careers depend on fair assessment. Moreover, ensuring the integrity and accuracy of published science is crucial for maintaining public trust. Our research has already been used in the evaluation of over 100,000 research papers and thousands of grant proposals, in over 200 venues. Beyond science, the challenges our research addresses extend naturally into other domains, and the algorithms we have developed are also deployed in diverse applications such as admissions decisions and competition judging. I tend to keep my research group relatively small and work very closely with all of my students.\tI am taking PhD students in Fall 2026. If you are interested, please apply to the Machine Learning or Computer Science departments and include my name in your application. Survey on Challenges, Solutions, and Experiments in Peer Review, and associated tutorial slides Slides on the current State of the Review-nion Blog on various aspects of academia, research, and peer review Email: nihars [at] cs.cmu.edu Office: GHC 8211 PUBLICATIONS Who is a Better Matchmaker? Human vs. Algorithmic Judge Assignment in a High-Stakes Startup Competition Sarina Xi, Orelia Pi, Miaomiao Zhang, Becca Xiong, Jacqueline Ng Lane, Nihar B. ShahIAAI 2026 The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems Ziming Luo, Atoosa Kasirzadeh, Nihar B. Shah Designing Rules to Pick a Rule: Aggregation by Consistency Ratip Emin Berker, Ben Armstrong, Vincent Conitzer, Nihar B. Shah Identity Theft in AI Conference Peer Review Nihar B. Shah, Melisa Bok, Xukun Liu, Andrew McCallumCommunications of the ACM (to appear) A Principled Approach to Randomized Selection under Uncertainty: Applications to Peer Review and Grant Funding Alexander Goldberg, Giulia Fanti, Nihar ShahNeurIPS 2025Blog Detecting LLM-Generated Peer Reviews Vishisht Rao, Aounon Kumar, Himabindu Lakkaraju, Nihar ShahPLOS ONE (to appear) Benchmarking Fraud Detectors on Private Graph Data Alexander Goldberg, Giulia Fanti, Nihar Shah, Steven WuKDD 2025 Enhancing Peer Review in Astronomy: A Machine Learning and Optimization Approach to Reviewer Assignments for ALMA John M. Carpenter, Andrea CorvillÃ³n, and Nihar ShahPublications of the Astronomical Society of the Pacific 2025 Vulnerability of Text-Matching in ML/AI Conference Reviewer Assignments to Collusions Jhih-Yi (Janet) Hsieh, Aditi Raghunathan, Nihar ShahUSENIX Security Symposium 2025 Causal Effect of Group Diversity on Redundancy and Coverage in Peer-ReviewingNavita Goyal, Ivan Stelmakh, Nihar Shah, Hal DaumÃ© III Usefulness of LLMs as an Author Checklist Assistant for Scientific Papers: NeurIPS'24 Experiment Alexander Goldberg, Ihsan Ullah, Thanh Gia Hieu Khuong, Benedictus Kent Rachmat, Zhen Xu, Isabelle Guyon, Nihar Shah What Can Natural Language Processing Do for Peer Review? Ilia Kuznetsov, Osama Mohammed Afzal, Koen Dercksen, Nils Dycke, Alexander Goldberg, Tom Hope, Dirk Hovy, Jonathan K. Kummerfeld, Anne Lauscher, Kevin Leyton-Brown, Sheng Lu, Mausam, Margot Mieskes, AurÃ©lie NÃ©vÃ©ol, Danish Pruthi, Lizhen Qu, Roy Schwartz, Noah A. Smith, Thamar Solorio, Jingyan Wang, Xiaodan Zhu, Anna Rogers, Nihar Shah, Iryna Gurevych A Randomized Controlled Trial on Anonymizing Reviewers to Each Other in Peer Review DiscussionsCharvi Rastogi, Xiangchen Song, Zhijing Jin, Ivan Stelmakh, Hal DaumÃ© III, Kun Zhang, and Nihar ShahPLOS ONE 2024. On the Detection of Reviewer-Author Collusion Rings From Paper BiddingSteven Jecmen, Nihar Shah, Fei Fang, and Leman AkogluTransactions on Machine Learning Research 2024 Peer Reviews of Peer Reviews: A Randomized Controlled Trial and Other ExperimentsAlexander Goldberg, Ivan Stelmakh, Kyunghyun Cho, Alice Oh, Alekh Agarwal, Danielle Belgrave, and Nihar ShahBlogPLOS ONE 2025 Testing for Reviewer Anchoring in Peer Review: A Randomized Controlled TrialRyan Liu, Steven Jecmen, Vincent Conitzer, Fei Fang, Nihar B. ShahPLOS ONE 2024 ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper ReviewingRyan Liu, Nihar B. ShahAAAI workshop on Scientific Document Understanding 2024. A Gold Standard Dataset for the Reviewer Assignment ProblemIvan Stelmakh, John Wieting, Sarina Xi, Graham Neubig, Nihar B. ShahTransactions on Machine Learning Research, 2025 Assisting Human Decisions in Document MatchingJoon Sik Kim, Valerie Chen, Danish Pruthi, Nihar B. Shah, Ameet TalwalkarTransactions on Machine Learning Research 2023 The Role of Author Identities in Peer ReviewNihar B. ShahPLOS ONE, 2023 Batching of Tasks by Users of Pseudonymous Forums: Anonymity Compromise and ProtectionAlexander Goldberg, Giulia Fanti, Nihar B. ShahACM SIGMETRICS 2023 How do Authorsâ Perceptions of their Papers Compare with Co-authorsâ Perceptions and Peer-review Decisions?Charvi Rastogi, Ivan Stelmakh, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, Jennifer Wortman Vaughan, Zhenyu Xue, Hal DaumÃ© III, Emma Pierson, and Nihar B. ShahPLOS ONE 2024 Near-Optimal Reviewer Splitting in Two-Phase Paper Reviewing and Conference Experiment DesignSteven Jecmen, Hanrui Zhang, Ryan Liu, Fei Fang, Vincent Conitzer, and Nihar B. ShahHCOMP 2022 Best Paper Honorable Mention Strategyproofing Peer Assessment via Partitioning: The Price in Terms of Evaluators' ExpertiseKomal Dhull, Steven Jecmen, Pravesh Kothari, Nihar B. ShahHCOMP 2022Code A Dataset on Malicious Paper Bidding in Peer Review Steven Jecmen, Minji Yoon, Vincent Conitzer, Nihar B. Shah, Fei FangTheWebConf 2023 Counterfactual Evaluation of Peer-Review Assignment Policies Martin Saveski, Steven Jecmen, Nihar Shah, Johan UganderNeurIPS 2023 To ArXiv or not to ArXiv: A Study Quantifying Pros and Cons of Posting Preprints Online Charvi Rastogi, Ivan Stelmakh, Xinwei Shen, Marina Meila, Federico Echenique, Shuchi Chawla, Nihar B. ShahPeer review congress (abstract), 2022 Cite-seeing and Reviewing: A Study on Citation Bias in Peer Review Ivan Stelmakh, Charvi Rastogi, Ryan Liu, Shuchi Chawla, Federico Echenique, Nihar B. ShahPLOS ONE, 2023 No Rose for MLE: Inadmissibility of MLE for Evaluation Aggregation Under Levels of ExpertiseCharvi Rastogi, Ivan Stelmakh, Nihar B. Shah, Sivaraman BalakrishnanISIT 2022 Integrating Rankings into Quantized Scores in Peer Review Yusha Liu, Yichong Xu, Nihar B. Shah, Aarti SinghTMLR 2022Workshop on ML Evaluation Standards at ICLR 2022 Outstanding Paper Award and People's Choice Award Tradeoffs in Preventing Manipulation in Paper Bidding for Reviewer Assignment Steven Jecmen, Nihar B. Shah, Fei Fang, Vincent ConitzerWorkshop on ML Evaluation Standards at ICLR 2022 Outstanding Paper Award Calibration with Privacy in Peer ReviewWenxin Ding, Gautam Kamath, Weina Wang, and Nihar B. ShahISIT 2022 Uncovering Latent Biases in Text: Method and Application to Peer ReviewEmaad Manzoor and Nihar B. ShahAAAI 2021.Code Mitigating Manipulation in Peer Review via Randomized Reviewer AssignmentsSteven Jecmen, Hanrui Zhang, Ryan Liu, Nihar B. Shah, Vincent Conitzer, and Fei FangNeurIPS 2020.Code Prior and Prejudice: The Novice Reviewers' Bias against Resubmissions in Conference Peer Review.Ivan Stelmakh, Nihar B. Shah, Aarti Singh and Hal DaumÃ© IIICSCW 2021. A Large Scale Randomized Controlled Trial on Herding in Peer-Review Discussions.Ivan Stelmakh, Charvi Rastogi, Nihar B. Shah, Aarti Singh and Hal DaumÃ© IIIPLOS ONE 2023. A Novice-Reviewer Experiment to Address Scarcity of Qualified Reviewers in Large ConferencesIvan Stelmakh, Nihar B. Shah, Aarti Singh and Hal DaumÃ© IIIAAAI 2021. Reviewer guidelines Catch Me if I Can: Detecting Strategic Behaviour in Peer AssessmentIvan Stelmakh, Nihar B. Shah and Aarti SinghAAAI 2021. Data Debiasing Evaluations that are Biased by EvaluationsJingyan Wang, Ivan Stelmakh, Yuting Wei and Nihar B. ShahAAAI 2021. On the Privacy-Utility Tradeoff in Peer-Review Data AnalysisWenxin Ding, Nihar B. Shah, and Weina WangAAAI Privacy-Preserving Artificial Intelligence (PPAI-21) workshop 2021. On Testing for Biases in Peer ReviewIvan Stelmakh, Nihar B. Shah and Aarti SinghNeurIPS 2019. A SUPER* Algorithm to Optimize Paper Bidding in Peer ReviewTanner Fiez, Nihar B. Shah and Lillian RatliffUAI 2020Code PeerReview4All: Fair and Accurate Reviewer Assignment in Peer ReviewIvan Stelmakh, Nihar B. Shah and Aarti SinghJMLR 2021 (shorter version at ALT 2019) Code for the PeerReview4All paper-reviewer assignment algorithmDataset Your 2 is My 1, Your 3 is My 9: Handling Arbitrary Miscalibrations in RatingsJingyan Wang and Nihar B. ShahAAMAS 2019 My PhD student Jingyan Wang won the Best Student Paper Award at AAMAS 2019Nominated for the Best Paper Award Loss Functions, Axioms, and Peer ReviewRitesh Noothigattu, Nihar B. Shah and Ariel ProcacciaJournal of Artificial Intelligence Research (JAIR), 2021.Code COMSOC 2021 Best Poster runner up On Strategyproof Conference ReviewYichong Xu, Han Zhao, Xiaofei Shi and Nihar B. ShahIJCAI 2019Code and data Design and Analysis of the NIPS 2016 Review ProcessNihar B. Shah, Behzad Tabibian, Krikamol Muandet, Isabelle Guy",
  "content_length": 24480,
  "method": "requests",
  "crawl_time": "2025-12-01 14:05:53"
}