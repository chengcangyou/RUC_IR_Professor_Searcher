{
  "name": "Taiji Suzuki",
  "homepage": "http://ibis.t.u-tokyo.ac.jp/suzuki",
  "status": "success",
  "content": "Taiji Suzuki's Homepage (鈴木大慈) Professor Department of Mathematical Informatics Graduate School of Information Science and Technology The University of Tokyo Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan Room: Room No. 352, Faculty of Engineering Building 6 (map) Postal Address: Hongo 7-3-1, Bunkyo-ku, Tokyo 113-8656, JAPAN Phone: +81-3-5841-6921 E-mail: Topic MLSS 2024 \"Deep Learning Theory\" slide: slide. Online Asian Machine Learning School (OAMLS) 2021 \"Deep Learning Theory and Optimization\" slide: slide. MLSS 2015 \"Stochastic optimization\" slides: slide1, slide2, slide3 Research Interests I am interested in Machine Learning and Statistics, especially the following research topics. Statistical learning theory Deep learning Kernel method Nonparametric convergence analysis Optimization Stochastic optimization Optimization for deep learning Information geometry Prior selection Objective Bayes CV Born in Japan, Oct, 1981. Education: Bachelor of Engineering from Mathematical Engineering Course, Department of Mathematical Engineering and Infomation Physics, Faculty of Engineering, The University of Tokyo, 2004. Master of Information Science and Technology from Department of Mathematical Informatics, Graduate School of Information Science and Technology, The University of Tokyo, 2006. (supervisor: Professor Kazuyuki Aihara) Ph.D of Information Science and Technology from Department of Mathematical Informatics, Graduate School of Information Science and Technology, The University of Tokyo, 2009. (thesis advisor: Associate Professor Fumiyasu Komaki) Research Experience 4/2006-3/2009: JSPS Research Fellow (DC1) 4/2009-6/2013: Assistant Professor (or Research Associate, with no students) in University of Tokyo 7/2013-3/2017: Associate Professor in Tokyo Institute of Technology 4/2017-Now: Associate Professor in University of Tokyo 10/2014-3/2018: Sakigake (PRESTO), JST Publications and Presentations New： Naoki Nishikawa, Taiji Suzuki: State Space Models are Provably Comparable to Transformers in Dynamic Token Selection. The Thirteenth International Conference on Learning Representations (ICLR2025), 2025. Toshimitsu Uesaka, Taiji Suzuki, Yuhta Takida, Chieh-Hsin Lai, Naoki Murata, Yuki Mitsufuji: Weighted Point Cloud Embedding for Multimodal Contrastive Learning Toward Optimal Similarity Metric. The Thirteenth International Conference on Learning Representations (ICLR2025), 2025. Bingrui Li, Wei Huang, Andi Han, Zhanpeng Zhou, Taiji Suzuki, Jun Zhu, Jianfei Chen: On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent. The Thirteenth International Conference on Learning Representations (ICLR2025), 2025. Juno Kim, Taiji Suzuki: Transformers Provably Solve Parity Efficiently with Chain of Thought. The Thirteenth International Conference on Learning Representations (ICLR2025), 2025. Oral presentation (1.82% of all submissions; 213/11672). Juno Kim, Dimitri Meunier, Arthur Gretton, Taiji Suzuki, Zhu Li: Optimality and Adaptivity of Deep Neural Features for Instrumental Variable Regression. The Thirteenth International Conference on Learning Representations (ICLR2025), 2025. Kenji Fukumizu, Taiji Suzuki, Noboru Isobe, Kazusato Oko, Masanori Koyama: Flow matching achieves almost minimax optimal convergence. The Thirteenth International Conference on Learning Representations (ICLR2025), 2025. Ryotaro Kawata, Kazusato Oko, Atsushi Nitanda, Taiji Suzuki: Direct Distributional Optimization for Provable Alignment of Diffusion Models. The Thirteenth International Conference on Learning Representations (ICLR2025), 2025. Wei Huang, Yuan Cao, Haonan Wang, Xin Cao, Taiji Suzuki: Quantifying the Optimization and Generalization Advantages of Graph Neural Networks Over Multilayer Perceptrons. The 28th International Conference on Artificial Intelligence and Statistics (AISTATS2025), 2025. accepted. Tomoya Murata, Atsushi Nitanda, Taiji Suzuki: Clustered Invariant Risk Minimization. The 28th International Conference on Artificial Intelligence and Statistics (AISTATS2025), 2025. accepted. Yusuke Hayashi, Taiji Suzuki: Meta Cyclical Annealing Schedule: A Simple Approach to Avoiding Meta-Amortization Error. arXiv:2003.01889. Kosuke Haruki, Taiji Suzuki, Yohei Hamakawa, Takeshi Toda, Ryuji Sakai, Masahiro Ozawa, Mitsuhiro Kimura: Gradient Noise Convolution (GNC): Smoothing Loss Function for Distributed Large-Batch SGD. arXiv:1906.10822. Atsushi Nitanda and Taiji Suzuki: Refined Generalization Analysis of Gradient Descent for Over-parameterized Two-layer Neural Networks with Smooth Activations on Classification Problems. arXiv:1905.09870. Tomoya Murata and Taiji Suzuki: Accelerated Sparsified SGD with Error Feedback. arXiv:1905.12224. Atsushi Nitanda and Taiji Suzuki: Stochastic Particle Gradient Descent for Infinite Ensembles. arXiv:1712.05438. Ryota Tomioka and Taiji Suzuki: Spectral norm of random tensors. arXiv:1407.1870. International Conference papers (Refereed)： Rei Higuchi, Ryotaro Kawata, Naoki Nishikawa, Kazusato Oko, Shoichiro Yamaguchi, Sosuke Kobayashi, Seiya Tokui, Kohei Hayashi, Daisuke Okanohara, Taiji Suzuki: When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars. Second Conference on Language Modeling (COLM2025), 2025. Andi Han, Wei Huang, Zhanpeng Zhou, Gang Niu, Wuyang Chen, Junchi Yan, Akiko Takeda, Taiji Suzuki: On the Role of Label Noise in the Feature Learning Process. Forty-second International Conference on Machine Learning (ICML2025), 2025. Ryotaro Kawata, Kohsei Matsutani, Yuri Kinoshita, Naoki Nishikawa, Taiji Suzuki: Mixture of Experts Provably Detect and Learn the Latent Cluster Structure in Gradient-Based Learning Forty-second International Conference on Machine Learning (ICML2025), 2025. Juno Kim, Denny Wu, Jason D. Lee, Taiji Suzuki: Metastable Dynamics of Chain-of-Thought Reasoning: Provable Benefits of Search, RL and Distillation. Forty-second International Conference on Machine Learning (ICML2025), 2025. Rei Higuchi, Taiji Suzuki: Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models. Forty-second International Conference on Machine Learning (ICML2025), 2025. Rom Parnichkun, Neehal Tumma, Armin W Thomas, Alessandro Moro, Qi An, Taiji Suzuki, Atsushi Yamashita, Michael Poli, Stefano Massaroli: Quantifying Memory Utilization with Effective State-Size. Forty-second International Conference on Machine Learning (ICML2025), 2025. Atsushi Nitanda, Anzelle Lee, Damian Tan Xing Kai, Mizuki Sakaguchi, Taiji Suzuki: Propagation of Chaos for Mean-Field Langevin Dynamics and its Application to Model Ensemble. Forty-second International Conference on Machine Learning (ICML2025), 2025. Dake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Qingfu Zhang, Hau-San Wong, Taiji Suzuki: Provable In-Context Vector Arithmetic via Retrieving Task Concepts. Forty-second International Conference on Machine Learning (ICML2025), 2025. Naoki Nishikawa, Yujin Song, Kazusato Oko, Denny Wu, Taiji Suzuki: Nonlinear transformers can perform inference-time feature learning. Forty-second International Conference on Machine Learning (ICML2025), 2025. Naoki Nishikawa, Taiji Suzuki: State Space Models are Provably Comparable to Transformers in Dynamic Token Selection. The Thirteenth International Conference on Learning Representations (ICLR2025), 2025. Toshimitsu Uesaka, Taiji Suzuki, Yuhta Takida, Chieh-Hsin Lai, Naoki Murata, Yuki Mitsufuji: Weighted Point Cloud Embedding for Multimodal Contrastive Learning Toward Optimal Similarity Metric. The Thirteenth International Conference on Learning Representations (ICLR2025), 2025. Bingrui Li, Wei Huang, Andi Han, Zhanpeng Zhou, Taiji Suzuki, Jun Zhu, Jianfei Chen: On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent. The Thirteenth International Conference on Learning Representations (ICLR2025), 2025. Juno Kim, Taiji Suzuki: Transformers Provably Solve Parity Efficiently with Chain of Thought. The Thirteenth International Conference on Learning Representations (ICLR2025), 2025. Juno Kim, Dimitri Meunier, Arthur Gretton, Taiji Suzuki, Zhu Li: Optimality and Adaptivity of Deep Neural Features for Instrumental Variable Regression. The Thirteenth International Conference on Learning Representations (ICLR2025), 2025. Kenji Fukumizu, Taiji Suzuki, Noboru Isobe, Kazusato Oko, Masanori Koyama: Flow matching achieves almost minimax optimal convergence. The Thirteenth International Conference on Learning Representations (ICLR2025), 2025. Ryotaro Kawata, Kazusato Oko, Atsushi Nitanda, Taiji Suzuki: Direct Distributional Optimization for Provable Alignment of Diffusion Models. The Thirteenth International Conference on Learning Representations (ICLR2025), 2025. Wei Huang, Yuan Cao, Haonan Wang, Xin Cao, Taiji Suzuki: Quantifying the Optimization and Generalization Advantages of Graph Neural Networks Over Multilayer Perceptrons. The 28th International Conference on Artificial Intelligence and Statistics (AISTATS2025), 2025. accepted. Tomoya Murata, Atsushi Nitanda, Taiji Suzuki: Clustered Invariant Risk Minimization. The 28th International Conference on Artificial Intelligence and Statistics (AISTATS2025), 2025. accepted. Jiarui Jiang, Wei Huang, Miao Zhang, Taiji Suzuki, Liqiang Nie: Unveil Benign Overfitting for Transformer in Vision: Training Dynamics, Convergence, and Generalization. Advances in Neural Information Processing Systems 37 (NeurIPS 2024). pp. 135464--135625, 2024. Juno Kim, Tai Nakamaki, Taiji Suzuki: Transformers are Minimax Optimal Nonparametric In-Context Learners. Advances in Neural Information Processing Systems 37 (NeurIPS 2024). pp. 106667--106713, 2024. (This paper was also accepted by ICML 2024 Workshop on Theoretical Foundations of Foundation Models and won Best Paper Award). Kazusato Oko, Yujin Song, Taiji Suzuki, Denny Wu: Pretrained Transformer Efficiently Learns Low-Dimensional Target Functions ",
  "content_length": 56583,
  "method": "requests",
  "crawl_time": "2025-12-01 14:35:35"
}