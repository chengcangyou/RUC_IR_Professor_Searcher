{
  "name": "Ilias Diakonikolas",
  "homepage": "http://www.iliasdiakonikolas.org",
  "status": "success",
  "content": "Ilias Diakonikolas Homepage Ilias Diakonikolas Home Research Overview Bio Book/Surveys News Publications Activities Teaching Students Contact \\( \\newcommand{\\opt}{\\mathrm{opt}} \\newcommand{\\eps}{\\epsilon} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\vec}{\\mathbf} \\newcommand{\\wstar}{\\w^{\\ast}} \\newcommand{\\x}{\\vec x} \\newcommand{\\w}{\\vec w} \\newcommand{\\wt}{\\widetilde} \\newcommand{\\wh}{\\widehat} \\newcommand{\\poly}{\\mathrm{poly}} \\newcommand{\\polylog}{\\mathrm{polylog}} \\newcommand{\\var}{\\mathbf{Var}} \\newcommand{\\cov}{\\mathbf{Cov}} \\) I am the Sheldon B. Lubar professor in the CS department at UW Madison, where I am a member of the theory of computing group, machine learning@uw-madison, and the Institute for Foundations of Data Science. I am also affiliated with the department of Statistics, the Wisconsin Institute for Discovery, and the Data Science Institute. Prior to joining UW, I was Andrew and Erna Viterbi Early Career Chair in Computer Science at USC, and a faculty member at the University of Edinburgh. Prior to that, I spent two years at UC Berkeley as the Simons Postdoctoral Fellow in Theoretical Computer Science. I obtained my Ph.D. in Computer Science at Columbia University, advised by Mihalis Yannakakis. I did my undergraduate studies in Greece, at the National Technical University of Athens. Research: My research interests are in algorithms and machine learning. A major goal of my work is to understand the tradeoff between statistical efficiency, computational efficiency, and robustness for fundamental problems in statistics and machine learning. Areas of current focus include high-dimensional robust statistics, information-computation tradeoffs, foundations of deep learning, nonparametric estimation, distribution testing, and data-driven algorithm design. I also have strong interests in applied probability, algorithmic game theory, and their connections to machine learning. My work has been recognized with the ACM Grace Murray Hopper Award, a Sloan Fellowship, an NSF CAREER Award, a Marie Curie Fellowship, a Google Faculty Research Award, best paper awards at NeurIPS and COLT, and the IBM Research Pat Goldberg Best Paper Award. My research has been supported by the NSF, DARPA, ONR, EPSRC, WARF, Google, and the European Commission. Interested in working with me? Apply to our Ph.D. program. Robust Statistics Book New Book with Daniel Kane on Algorithmic High-Dimensional Robust Statistics published by Cambridge University Press. Expository Articles Robustness Meets Algorithms [abstract] [fulltext] I. Diakonikolas, G. Kamath, D. Kane, J. Li, A. Moitra, A. Stewart Communications of the ACM, Research Highlights, May 2021 In every corner of machine learning and statistics, there is a need for estimators that work not just in an idealized model, but even when their assumptions are violated. Unfortunately, in high dimensions, being provably robust and being efficiently computable are often at odds with each other. We give the first efficient algorithm for estimating the parameters of a high-dimensional Gaussian that is able to tolerate a constant fraction of corruptions that is independent of the dimension. Prior to our work, all known estimators either needed time exponential in the dimension to compute or could tolerate only an inverse-polynomial fraction of corruptions. Not only does our algorithm bridge the gap between robustness and algorithms, but also it turns out to be highly practical in a variety of settings. Recent Advances in Algorithmic High-Dimensional Robust Statistics [abstract] [arxiv] I. Diakonikolas and D. Kane A shorter version appears as an Invited Book Chapter in Beyond the Worst-Case Analysis of Algorithms, Cambridge University Press, September 2020 Learning in the presence of outliers is a fundamental problem in statistics. Until recently, all known efficient unsupervised learning algorithms were very sensitive to outliers in high dimensions. In particular, even for the task of robust mean estimation under natural distributional assumptions, no efficient algorithm was known. Recent work in theoretical computer science gave the first efficient robust estimators for a number of fundamental statistical tasks, including mean and covariance estimation. Since then, there has been a flurry of research activity on algorithmic high-dimensional robust estimation in a range of settings. In this survey article, we introduce the core ideas and algorithmic techniques in the emerging area of algorithmic high-dimensional robust statistics with a focus on robust mean estimation. We also provide an overview of the approaches that have led to computationally efficient robust estimators for a range of broader statistical tasks and discuss new directions and opportunities for future work. Research Vignette: Algorithmic High-Dimensional Robust Statistics I. Diakonikolas, S. Vempala, D. Woodruff UC Berkeley Simons Institute newsletter, September 2019 issue Learning Structured Distributions [abstract] [pdf] I. Diakonikolas Invited Book Chapter, in Handbook of Big Data, Chapman and Hall/CRC, February 2016 Estimating distributions from samples is a paradigmatic and fundamental unsupervised learning problem that has been studied in statistics since the late nineteenth century, starting with the pioneering work of Karl Pearson. During the past couple of decades, there has been a large body of work in computer science on this topic with a focus on {\\em computational efficiency.} The area of distribution estimation is well-motivated in its own right, and has seen a recent surge of research activity, in part due to the ubiquity of structured distributions in the natural and social sciences. Such structural properties of distributions are sometimes direct consequences of the underlying application problem, or they are a plausible explanation of the model under investigation. In this chapter, we give a survey of both classical and modern techniques for distribution estimation, with a focus on recent algorithmic ideas developed in theoretical computer science. These ideas have led to computationally and statistically efficient algorithms for learning broad families of models. For the sake of concreteness, we illustrate these ideas with specific examples. Finally, we highlight outstanding challenges and research directions for future work. Recent/Upcoming Activities Upcoming Research Summer School in Robust Statistics and Reliable Learning Algorithms at Warwick Statistics, co-organized by the London Mathematical Society. TTIC Summer 2025 workshop on Information-Computation Tradeoffs for Statistical Tasks. TTIC Summer 2024 workshop on New Frontiers in Robust Statistics. Tutorial on Robust Statistics at CIRM Meeting in Mathematical Statistics (December 2023). See here for the slides! Research Publications [chronologically][by topic] Agnostic Product Mixed State Tomography via Robust Statistics [abstract] [arxiv] A. Arulandu, I. Diakonikolas, D. Kane, J. Li Manuscript, 2025 We consider the problem of agnostic tomography with mixed state ansatz, and specifically, the natural ansatz class of product mixed states. In more detail, given $N$ copies of an $n$-qubit state $\\rho$ which is $\\eps$-close to a product mixed state $\\pi$, the goal is to output a nearly-optimal product mixed state approximation to $\\rho$. While there has been a flurry of recent work on agnostic tomography, prior work could only handle pure state ansatz, such as product states or stabilizer states. Here we give an algorithm for agnostic tomography of product mixed states which finds a product state which is $O(\\eps \\log 1 / \\eps)$ close to $\\rho$ which uses polynomially many copies of $\\rho$, and which runs in polynomial time. Moreover, our algorithm only uses single-qubit, single-copy measurements. To our knowledge, this is the first efficient algorithm that achieves any non-trivial agnostic tomography guarantee for any class of mixed state ansatz. Our algorithm proceeds in two main conceptual steps, which we believe are of independent interest. First, we demonstrate a novel, black-box efficient reduction from agnostic tomography of product mixed states to the classical task of robustly learning binary product distributionsâa textbook problem in robust statistics. Crucially, our reduction requires one step of adaptivity in the choice of measurement. We then demonstrate a nearly-optimal efficient algorithm for the classical task of robustly learning a binary product, answering an open problem in the literature. Our approach hinges on developing a new optimal certificate of closeness for binary product distributions that can be leveraged algorithmically via a carefully defined convex relaxation. Finally, we complement our upper bounds with a lower bound demonstrating that adaptivity is information-theoretically necessary for our agnostic tomography task, so long as the algorithm only uses single-qubit two-outcome projective measurements. Robust Regression of General ReLUs with Queries I. Diakonikolas, D. Kane, M. Ma Advances in Neural Information Processing Systems (NeurIPS 2025) Information-Computation Tradeoffs for Noiseless Linear Regression with Oblivious Contamination [abstract] [arxiv] I. Diakonikolas, C. Gao, D. Kane, J. Lafferty, A. Pensia Advances in Neural Information Processing Systems (NeurIPS 2025) We study the task of noiseless linear regression under Gaussian covariates in the presence of additive oblivious contamination. Specifically, we are given i.i.d. samples from a distribution $(x, y)$ on $\\R^d \\times \\R$ with $x \\sim N(0, I_d)$ and $y = x^\\top \\beta + z$, where $z$ is drawn independently of $x$ from an unknown distribution $E$. Moreover, $z$ satisfies $Pr_E[z = 0] = \\alpha>0$. The goal is to accurately recover the regressor $\\beta$ to small $\\ell_2$-error. Ignoring computational considerations, this problem is known to be solvable using $O(d/\\alpha)$ samples. On the other hand, the best known polynomial-time algorithms require $\\Omeg",
  "content_length": 289972,
  "method": "requests",
  "crawl_time": "2025-12-01 13:22:52"
}