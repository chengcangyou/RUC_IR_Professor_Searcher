{
  "name": "Fei Sun 0001",
  "homepage": "https://ofey.me",
  "status": "success",
  "content": "Fei Sun An Asian man wearing sunglasses and a red T-shirt with the infinity symbol, standing on top of Shurijo Castle, Okinawa -- captioned by ğŸ¤– Iâ€™m an associate professor at Institute of Computing Technology, CAS. I lead the STAR Group, a small yet passionate team dedicated to advancing research in AI Safety. Iâ€™m broadly interested in all aspects of machine learning, with a particular focus on topics related to Safety & Trustworthy, including: Knowledge mechanisms in LLMs: how they learn, memorize, recall, update/edit, and forget knowledgeâ€”basically reverse-engineering the brain of a neural network. Safety challenges in AI applications, especially in LLMs and RecSys. ğŸ“Š Simple Stats About My Academic Life ğŸ¤“: ğŸ“ 60 peer-reviewed papers in top conferences and journals. ğŸ† Stanford/Elsevierâ€™s Top 2% Scientists (2023, 2024, 2025). ğŸ“ˆ ~1/11 papers in GScholarâ€™s top 100 (5Y); 4 in top 20 [e.g., 1, 2, 3]. ğŸ– One paper ranks in the top 3 most-cited papers in CIKMâ€™s history. ğŸ§ Served as PC/SPC 50+ times for top-tier conferences. Join Us! ğŸ‘©â€ğŸ’»ğŸ‘¨â€ğŸ’» Iâ€™m always looking for strong and motivated students to join us! Just drop me an email with your cv. news Nov 09, 2025 We will hold The 2st Workshop on Human-Centered Recommender Systems on WWW 26. Contributions are welcome ï¼ Oct 31, 2025 One paper is accepted by AAAI 2026 Demo about algorithm auditing. Congrats to Zhenxing! Aug 21, 2025 Three papers are accepted by EMNLP 2025 about Hallucination/Uncertainty Estimation, Backdoor, and Jailbreaking. Aug 01, 2025 Congratulations to Wanli on receiving the Best Paper Award at the KnowFM @ ACL25 Workshopâ€”a well-deserved recognition of his excellent research! May 15, 2025 Three papers are accepted by ACL2025 about model editing, LLM inductive reasoning, and watermarking. View All â†’ selected publications The Mirage of Model Editing: Revisiting Evaluation in the Wild Wanli Yang ,Â Fei Sun ,Â Jiajun Tan ,Â Xinyu Ma ,Â Qi Cao ,Â Dawei Yin ,Â Huawei Shen ,Â andÂ Xueqi Cheng In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Jul 2025 Abs Bib Review PDF Web GitHub Dataset Leaderboard Despite near-perfect results reported in the literature, the effectiveness of model editing in real-world applications remains unclear. To bridge this gap, we introduce QAEdit, a new benchmark aligned with widely used question answering (QA) datasets, and WILD, a task-agnostic evaluation framework designed to better reflect real-world usage of model editing. Our single editing experiments show that current editing methods perform substantially worse than previously reported (38.5% vs. 96.8%). We demonstrate that it stems from issues in the synthetic evaluation practices of prior work. Among them, the most severe is the use of teacher forcing during testing, which leaks both content and length of the ground truth, leading to overestimated performance. Furthermore, we simulate practical deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits. This work calls for a shift in model editing research toward rigorous evaluation and the development of robust, scalable methods that can reliably update knowledge in LLMs for real-world use. @inproceedings{yang-etal-2025-mirage, title = {The Mirage of Model Editing: Revisiting Evaluation in the Wild}, author = {Yang, Wanli and Sun, Fei and Tan, Jiajun and Ma, Xinyu and Cao, Qi and Yin, Dawei and Shen, Huawei and Cheng, Xueqi}, booktitle = {Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, month = jul, year = {2025}, address = {Vienna, Austria}, publisher = {Association for Computational Linguistics}, url = {https://aclanthology.org/2025.acl-long.745/}, pages = {15336--15354}, isbn = {979-8-89176-251-0} } ACL KnowFM@ACL 25 Best Paper The Fall of ROME: Understanding the Collapse of LLMs in Model Editing Wanli Yang ,Â Fei Sun ,Â Jiajun Tan ,Â Xinyu Ma ,Â Du Su ,Â Dawei Yin ,Â andÂ Huawei Shen In Findings of the Association for Computational Linguistics: EMNLP 2024, Nov 2024 Abs Bib Review PDF Web GitHub Despite significant progress in model editing methods, their application in real-world scenarios remains challenging as they often cause large language models (LLMs) to collapse. Among them, ROME is particularly concerning, as it could disrupt LLMs with only a single edit. In this paper, we study the root causes of such collapse. Through extensive analysis, we identify two primary factors that contribute to the collapse: i) inconsistent handling of prefixed and unprefixed keys in the parameter update equation may result in very small denominators, causing excessively large parameter updates; ii) the subject of collapse cases is usually the first token, whose unprefixed key distribution significantly differs from the prefixed key distribution in autoregressive transformers, causing the aforementioned issue to materialize. To validate our findings, we propose a simple yet effective approach: uniformly using prefixed keys during editing phase and adding prefixes during testing phase to ensure the consistency between training and testing. The experimental results show that the proposed solution can prevent model collapse while maintaining the effectiveness of the edits. @inproceedings{yang-etal-2024-fall, title = {The Fall of {ROME}: Understanding the Collapse of {LLM}s in Model Editing}, author = {Yang, Wanli and Sun, Fei and Tan, Jiajun and Ma, Xinyu and Su, Du and Yin, Dawei and Shen, Huawei}, editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung}, booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024}, month = nov, year = {2024}, address = {Miami, Florida, USA}, publisher = {Association for Computational Linguistics}, url = {https://aclanthology.org/2024.findings-emnlp.236/}, doi = {10.18653/v1/2024.findings-emnlp.236}, pages = {4079--4087} } EMNLP (Findings) The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse Wanli Yang ,Â Fei Sun ,Â Xinyu Ma ,Â Xun Liu ,Â Dawei Yin ,Â andÂ Xueqi Cheng In Findings of the Association for Computational Linguistics: ACL 2024, Aug 2024 Abs Bib Review HTML PDF GitHub Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks. However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating changes in an edited modelâ€˜s perplexity are strongly correlated with its downstream task performances. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse after only few edits. To facilitate further research, we have utilized GPT-3.5 to develop a new dataset, HardEdit, based on those hard cases. This dataset aims to establish the foundation for pioneering research in reliable model editing and the mechanisms underlying editing-induced model collapse. We hope this work can draw the communityâ€˜s attention to the potential risks inherent in model editing practices. @inproceedings{yang-etal-2024-butterfly, title = {The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse}, author = {Yang, Wanli and Sun, Fei and Ma, Xinyu and Liu, Xun and Yin, Dawei and Cheng, Xueqi}, editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek}, booktitle = {Findings of the Association for Computational Linguistics: ACL 2024}, month = aug, year = {2024}, address = {Bangkok, Thailand}, publisher = {Association for Computational Linguistics}, url = {https://aclanthology.org/2024.findings-acl.322/}, doi = {10.18653/v1/2024.findings-acl.322}, pages = {5419--5437} } ACL (Findings) Graph Neural Networks in Recommender Systems: A Survey Shiwen Wu ,Â Fei Sun ,Â Wentao Zhang ,Â Xu Xie ,Â andÂ Bin Cui ACM Comput. Surv., Dec 2022 Abs Bib PDF With the explosive growth of online information, recommender systems play a key role to alleviate such information overload. Due to the important application value of recommender systems, there have always been emerging works in this field. In recommender systems, the main challenge is to learn the effective user/item representations from their interactions and side information (if any). Recently, graph neural network (GNN) techniques have been widely utilized in recommender systems since most of the information in recommender systems essentially has graph structure and GNN has superiority in graph representation learning. This article aims to provide a comprehensive review of recent research efforts on GNN-based recommender systems. Specifically, we provide a taxonomy of GNN-based recommendation models according to the types of information used and recommendation tasks. Moreover, we systematically analyze the challenges of applying GNN on different types of data and discuss how existing works in this field address these challenges. Furthermore, we state new perspectives pertaining to the development of this field. We collect the representative papers along with their open-source implementations in . @article{Wu-2022-GNN, author = {Wu, Shiwen and Sun, Fei and Zhang, Wentao and Xie, Xu and Cui, Bin}, title = {Graph Neural Networks in Recommender Systems: A Survey}, year = {2022}, issue_date = {May 2023}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {55}, number = {5}, issn = {0360-0300}, url = {https:/",
  "content_length": 13632,
  "method": "requests",
  "crawl_time": "2025-12-01 13:09:32"
}