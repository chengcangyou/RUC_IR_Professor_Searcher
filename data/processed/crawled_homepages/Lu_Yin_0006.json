{
  "name": "Lu Yin 0006",
  "homepage": "https://www.surrey.ac.uk/people/lu-yin",
  "status": "success",
  "content": "Dr Lu Yin | University of Surrey This website uses cookiesSome of these cookies are necessary and are used to help make our site work. With your consent, we will also use cookies to improve your experience, analyse site usage and assist in our marketing efforts. By clicking 'Accept all', you consent to our use of cookies.You can learn more about these cookies and manage your preferences at any time on our cookies page .Necessary Necessary These cookies enable essential website functions and ensure the site operates properly. They do not store personally identifiable information. While you can disable them in your browser settings, some features may not function correctly.Analytics Analytics These cookies help us understand how visitors interact with our website by collecting anonymous usage data. We use this information to improve website performance and enhance your experience.Personalisation Personalisation These cookies allow us to customise the content you see based on your activity, such as the courses you view, applications you make, or your location. This helps us provide a more relevant and tailored experience when using our site.Marketing Marketing These cookies deliver personalised ads based on your browsing activity and help us measure the effectiveness of our advertising campaigns. Accept all Manage preferences Save preferences Reject all Google Scholar Dr Lu Yin Assistant Professor in AI l.yin@surrey.ac.uk https://luuyin.com/15 BB 02Academic and research departments Computer Science Research Centre, Nature Inspired Computing and Engineering Research Group. AboutBiographyGreetings! I’m Lu, an Assistant Professor in the School of Computer Science and Electronic Engineering at the University of Surrey, a long-term visitor at the Visual Informatics Group (VITA) at UT Austin, and a long-term visiting researcher at Eindhoven University of Technology (TU/e). Previously, I served as a Postdoctoral Fellow at TU/e and gained experience as a research scientist and intern at Google’s New York City office. I work closely with leading industry entities, including Meta London, Google NYC, Intel Research, and JingDong. My research interests include:#AI Efficiency#AI for Science#Large Foundation/Language ModelsFeel free to reach out if you’d like to discuss anything with me :) I am open to collaboration with students and visitors, including those who may be remote, who are interested in AI Efficiency, Large Foundation Models understanding,  AI for Science, etc.I welcome PhD applications from candidates with strong self-motivation. Expand biography PublicationsPengxiang Li, Lu Yin, John Collomose, Shiwei Liu Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LNDOI: 10.48550/arxiv.2412.13795Large Language Models (LLMs) have achieved remarkable success, yet recent findings reveal that their deeper layers often contribute minimally and can be pruned without affecting overall performance. While some view this as an opportunity for model compression, we identify it as a training shortfall rooted in the widespread use of Pre-Layer Normalization (Pre-LN). We demonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads to diminished gradient norms in its deeper layers, reducing their effectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger gradient norms in deeper layers but suffers from vanishing gradients in earlier layers. To address this, we introduce Mix-LN, a novel normalization technique that combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN applies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring more uniform gradients across layers. This allows all parts of the network--both shallow and deep layers--to contribute effectively to training. Extensive experiments with various model sizes from 70M to 7B demonstrate that Mix-LN consistently outperforms both Pre-LN and Post-LN, promoting more balanced, healthier gradient norms throughout the network, and enhancing the overall quality of LLM pre-training. Furthermore, we demonstrate that models pre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN during supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), highlighting the critical importance of high-quality deep layers. By effectively addressing the inefficiencies of deep layers in current LLMs, Mix-LN unlocks their potential, enhancing model capacity without increasing model size. Our code is available at https://github.com/pixeli99/MixLN.Adarsh Kappiyath, Abhra Chaudhuri, Ajay Jaiswal, Ziquan Liu, Yunpeng Li, Xiatian Zhu, Lu Yin (2025)SEBRA : Debiasing through Self-Guided Bias Ranking, In: The Thirteenth International Conference on Learning RepresentationsRanking samples by fine-grained estimates of spuriosity (the degree to which spurious cues are present) has recently been shown to significantly benefit bias mitigation, over the traditional binary biased-vs-unbiased partitioning of train sets. However, this spuriousity ranking comes with the requirement of human supervision. In this paper, we propose a debiasing framework based on our novel Self-Guided Bias Ranking (Sebra), that mitigates biases via an automatic ranking of data points by spuriosity within their respective classes. Sebra leverages a key local symmetry in Empirical Risk Minimization (ERM) training -- the ease of learning a sample via ERM inversely correlates with its spuriousity; the fewer spurious correlations a sample exhibits, the harder it is to learn, and vice versa. However, globally across iterations, ERM tends to deviate from this symmetry. Sebra dynamically steers ERM to correct this deviation, facilitating the sequential learning of attributes in increasing order of difficulty, ie, decreasing order of spuriosity. As a result, the sequence in which Sebra learns samples naturally provides spuriousity rankings. We use the resulting fine-grained bias characterization in a contrastive learning framework to mitigate biases from multiple sources. Extensive experiments show that Sebra consistently outperforms previous state-of-the-art unsupervised debiasing techniques across multiple standard benchmarks, including UrbanCars, BAR, and CelebA.Lu Yin, You Wu, Zhenyu Zhang, Cheng-yu Hsieh, Yaqing Wang, Yijing Jia, Gen Li, Ajay Jaiswal, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, Shiwei Liu (2024)Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity, In: The Forty-First International Conference on Machine Learning Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of non-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, OWL exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70%, respectively, while delivering 2.6x end-to-end inference speed-up in the DeepSparse inference engine.Pengxiang Li, Lu Yin, Shiwei Liu (2025)The Thirteenth International Conference on Learning Representations, In: The Thirteenth International Conference on Learning RepresentationsLarge Language Models (LLMs) have achieved remarkable success, yet recent findings reveal that their deeper layers often contribute minimally and can be pruned without affecting overall performance. While some view this as an opportunity for model compression, we identify it as a training shortfall rooted in the widespread use of Pre-Layer Normalization (Pre-LN). We demonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads to diminished gradient norms in its deeper layers, reducing their effectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger gradient norms in deeper layers but suffers from vanishing gradients in earlier layers. To address this, we introduce Mix-LN, a novel normalization technique that combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN applies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring more uniform gradients across layers. This allows all parts of the network—both shallow and deep layers—to contribute effectively to training. Extensive experiments with various model sizes from 70M to 7B demonstrate that Mix-LN consistently outperforms both Pre-LN and Post-LN, promoting more balanced, healthier gradient norms throughout the network, and enhancing the overall quality of LLM pre-training. Furthermore, we demonstrate that models pre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN ",
  "content_length": 10380,
  "method": "requests",
  "crawl_time": "2025-12-01 13:48:11"
}