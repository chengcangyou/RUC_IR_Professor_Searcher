{
  "name": "Kristin J. Dana",
  "homepage": "https://www.ece.rutgers.edu/~kdana",
  "status": "success",
  "content": "Prof. Kristin Dana — Rutgers ECE Vision & Robotics Lab Kristin Dana Professor Rutgers University ECE Department (Electrical and Computer Engineering) Rutgers Computer Science Dept: Member of Graduate Faculty ECE Vision and Robotics Lab, Director, SOCRATES 848-445-5253 kristin.dana at rutgers dot edu Welcome The ECE Vision and Robotics Laboratory, conducts innovative research at the intersection of computer vision and robotics, key branches of AI. The lab is developing new methods for computer vision, human-robotic interaction, precision agriculture, remote sensing, and computational photography. Select Research Projects AI and Vision in Agriculture Agtech Framework for Cranberry-Ripening Analysis Using Vision Foundation Models Johnson, Faith, Ryan Meegan, Jack Lowry, Peter Oudemans, and Kristin Dana. 2025. “Agtech Framework for Cranberry-Ripening Analysis Using Vision Foundation Models.” Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 1207–1216. This work presents a computer vision framework for analyzing the ripening process of cranberry crops using both aerial drone and ground-based imaging across a full growing season. By leveraging vision transformers (ViT) and UMAP dimensionality reduction, the framework enables interpretable visualizations of berry appearance and quantifies ripening trajectories. The approach supports precision agriculture tasks such as high-throughput phenotyping and crop variety comparison. This is the first visual framework for cranberry ripening assessment, with potential impact across other crops like wine grapes and olives. [BibTeX] | [Project Page] Vision on the bog: Cranberry crop risk evaluation with deep learning Akiva, Peri, Benjamin Planche, Aditi Roy, Peter Oudemans, and Kristin Dana. \"Vision on the bog: Cranberry crop risk evaluation with deep learning.\" Computers and Electronics in Agriculture 203 (2022) Vision-on-the-bog is framework for smart agriculture that enables real-time decision-making by monitoring cranberry crops. It performs instance segmentation to count sun-exposed cranberries at risk of overheating and predicts internal berry temperature using drone and sky imaging. A weakly supervised segmentation method reduces annotation effort, while a differentiable model jointly estimates solar irradiance and berry temperature. These tools support short-term risk assessment to inform irrigation decisions. The approach is validated over two growing seasons and can be extended to crops such as grapes, olives, and grain. [BibTeX] | [Project Page] Visual Navigation A Landmark-Aware Visual Navigation Dataset Akiva, Peri, Benjamin Planche, Aditi Roy, Peter Oudemans, and Kristin Dana. \"Vision on the bog: Cranberry crop risk evaluation with deep learning.\" Computers and Electronics in Agriculture 203 (2022) This work introduces the Landmark-Aware Visual Navigation (LAVN) dataset to support supervised learning of human-centric exploration and map-building policies. The dataset includes RGB-D observations, human point-clicks for navigation waypoints, and annotated visual landmarks from both virtual and real-world environments. These annotations enable direct supervision for learning efficient exploration strategies and landmark-based mapping. LAVN spans diverse scenes and is publicly released with comprehensive documentation to facilitate research in visual navigation. [BibTeX] | [Project Page] Feudal Networks for Visual Navigation Johnson, Faith, Bryan Bo Cao, Ashwin Ashok, Shubham Jain, and Kristin Dana. \"Feudal networks for visual navigation.\", presented at Embodied AI Workshop EAI-CVPR2024, arXiv preprint arXiv:2402.12498 (2024) This work proposes a novel feudal learning approach to visual navigation that eliminates the need for reinforcement learning, metric maps, graphs, or odometry. The hierarchical architecture includes a high-level manager with a self-supervised memory proxy map and a mid-level manager with a waypoint network trained to mimic human navigation behaviors. Each level of the agent hierarchy operates at different spatial and temporal scales, enabling efficient, human-like exploration. The system is trained using a small set of teleoperation videos and achieves near state-of-the-art performance on image-goal navigation tasks in previously unseen environments. [BibTeX] | [Project Page] Learning a Pedestrian Social Behavior Dictionary Johnson, Faith, and K. Dana. \"Learning a Pedestrian Social Behavior Dictionary.\" British Machine Vision Conference, 2023. This work presents an unsupervised framework for learning a dictionary of pedestrian behaviors from trajectory data, enabling semantic interpretation without the need for labeled examples. A trajectory latent space is learned and clustered to form a taxonomy of behaviors specific to the environment. This dictionary is then used to generate behavior maps that visualize how pedestrians use space and to compute distributions over different behavior types. By conditioning on behavior labels, the method enables a simple yet effective approach to trajectory prediction. The lightweight, low-parameter model achieves results comparable to state-of-the-art methods on the ETH and UCY benchmark datasets. [BibTeX] | [Project Page]| [Paper] Photographic Steganography and Watermarking Light Field Messaging With Deep Photographic Steganography Wengrowski, Eric, and Kristin Dana. \"Light field messaging with deep photographic steganography.\" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1515-1524. 2019. We develop Light Field Messaging (LFM), a process of embedding, transmitting, and receiving hidden information in video that is displayed on a screen and captured by a handheld camera. The goal of the system is to minimize perceived visual artifacts of the message embedding, while simultaneously maximizing the accuracy of message recovery on the camera side. LFM requires photographic steganography for embedding messages that can be displayed and camera-captured. Unlike digital steganography, the embedding requirements are significantly more challenging due to the combined effect of the screen's radiometric emittance function, the camera's sensitivity function, and the camera-display relative geometry. We devise and train a network to jointly learn a deep embedding and recovery algorithm that requires no multi-frame synchronization. A key novel component is the camera display transfer function (CDTF) to model the camera-display pipeline. To learn this CDTF we introduce a dataset (Camera-Display 1M) of 1,000,000 camera-captured images collected from 25 camera-display pairs. The result of this work is a high-performance real-time LFM system using consumer-grade displays and smartphone cameras. [BibTeX] | [Project Page] Vision+Tactile Teaching Cameras to Feel: Estimating Tactile Physical Properties of Surfaces from Images Purri, Matthew, and Kristin Dana. \"Teaching Cameras to Feel: Estimating Tactile Physical Properties of Surfaces from Images.\" *European Conference on Computer Vision (ECCV)*, 2020. This work introduces a novel task of estimating 15 tactile physical properties—such as friction, compliance, adhesion, and thermal conductance—from visual images using a multi-view dataset of over 400 surfaces. The authors propose a cross-modal framework combining adversarial objectives and a joint visuo-tactile classification loss, along with neural architecture search to optimally select viewpoints. Results demonstrate effective prediction of material properties purely from images. The dataset formed (Surface Property Synesthesia Dataset) is among the largest to tackle vision-to-touch prediction and is publicly released. [BibTeX] | [Project Page] Segmentation and Object Detection Self‑Supervised Object Detection from Egocentric Videos Akiva, Peri, Jing Huang, Kevin J. Liang, Rama Kovvuri, Xingyu Chen, Matt Feiszli, Kristin Dana, and Tal Hassner. 2023. “Self‑Supervised Object Detection from Egocentric Videos.” *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, 5225–5237. This paper presents DEVI, a self-supervised, class-agnostic object detector trained on egocentric video without annotations or pretraining. It uses multi-view and scale-regression losses—motivated by appearance-based cues—to learn dense, category-specific features. The learned cluster residual module enables robust object localization in complex environments. DEVI achieves significant gains in detection metrics on the Ego4D and EgoObjects datasets while using a lightweight, end-to-end architecture. [BibTeX] | [Project Page] Single Stage Weakly Supervised Semantic Segmentation of Complex Scenes Akiva, P. & Dana, K. (2023). Single Stage Weakly Supervised Semantic Segmentation of Complex Scenes*. In 2023 IEEE Winter Conference on Applications of Computer Vision (WACV), 5943–5954. This paper presents a single-stage method for weakly‑supervised semantic segmentation using point annotations to generate reliable pseudo‑masks on the fly, eliminating the need for pre‑training, multi‑stage pipelines, or refined CAMs. It achieves state‑of‑the‑art performance across challenging real‑world benchmarks such as CRAID, CityPersons, ADE20K and CityScapes, showing advantages in both accuracy and generalizability. The method is trained from scratch, handles complex scenes, and outperforms multi‑stage methods that rely on pre‑trained networks. Its innovation lies in combining expanding distance fields with pixel‑adaptive convolutions to refine masks at each training step. [BibTeX] [Project Page] Remote Sensing Urban Semantic 3D Reconstruction from Multiview Satellite Imagery Leotta, M. J., Long, C., Jacquet, B., Zins, M., Lipsa, D., Shan, J., Xu, B., Li, Z., Zhang, X., Chang, S.‑F., Purri, M., Xue, J., & Dana, K. (2019). Urban Semantic 3D Reconstruction from Multiview Satellite Imagery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
  "content_length": 22064,
  "method": "requests",
  "crawl_time": "2025-12-01 13:44:29"
}