{
  "name": "Aravindan Vijayaraghavan",
  "homepage": "http://www.eecs.northwestern.edu/~aravindv",
  "status": "success",
  "content": "Aravindan Vijayaraghavan Home Contact Aravindan Vijayaraghavan I am an Associate Professor in the CS department at Northwestern University, and by courtesy, the IEMS department. I'm a member of the Theory CS Group and my research interests are broadly in theoretical computer science. I work on the algorithmic foundations of machine learning, data science, combinatorial optimization, and more recently, quantum information. I am particularly interested in using paradigms that go Beyond Worst-Case Analysis to obtain good algorithmic guarantees. I also serve as a Site Director (Northwestern) of the Institute for Data, Economics, Algorithms and Learning (IDEAL), and served as the Institute Director in 2023-24. IDEAL is an NSF-funded collaborative institute across Northwestern, TTI Chicago, UIC, U of Chicago, and IIT. My research has also been supported by an NSF CAREER award, an NSF AITF award CCF-1637585 (with David Sontag), CCF-2154100 (with Julia Gaudio), the Google Research Scholar program and the Amazon Research Awards program. Prior to joining Northwestern in Fall 2015, I was at Courant, NYU for a year as a part of the Simons Collaboration on Algorithms and Geometry , and was a Simons Postdoctoral Research Fellow with the Theory Group at Carnegie Mellon University. I obtained my PhD from Princeton University in Computer Science with Prof. Moses Charikar, after getting my bachelor's degree in CS from the Indian Institute of Technology Madras. I spent the first fifteen years of my life in Pondicherry, a beautiful town in Southern India, where Pi Patel hails from. Teaching CS262: Mathematical Foundations of CS Part 2- Continuous Mathematics for Computer Science. Spring 2024. CS212: Mathematical Foundations of Computer Science. Fall 2015, 2016, 2017, Spring 2019, Fall 2019, Winter 2021, Fall 2022. CS496: Graduate Algorithms. Winter 2016, 2017, Spring 2018, Winter 2019, Winter 2022. CS 396/496: Foundations of Quantum Computation & Quantum Information Winter 2022 (co-taught with S. Rao), Winter 2024, Fall 2024 CS 497: Recent Highlights in Machine Learning Theory Spring 2025. CS 335: Intro to the Theory of Computation (co-taught with Jason Hartline) Fall 2020, Spring 2022. CS 496: Foundations of Reliable Machine Learning Fall 2021, Fall 2023. CS 497: Recent Highlights in Theoretical CS Winter 2022. CS496: Theoretical Foundations of Data Science Spring 2021. CS496/ ECE495: Algorithmic Aspects of Network Inference Spring 2020 (co-taught with R. Berry). CS496: Topics in Theoretical Machine Learning. Winter 2018. CS 496 : Beyond Worst-Case Analysis. Spring 2017. Students and Postdocs Mentored Current PhD students: Sanchit Kalhan (co-advised with Konstantin Makarychev), Vaidehi Srinivas , Anxin (Bob) Guo , Dionysios Arvanitakis (co-advised with Konstantin Makarychev), Tanmay Sinha. Current Postdocs (co-mentored): He Jia (IDEAL Postdoctoral Fellow), Dravyansh Sharma (IDEAL Postdoctoral Fellow). Former PhD students: Abhratanu Dutta (defended in 2020; first job: Facebook research), Aidao Chen (defended in 2023; first job: Accutar Biotech. Co-advised with Anindya De) Aravind Reddy (defended in 2023; Broad institute. Co-advised with Konstantin Makarychev), Alex Tang (defended in 2024), Former Postdocs co-mentored: Xue Chen (first job: Assistant Professor, George Mason University, CS), Shravas Rao (first job: Assistant Professor, Portland State University, CS), Jinshuo Dong (first job: Assistant Professor, Peking University, Yau Institute of Mathematical Sciences), Eric Evert (first job: University of Florida, Math). Prospective Students: See here for details. Professional Activities and Theory Events FOCS 2024: General Chair. Junior Theory Workshop series and Northwestern QTW series: Co-organizer. Program Committees/ Senior Area Chairs/ Area Chairs (or equivalent): COLT 2025, ICML 2025, COLT 2024, ALT 2024, ICML 2024, FOCS 2023, ICML 2023, COLT 2023, ALT 2023, NeurIPS 2022, COLT 2022, ALT 2022, STOC 2021, COLT 2021, WADS 2021, COLT 2020, ESA 2019, COLT 2019, APPROX 2018, SODA 2015 IDEAL workshops and events: See IDEAL for details about IDEAL events and seminars. Workshop on Predictions and Uncertainty at COLT 2025: Co-organizing with Vaidehi Srinivas and Jessica Hullman. See Vaidehi's webpage for more details. Book Chapters or Surveys Efficient Tensor Decomposition. [ArXiv| Book Link | Abstract] This chapter studies the problem of decomposing a tensor into a sum of constituent rank one tensors. While tensor decompositions are very useful in designing learning algorithms and data analysis, they are NP-hard in the worst-case. We will see how to design efficient algorithms with provable guarantees under mild assumptions, and using beyond worst-case frameworks like smoothed analysis. Book Chapter in Beyond the Worst Case Analysis of Algorithms Edited by Tim Roughgarden, Cambridge University Press 2020. Publications Computing High-Dimensional Confidence Sets for Arbitrary Distributions, COLT 2025 (With Chao Gao, Liren Shan, Vaidehi Srinivas).[ArXiv] Agnostic Learning of Arbitrary ReLU Activation under Gaussian Marginals, COLT 2025 (With Bob Guo). [ArXiv] Volume Optimality in Conformal Prediction with Structured Prediction Sets, ICML 2025 (With Chao Gao, Liren Shan, Vaidehi Srinivas).[ArXiv] Theoretical Analysis of Weak-to-Strong Generalization, NeurIPS 2024 (With Hunter Lang, David Sontag). [ArXiv| Abstract] Strong student models can learn from weaker teachers: when trained on the predictions of a weaker model, a strong pretrained student can learn to correct the weak model's errors and generalize to examples where the teacher is not confident, even when these examples are excluded from training. This enables learning from cheap, incomplete, and possibly incorrect label information, such as coarse logical rules or the generations of a language model. We show that existing weak supervision theory fails to account for both of these effects, which we call pseudolabel correction and coverage expansion, respectively. We give a new bound based on expansion properties of the data distribution and student hypothesis class that directly accounts for pseudolabel correction and coverage expansion. Our bounds capture the intuition that weak-to-strong generalization occurs when the strong model is unable to fit the mistakes of the weak teacher without incurring additional error. We show that these expansion properties can be checked from finite data and give empirical evidence that they hold in practice. Efficient Certificates of Anti-Concentration Beyond Gaussians, FOCS 2024 (With Ainesh Bakshi, Pravesh Kothari, Goutham Rajendran and Madhur Tulsiani). [ArXiv| Abstract] A set of high dimensional points X={x1,x2,…,xn}⊂Rd in isotropic position is said to be δ-anti concentrated if for every direction v, the fraction of points in X satisfying |⟨xi,v⟩|≤δ is at most O(δ). Motivated by applications to list-decodable learning and clustering, recent works have considered the problem of constructing efficient certificates of anti-concentration in the average case, when the set of points X corresponds to samples from a Gaussian distribution. Their certificates played a crucial role in several subsequent works in algorithmic robust statistics on list-decodable learning and settling the robust learnability of arbitrary Gaussian mixtures, yet remain limited to rotationally invariant distributions. This work presents a new (and arguably the most natural) formulation for anti-concentration. Using this formulation, we give quasi-polynomial time verifiable sum-of-squares certificates of anti-concentration that hold for a wide class of non-Gaussian distributions including anti-concentrated bounded product distributions and uniform distributions over Lp balls (and their affine transformations). Consequently, our method upgrades and extends results in algorithmic robust statistics e.g., list-decodable learning and clustering, to such distributions. Our approach constructs a canonical integer program for anti-concentration and analysis a sum-of-squares relaxation of it, independent of the intended application. We rely on duality and analyze a pseudo-expectation on large subsets of the input points that take a small value in some direction. Our analysis uses the method of polynomial reweightings to reduce the problem to analyzing only analytically dense or sparse directions. New tools for smoothed analysis: least singular values for random matrices with dependent entries, STOC 2024 (With Aditya Bhaskara, Eric Evert and Vaidehi Srinivas). [ArXiv| Abstract] We develop new techniques for proving lower bounds on the least singular value of random matrices with limited randomness. The matrices we consider have entries that are given by polynomials of a few underlying base random variables. This setting captures a core technical challenge for obtaining smoothed analysis guarantees in many algorithmic settings. Least singular value bounds often involve showing strong anti-concentration inequalities that are intricate and much less understood compared to concentration (or large deviation) bounds. First, we introduce a general technique for proving anti-concentration that uses well-conditionedness properties of the Jacobian of a polynomial map, and show how to combine this with a hierarchical $\\epsilon$-net argument to prove least singular value bounds. Our second tool is a new statement about least singular values to reason about higher-order lifts of smoothed matrices and the action of linear operators on them. Apart from getting simpler proofs of existing smoothed analysis results, we use these tools to now handle more general families of random matrices. This allows us to produce smoothed analysis guarantees in several previously open settings. These new settings include smoothed analysis guarantees for power sum decompositions and certifying robust entanglement of subspaces, where prior work could only establish least singular value bounds for fully random instances or",
  "content_length": 64694,
  "method": "requests",
  "crawl_time": "2025-12-01 13:00:44"
}