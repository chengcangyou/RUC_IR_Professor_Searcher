{
  "name": "Richard Lenz",
  "homepage": "https://www.cs6.tf.fau.de/lehrstuhl/personen/richard-lenz",
  "status": "success",
  "content": "Richard Lenz - Lehrstuhl für Informatik 6 Organisationsmenü öffnen Organisationsmenü schließen Friedrich-Alexander-Universität Lehrstuhl für Informatik 6 CS6 Lehrstuhl für Informatik 6Datenmanagement Adresse Martensstraße 391058 Erlangen Zimmer: 08.136, Geschoss: 08 Kontakt E-Mail: richard.lenz@fau.de Telefon: +49 9131 85 – 27899 Sprechstunden Nach Vereinbarung nach Anmeldung in StudOn Prof. Dr. Richard Lenz ist seit 2007 Inhaber der Professur für evolutionäres Datenmanagement und Professor am Lehrstuhl für Informatik 6 (Datenmanagement). Zwischen Mai 2020 und März 2021 hat er den Lehrstuhl kommissarisch geleitet. Im Oktober 2022 hat er erneut die kommissarische Leitung des Lehrstuhls bis zu dessen Wiederbesetzung übernommen. Forschung Forschungsschwerpunkte Evolutionäre Informationssysteme Informationssysteme im Gesundheitswesen Datenqualität und Datenintegration Dokument- und Prozessverwaltung (Workflow Management) Forschungsdatenmanagement Forschungsprojekte Adaptives Datenqualitätsmanagement in evolutionär entstandenen Data-Cloud Architekturen (Drittmittelfinanzierte Einzelförderung) Laufzeit: 1. Januar 2023 – 1. Januar 2026Mittelgeber: Siemens AG Abstract We propose to investigate the following research questions:- Which characteristics enable a data quality framework to best identify and cluster the most relevant data quality problems in arbitrary business data landscapes?- Can we capture the knowledge about typical data quality concerns and possible solutions in a knowledge graph in order to infer potential solutions in any given case?- How can the data quality metrics in such a framework be designed in general to align well with fitness for use in different business contexts?To address these questions we will investigate the types of data quality problems that occur in such an environment. We will also investigate and compare possible methods to systematically detect and monitor such data quality problems. We will conceptualize a framework for data quality monitoring based on an extensible metadata schema for data quality concerns.We will extract and classify relevant generalizable data quality problems. Furthermore, we will examine the limitations of such a framework regarding transferability to different IT landscapes. We will develop a set of tools and methods which solve the data quality reporting problem independently from the specific environment. We will evaluate our proposed framework and adaptation strategy through a proof-of-concept implementation. →Mehr Informationen Data Driven Relationship Discovery in Large Time Series Datasets (Drittmittelfinanzierte Einzelförderung) Laufzeit: 1. April 2022 – 31. März 2025Mittelgeber: Siemens AG Abstract Modern complex systems, such as power plants or other industrial structures, combined with the rise of IoT and Industry 4.0, produce thousands of time series measuring different aspects within these systems. As time series measure the state of these complex systems, the correct identification and integration of these time series are key to enabling advanced analytics and further optimization. As acquiring contextual information about each time series and their relations is currently a time-consuming and error-prone manual process, techniques to support or even automate this process are in high demand. While there are different available metadata formats, such as Brick, this metadata often is not available for all data sources and is not commonly used for all systems. Integrating time series at scale requires efficient algorithms and robust concepts that can deal with the heterogeneity and high volume of time series from different domains.Additional Applications and Outcomes:Changepoynt Python PackageChangepoint correlation heavily relies on suitable changepoint detection algorithms, many of which were implemented from research papers within a pip-installable package „changepoynt“ (https://changepoynt.de). Changepoint detection, a critical task in time series analysis, identifies abrupt shifts or transitions in data patterns, offering insights into underlying phenomena. Developed with flexibility and scalability in mind, „changepoynt“ integrates a range of state-of-the-art methods for changepoint detection, empowering researchers across domains to efficiently analyze and interpret their data.CATCH: Contextual Anomaly Tracking with Changepoint DetectionTogether with a research partner from the industry, we basically use the inverse of our idea of relationship discovery to detect contextual anomalies. The hypothesis of the project states that signals, which should have relations (e.g. Input-Output measurements of a dynamical system), behave anomalously if they stop showing simultaneous changes. In contrast to classical anomaly detection methods, change point anomaly (the comparison of multiple changepoint signals) is mainly targeted at contextual anomalies, where two signals are measuring the same component, and consequentially should change at similar times when the plant changes operational status. In case the signals change separately, a contextual anomaly occurs.  While the methods are available in theory, the project is necessary to test the applicability, feasibility, and correct parametrization of the methods for selected use cases. A demonstrator for a two-dimensional case can be found under https://anomaly.changescore.de/ and for the multi-dimensional case under https://heatmap.changescore.de/. →Mehr Informationen Processing Heterogeneous Assets and Resources to discover Ontologies and Semantics (Projekt aus Eigenmitteln) Laufzeit: seit 1. Juni 2019 Abstract Der Zweck des Semantic Web ist es, den weltweiten Zugang zum Wissen der Menschheit in maschinenverarbeitbarer Form zu ermöglichen. Ein großes Hindernis dabei ist, dass Wissen oft entweder inkohärent repräsentiert oder gar nicht externalisiert und nur in den Köpfen von Menschen vorhanden ist. Der Aufbau eines Wissensgraphen und die manuelle Erstellung und Fortschreibung einer Ontologie durch einen Domänenexperten ist eine mühsame Arbeit, die einen großen initialen Aufwand erfordert, bis das Ergebnis verwendet werden kann. Infolgedessen wird vieles Wissen dem Semantic Web oft nie zur Verfügung gestellt werden. Ziel dieser Doktorarbeit ist die Entwicklung eines neuen Ansatzes zum Aufbau von Ontologien aus implizitem Benutzerwissen, das in verschiedenen Artefakten wie Anfrageprotokollen oder Nutzerverhalten verborgen ist. →Mehr Informationen Schemainferenz und maschinelles Lernen (Projekt aus Eigenmitteln) Laufzeit: 1. August 2018 – 30. September 2021 Abstract Im Rahmen des Projekts SIML (Schemainferenz undmaschinelles Lernen) sollen aus unstrukturierten und semi-strukturierten Daten Informationen gewonnen werden, aus denen ein partielles konzeptuelles Schema abgeleitet werden kann. Methoden der topologischen Datenanalyse (TDA) werden in Kombination mit maschinellen Lernverfahren eingesetzt um dies weitestgehend zu automatisieren. Die Untersuchung von topologischen Merkmalen bietet eine Möglichkeit Informationen über Daten zu gewinnen, die als qualitativ verstanden werden. Insbesondere interessieren wir uns für eine stabile, persistente Form von natürlichen Daten bei der Verwendung von unüberwachten Lernverfahren. Als Kernkonzept sollen funktionale Abhängigkeiten nach der Aufbereitung der Daten untersucht werden, mit deren Hilfe anschließend ein geeignetes Schema definiert werden kann. Dabei gibt es Parallelen und Unterschiede für Zeitreihen bzw. persistente Daten, die ebenfalls herausgearbeitet werden sollen.Motivation des Projekts ist der Nachweis, dass Schemataeine natürliche geometrische Struktur in Form eines Simplizialkomplexes aufweisen,die mittels topologischer Methoden untersucht bzw. sichtbar gemacht werden kann. →Mehr Informationen Sprechaktbasiertes Fallmanagement (Projekt aus Eigenmitteln) Laufzeit: 1. Januar 2015 – 30. September 2018 Abstract Fallmanagementsysteme unterstützen Interaktionen zwischen kooperierenden Benutzern typischerweise, indem gemeinsam zu verwendende Dokumente in einem gemeinsamen Repositorium vorgehalten werden. Im vorliegenden Projekt wird untersucht, ob und wie diese Interaktionen durch Klassifikation als Sprechakte besser unterstützt werden können. Die Sprechakt-Theorie beschreibt die pragmatischen Aspekte kommunikativen Handelns. Dabei werden Äußerungen je nach der pragmatischen Intention des Sprechers in verschiedene Typen von Sprechakten unterteilt, z.B. Fragen, Versprechen, Aufforderungen etc. Diese Intention ist dem Sprecher wohl bewusst, nicht jedoch den Fallmanagementsystemen, die ihn bei seiner Tätigkeit unterstützen sollen.Im Rahmen des Projekts wird erforscht, wie genau die Intention explizit gemacht werden kann, ohne damit den Dokumentationsaufwand substantiell zu erhöhen. Gelingt dies, können aus den getätigten Sprechakten Inferenzen abgeleitet werden, z.B. um an die Erfüllung von Versprechen zu erinnern, Fehler zu vermeiden und fallspezifisch bekannte Kommunikationsmuster zu unterstützen.Um den konkreten Bedarf an Sprechaktunterstützung bewerten zu können, sollen die Anforderungen von Wissensarbeitern verschiedener Domänen sowie aktueller ACM-Systeme und Groupware analysiert werden (ACM = Adaptive Case Management). Basierend auf diesen Anforderungen soll eine Architektur für ein sprechaktbasiertes ACM-System entworfen werden. →Mehr Informationen Open and Collaborative Query-Driven Analytics (Projekt aus Eigenmitteln) Laufzeit: 1. November 2013 – 31. August 2024 Abstract Mehr und mehr Unternehmen sammeln möglichst alle anfallenden Daten in sogenannten „Data Lakes“. Obwohl die Daten damit prinzipiell für beliebige Analysen zur Verfügung stehen, bleibt es dennoch unerlässlich für die Analyse, ein Verständnis für die Bedeutung und die Verknüpfungsoptionen der Daten zu entwickeln. Analysten, die diese Arbeit bereits geleistet haben, formulieren Anfragen, in denen solches Wissen implizit enthalten ist. Wenn dieses Wissen jedoch nicht mit anderen geteilt",
  "content_length": 64817,
  "method": "requests",
  "crawl_time": "2025-12-01 14:18:00"
}