{
  "name": "Fred Sala 0001",
  "homepage": "https://pages.cs.wisc.edu/~fredsala",
  "status": "success",
  "content": "Frederic Sala, University of Wisconsin-Madison FREDERIC SALA Assistant Professor, University of Wisconsin CS fredsala@cs.wisc.edu I lead the Sprocket Lab at UW-Madison, where we study the fundamentals of data-driven systems and machine learning. We are especially interested in data- and compute-efficient systems. Much of our current work focuses on data-centric AI, foundation models, and automated machine learning. I have a research leadership role at Snorkel AI, where we are building a data-first approach to AI. Previously, I was a postdoc in Stanford CS, associated with the Hazy group. I completed my Ph.D. in electrical engineering at UCLA, where I worked with the LORIS and StarAI groups. Lab | Research | Bio | CV | Scholar | X | Bluesky | Publications | Awards News Two papers from our group accepted at NeurIPS! Jiayu performs a fine-grained analysis of RL for reasoning (collaboration with folks from Salesforce). We improve weak verifiers with weak supervision (with our Stanford collaborators). TPBench, our new theoretical physics benchmark for AI models, was accepted to Machine Learning: Science and Technology! Manticore was accepted to COLM 2025! Try it out---get state-of-the-art hybrid architectures for cheap! Excited to be part of a team receiving a Research Forward award to build hyperspectral foundation models! We received a DARPA SAFRON award to sponsor our work on safe programmatic distillation---very excited to collaborate! Received the UW-Madison SACM Students' Choice Professor of the Year Award. Thank you to the students! My group and I won the DARPA Young Faculty Award, which will sponsor our work on data-efficient foundation models! Very excited for the upcoming collaborations. Research My group's work has most recently focused on: (more detailed) Data-centric AI, including: Data mixtures. How do we get optimal data for model training? Ex: R&B, Grad-Mimic, Skill-it! Weak supervision & efficient annotation. How do we generate, label, or annotate training data---with little or no ground truth? Ex: Data-centric W2S, The ALCHEmist, UniversalWS, TBAL, Colander Evaluation & Benchmarks. How do we validate models, techniques, and algorithms? Ex: TPBench, BoxWRENCH, AutoWS-Bench-101 Foundation Models, Architectures & AutoML: How do we design, build, and automate development for foundation models and foundation model-powered systems? Ex: Manticore, COSMOS Model Steering & Efficient Adaptation: How do we get models to behave the way we want them to---without extra training or data? Ex: RoboShot, OTTER, Chameleon, AlignEZ AI For Science. How do we use AI&ML to accelerate science? Ex: TheoreticalPhysicsBench, Multi-SALAD Data-centric AI, including: Data mixtures. How can we develop data that accelerates model training and induces desired behaviors in our models? How do we understand the effects of data choices on training, inference, and downstream? R&B (new!) Learns data domains and computes optimal mixtures with nearly zero overhead. Grad-Mimic (Dataworld '25 Oral) Data selection via gradient guidance from pretrained models. Skill-it! (NeurIPS '23 Spotlight) Generate optimal LLM training data mixtures by tracking skills. Weak supervision & efficient annotation. How can we obtain high-quality data for training, fine-tuning, RLHF, etc---without painful manual creation & annotation? We study techniques like weak-to-strong generalization, weak supervision, semi-supevised learning, self-training, and more. Data-centric W2S (ICLR '25) Shows that the driver behind weak-to-strong generalization is data that simultaneously admits multiple \"solutions\". The ALCHEmist (NeurIPS '24 Spotlight) Distills LLM labelers into programs for massive savings. Colander (NeurIPS '24) Learns optimal confidence functions for auto-labeling. TBAL (NeurIPS '23 Spotlight) First analysis of the fundamentals of auto-labeling. UniversalWS (ICLR '22) Vast generalization of weak supervision to any kind of annotation. Evaluation & Benchmarks. How do we validate models, techniques, and algorithms? TPBench (ML:ST '25) Theoretical physics reasoning benchmark testing frontier model capabilities. BoxWRENCH ( NeurIPS D&B '24) Realistic and specialized weak supervision benchmark. AutoWS-Bench-101 ( NeurIPS D&B '22) Benchmark testing automated weak supervision methods. Foundation Models, Architectures & AutoML. Given the vast space of pretrained models, established and emerging model architectures, and multi-model pipelines and workflows---how can we help practitioners make the optimal choice? Manticore (COLM '25) Learned hybrid model architectures: automatically and efficiently combine Transformers, SSMs, and more with our generalization of neural architecture search . COSMOS (new!) Ultra-efficient per-task LLM selection and design. Model Steering & Efficient Adaptation. How do we get models to behave as desired without needing more compute, training, data acquisition, etc. ? RoboShot (ICLR '24, R0-FoMo '23 Best Paper Honorable Mention) Makes models robust via LLM-generated insights and ultra-efficient model steering. OTTER (NeurIPS '24) Fix mismatch in pretrained model predictions distributions via optimal transport. Chameleon (NAACL Findings '25) Personalize LLMs by synthesizing user preferences and steering. AlignEZ ('24) Low-cost multi-objective alignment via synthetic preferences and representation editing. AI for Science. How do we use AI/ML to accelerate scientific discovery? TPBench (ML:ST '25) Theoretical physics reasoning benchmark testing frontier model capabilities. Multi-SALAD (JHEP '23)Anomaly detection with multiple reference datasets for resonant searches in HEP. Students Graduate students I advise / co-advise / frequently collaborate with: Nick Roberts Changho Shin Dyah Adila Brian Huang Sonia Cromp Kendall Park Albert Ge Zhiqi Gao Sungjun (June) Cho Jiayu Wang Gabe Orlanski Alumni Harit Vishwakarma (co-advised with Ramya Vinayak). Next: Postdoc at Oxford. Jitian Zhao (co-advised with Karl Rohe). Next: Meta Publications Conference | Journal | Preprint | Workshop 2025 Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning Jiayu Wang, Yifei Ming, Zixuan Ke, Caiming Xiong, Shafiq Joty, Aws Albarghouthi, Frederic Sala Neural Information Processing Systems (NeurIPS), 2025 arXiv Shrinking the Generation-Verification Gap with Weak Verifiers Jon Saad-Falcon, E. Kelly Buchananâ â, Mayee F. Chen, Tzu-Heng Huang, Brendan McLaughlin, Tanvir Bhathal, Shang Zhu, Ben Athiwaratkun, Frederic Sala, Scott Linderman, Azalia Mirhoseini, Christopher RÃ© Neural Information Processing Systems (NeurIPS), 2025 arXiv Theoretical Physics Benchmark (TPBench) -- a Dataset and Study of AI Reasoning Capabilities in Theoretical Physics Daniel J.H. Chung, Zhiqi Gao, Yurii Kvasiuk, Tianyi Li, Moritz MÃ¼nchmeyer, Maja Rudolph, Frederic Sala, Sai Chaitanya Tadepalli Machine Learning: Science and Technology, 2025 arXiv | Site Pretrained Hybrids with MAD Skills Nicholas Roberts, Samuel Guo, Zhiqi Gao, Srinath Namburi, Sonia Cromp, Chengjun Wu, Chengyu Duan, Frederic Sala Conference on Language Modeling (COLM), 2025 arXiv Rethinking Confidence and Thresholds in Pseudolabeling-based SSL Harit Vishwakarma, Yi Chen, Srinath Namburi, Sui Jiet Tay, Ramya Korlakai Vinayak, Frederic Sala International Conference on Machine Learning (ICML), 2025 draft Weak-to-Strong Generalization Through the Data-Centric Lens Changho Shin, John Cooper and Frederic Sala International Conference on Learning Representations (ICLR), 2025 arXiv Personalize Your LLM: Fake it then Align It Yijing Zhang, Dyah Adila, Changho Shin, and Frederic Sala NAACL Findings, 2025 OpenReview Product Manifold Representations for Learning on Biological Pathways Daniel McNeela, Frederic Sala, Anthony Gitter Great Lakes Bioinformatics (GLBIO), 2025 arXiv Tabby: Tabular Data Synthesis with Language Models Sonia Cromp, Satya Sai Srinath Namburi, Mohammed Alkhudhayri, Catherine Cao, Samuel Guo, Nicholas Roberts, Frederic Sala Preprint, 2025 arXiv TARDIS: Mitigate Temporal Misalignment via Representation Steering, Changho Shin, Xinya Yan, Suenggwan Jo, Sungjun Cho, Shourjo Aditya Chaudhuri, Frederic Sala Preprint, 2025 arXiv 2024 The AlCHEmist: Automated Labeling 500x CHEaper than LLM Data Annotators Tzu-Heng Huang, Catherine Cao, Vaishnavi Bhargava, Frederic Sala Neural Information Processing Systems (NeurIPS), 2024 (Spotlight) arXiv Pearls from Pebbles: Improved Confidence Functions for Auto-labeling Harit Vishwakarma, Sui Jiet Tay, Srinath Namburi, Frederic Sala, Ramya Korlakai Vinayak Neural Information Processing Systems (NeurIPS), 2024 arXiv OTTER: Improving Zero-Shot Classification via Optimal Transport Changho Shin, Jitian Zhao, Sonia Cromp, Harit Vishwakarma, Frederic Sala Neural Information Processing Systems (NeurIPS), 2024 arXiv Stronger Than You Think: Benchmarking Weak Supervision on Realistic Tasks Tianyi Zhang, Linrong Cai, Jeffrey Li, Nicholas Roberts, Neel Guha, Frederic Sala Neural Information Processing Systems (NeurIPS) (Datasets and Benchmarks Track), 2024 arXiv Look Whoâs Talking Now: Covert Channels From Biased LLMs Daniel Silva, Frederic Sala, Ryan Gabrys Empirical Methods in Natural Language Processing (EMNLP) Findings, 2024 Paper Zero-Shot Robustification of Zero-Shot Models Dyah Adila, Changho Shin, Linrong Cai, Frederic Sala International Conference on Learning Representations (ICLR), 2024 OpenReview | arXiv | code | blog Is Free Self-Alignment Possible? Dyah Adila, Changho Shin, Yijing Zhang, Frederic Sala Preprint, 2024 arXiv 2023 Domain Generalization via Nuclear Norm Regularization Zhenmei Shi, Yifei Ming, Ying Fan, Frederic Sala, Yingyu Liang Conference on Parsimony and Learning (CPAL) , 2023 (Oral) arXiv The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models Srinath Namburi, Makesh Narsimhan Sreedhar, Srinath Srinivasan, Frederic Sala Empirical Methods in Natural Language Processing (EMNLP) Findings",
  "content_length": 21486,
  "method": "requests",
  "crawl_time": "2025-12-01 13:11:54"
}