{
  "name": "Ying Wei 0001",
  "homepage": "https://wei-ying.net",
  "status": "success",
  "content": "Ying WEI hide forever  | hide once Home Ying WEI (魏 颖) ying.wei [at] zju [dot] edu [dot] cn I am currently a ZJU-100 Young Professor with College of Computer Science and Technology, Zhejiang University. I am generally interested in developing algorithms that equip machines with more general intelligence via knowledge transfer and compositionality. This includes allowing continuous transfer and adaptation of the knowledge previous learned (nowadays in LLMs) to quickly learn the current task with minimal human supervision, and autonomously evaluating the success of knowledge transfer. I am also passionate about applying these algorithms into real-world applications with small data, e.g., AI for Science. Previously, I was a Nanyang Assistant Professor with College of Computing and Data Science, Nanyang Technological University, an Assistant Professor at Department of Computer Science, City University of Hong Kong and a senior researcher at Tencent AI Lab. I completed my Ph.D. in Computer Science and Engineering at Hong Kong University of Science and Technology under the supervision of Professor Qiang Yang, and my B.S. in Automation at Huazhong University of Science and Technology. I have also spent time interning at Microsoft Research Asia. I am looking for highly-motivated full-time PhD students, post-doctoral research fellows, and also research assistants! PhD Thesis  / LinkedIn  / Google Scholar News [Sep 2024] Our paper \"Automatic Expert Discovery in LLM Upcycling via Sparse Interpolated Mixture-of-Experts\" was accepted by ACL 2025! [Sep 2024] Our paper \"Come Together, But Not Right Now: A Progressive Strategy to Boost Low-Rank Adaptation\" was accepted by ICML 2025! [Sep 2024] Our paper \"Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning\" was accepted by ICLR 2025! [Sep 2024] Our paper \"SD-LoRA: Scalable Decoupled Low-Rank Adaptation for Class Incremental Learning\" was accepted by ICLR 2025! [Sep 2024] Our paper \"CLDyB: Towards Dynamic Benchmarking for Continual Learning with Pre-trained Models\" was accepted by ICLR 2025! [Sep 2024] Our paper \"Time-Varying LoRA: Towards Effective Cross-Domain Fine-Tuning of Diffusion Models\" was accepted by NeurIPS 2024! [Sep 2024] Our paper \"Learning Where to Edit Vision Transformers\" was accepted by NeurIPS 2024! [Sep 2024] Our paper \"Mixture of Adversarial LoRAs: Boosting Robust Generalization in Meta-tuning\" was accepted by NeurIPS 2024! [Sep 2024] Our paper \"DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs\" was accepted by NeurIPS 2024! [Sep 2024] Our paper \"Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing\" was accepted by EMNLP 2024! [Sep 2024] Serving as an Area Chair for ICLR 2025 [July 2024] We are organizing the NeurIPS 2024 Workshop on Compositional Learning! For more information, please visit our website. [July 2024] We are organizing the NeurIPS 2024 Workshop on Advancements In Medical Foundation Models! For more information, please visit our website. [May 2024] Our paper \"Understanding and Patching Compositional Reasoning in LLMs\" was accepted by ACL 2024 Findings! [May 2024] Our paper \"Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generatio\" was accepted by ACL 2024! [May 2024] Honored to receive the ICLR 2024 Outstanding Honorable Mention! [May 2024] Our paper \"Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts\" was accepted by ICML 2024! [May 2024] Our paper \"One Meta-tuned Transformer is What You Need for Few-shot Learning\" was accepted by ICML 2024! [May 2024] Our paper \"Mitigating Catastrophic Forgetting in Online Continual Learning by Modeling Previous Task Interrelations\" was accepted by ICML 2024! [May 2024] Our paper \"Federated Continual Learning via Prompt-based Dual Knowledge Transfer\" was accepted by ICML 2024! [Apr 2024] Serving as an Area Chair for NeurIPS 2024 [Feb 2024] Our paper \"MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric\" was accepted by CVPR 2024! [Jan 2024] Our paper \"Meta Continual Learning Revisited: Implicitly Enhancing Online Hessian Approximation via Variance Reduction\" was accepted by ICLR 2024 (oral)! [Jan 2024] Our paper \"Gradual Domain Adaptation via Gradient Flow\" was accepted by ICLR 2024 (spotlight)! [Jan 2024] Our paper \"Active Retrosynthetic Planning Aware of Route Quality\" was accepted by ICLR 2024! [Jan 2024] Serving as an Area Chair for ICML 2024 [Jan 2024] Serving as an Action Editor for Transactions on Machine Learning Research (TMLR) Publications ( ___: equal contribution, *: corresponding author) Automatic Expert Discovery in LLM Upcycling via Sparse Interpolated Mixture-of-Experts Shengzhuang Chen, Ying Wei*, Jonathan Richard Schwarz* Sixty-third Annual Meeting of the Association for Computational Linguistics (ACL), 2025 pdf / code (oral) Existing dense LLMs lack efficient mechanisms for specializing across multiple domains without excessive computational overhead. This work proposes SIMoE, an end-to-end method that fine-tunes dense LLMs into sparse, domain-specialized MOEs. SIMoE identifies structurally sparse domain-specific experts and learns a router network for input-dependent expert merging. This achieves superior generalization and state-of-the-art instruction-tuning performance while maintaining optimal computational efficiency. Come Together, But Not Right Now: A Progressive Strategy to Boost Low-Rank Adaptation Zhan Zhuang, Xiequn Wang, Wei Li, Yulong Zhang, Qiushi Huang, Shuhao Chen, Xuehao Wang, Yanbin Wei, Yuhe Nie, Kede Ma, Yu Zhang*, Ying Wei* Forty-second International Conference on Machine Learning (ICML), 2025 pdf / code Current LoRA fine-tuning often traps LoRAs near suboptimal initializations, limiting model generalization and downstream adapter manipulation (merging/pruning). This work introduces CoTo, progressively increasing LoRA activation during training. This stochastic method promotes better optimization, stability, and exploration. Theoretically, CoTo supports dropout stability and linear mode connectivity, measuring LoRAs' contributions through cooperative-game analysis. Experiments show CoTo improves performance, merging accuracy, pruning robustness, and training efficiency, compatible across diverse LoRA methods. Reaction Graph: Towards Reaction-Level Modeling for Chemical Reactions with 3D Structures Yingzhao Jian, Yue Zhang, Ying Wei, Hehe Fan, Yi Yang Forty-second International Conference on Machine Learning (ICML), 2025 pdf / code Current AI approaches excel in single-molecule property predictions but largely overlook modeling intermolecular interactions, especially chemical reactions. This paper introduces Reaction Graph (RG), a unified representation capturing both reactants and products' molecular structures and interatomic relationships in chemical reactions, explicitly incorporating 3D molecular information. RG significantly improves performance on reaction classification, condition prediction, and yield prediction tasks, achieving state-of-the-art accuracy across multiple datasets. Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning Gangwei Jiang, Caigao Jiang, Zhaoyi Li, Siqiao Xue, Jun Zhou, Linqi Song, Defu Lian*, Ying Wei* Thirteenth International Conference on Learning Representations (ICLR), 2025 pdf / code (oral) Current studies on catastrophic forgetting in LLMs mainly analyze a single training sequence and overlook how different tasks and model architectures influence forgetting, lacking a deeper understanding of underlying mechanisms. This work introduces function vectors (FVs) to interpret and quantify forgetting in LLMs, revealing that CF stems from biased function activation rather than task overwriting. Based on this insight, the authors propose an FV-guided training method with regularization to stabilize FVs and reduce forgetting. The approach is theoretically grounded and empirically validated across four benchmarks. SD-LoRA: Scalable Decoupled Low-Rank Adaptation for Class Incremental Learning Yichen Wu, Hongming Piao, Long-Kai Huang, Renzhen Wang, Hanspeter Pfister, Deyu Meng, Kede Ma*, Ying Wei* Thirteenth International Conference on Learning Representations (ICLR), 2025 pdf / code (oral) The paper tackles the scalability limitations of existing continual learning methods with foundation models—specifically, the need to expand prompt/LoRA pools or store task data, which becomes inefficient as the number of tasks increases. The paper introduces SD-LoRA, a scalable continual learning method that avoids storing past data or expanding model components. It separates how LoRA learns direction and magnitude to improve stability and flexibility. The method supports efficient end-to-end training and inference, and performs strongly across multiple benchmarks and foundation models. CLDyB: Towards Dynamic Benchmarking for Continual Learning with Pre-trained Models Shengzhuang Chen, Yikai Liao, Xiaoxiao Sun, Kede Ma*, Ying Wei* Thirteenth International Conference on Learning Representations (ICLR), 2025 pdf / code The paper addresses two key issues in continual learning with foundation models: data contamination risks in large-scale pre-training datasets, and the static, overly simplistic nature of standard benchmarks that fail to reflect real-world complexities, leading to poor robustness and overfitting. It introduces CLDyB, a dynamic benchmarking framework that generates challenging task sequences via tree-search to better evaluate continual learning methods. CLDyB identifies performance gaps in current methods, offers insight into their strengths and weaknesses, and provides the community with reusable, high-difficulty benchmarks for more robust CL evaluation. Time-Vary",
  "content_length": 51204,
  "method": "requests",
  "crawl_time": "2025-12-01 14:51:30"
}