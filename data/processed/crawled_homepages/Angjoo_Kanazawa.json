{
  "name": "Angjoo Kanazawa",
  "homepage": "http://people.eecs.berkeley.edu/~kanazawa",
  "status": "success",
  "content": "Angjoo Kanazawa Angjoo Kanazawa I am an Assistant Professor in the Department of Electrical Engineering and Computer Sciences at the University of California, Berkeley. I lead the Kanazawa AI Research (KAIR) lab under BAIR. Alongside my academic work, I have served as Chief Technical Advisor for Luma AI and on the advisory board of Wonder Dynamics. I am also an Amazon Scholar in the Frontier AI & Robotics team. Previously, I was a Research Scientist at Google Research, and BAIR postdoc at UC Berkeley advised by Jitendra Malik, Alexei A. Efros and Trevor Darrell. I completed my PhD in Computer Science at the University of Maryland, College Park with my advisor David Jacobs. During my PhD, I had the pleasure to visit the Max Planck InstituteÂ in TÃ¼bingen, Germany under the guidance of Michael Black. CV | Google Scholar | twitter | bluesky Email: kanazawa (at) eecs.berkeley.edu Dear prospective students: Click here for information. Thank you for your interest in joining my group! Sorry I can't respond to your email if you ask me about PhD admissions, please apply directly through the department, thank you! PhD applicants: I will be looking to hire one or two graduate students every year, so please apply through the EECS department, apply for the CS division, and if you want to work with me, its best to apply in AI-CV or AI-ROB. Graduate or undergraduate students at UCB: Please send me an email with your interests, resume and your transcript. For undergraduates, the requirement is a solid engineering internship and CS189 or CS182/282. I prefer students who've taken some of these classes: CS184/284, CS194-26/294-26, CS280, and advanced math courses. Experience with Unity/Blender/Unreal is a plus, art/photography experience is also a plus. Here are some more information to get you started: If you're interested in NeRF research, use nerf.studio and make two cool captures! Be creative. Then, address at least two github issues. You can also look into the gsplat library, and resolve issues there as well. Then send me an email with your results and the PRs. If you're interested in research to perceive 3D people, like SLAHMR, run SLAHMR on two of your videos. Then send me an email with the results and your thoughts. Interns: We are not taking visitors or students who are not at UC Berkeley at this time. News April 2025: Talk at the MIT Robotics Seminar: October 2024: Gave talks at ECCV 2024 workshops: Details and slides 9/29 Sunday 11:00am 3D Vision and Modelling Challenges in eCommerce: I will talk about how to scale NeRFs across multiple GPUs (ECCV 2024), and the latest updates on our opensource tools: nerfstudio, gsplat, and viser: Slides 9/30 Monday 9:45am VENUE: Video Content Understanding and Generation: about our latest works in 4D reconstruction like Shape of Motion, challenges thereof, and more. 9/30 Monday 11:10am Wild3D: 3D Modeling, Reconstruction, and Generation in the Wild: about incorporating semantics — grouping — in 3D, and how to use part knowledge for 4D reconstruction of objects with movable parts for robotics! We just released this work, to be presented at CoRL 2024 (oral).: Slides 9/30 Monday 2:35pm AI for 3D Content Creation: about our new NeurIPS 2024 paper that makes sense of SDS!: Slides 9/30 Monday 4:50pm Foundation Models for 3D Humans: last but not least, about hard problems that need to be solved for 3D human foundation models and our latest work EgoAllo: Slides . June 2024: Received the PAMI Young Researcher Award, thank you! Oct 2023: Talk on Creative Horizons with 3D Capture at Stanford HAI Fall Conference. Link March 2023: GTC talk on nerf.studio: Link February 2023: Named a 2023 Sloan Research Fellow, thank you! October 2022: We ran a tutorial on NeRFs at ECCV 2022, all videos here. Research My research lies at the intersection of computer vision, computer graphics, and machine learning. We live in a 3D world that is dynamic, full of life with people and animals interacting with each other and the environment. How can we build a system that can capture, perceive, and understand this complex 4D world like humans can from everyday photographs and video? More generally, how can we develop a computational system that can continually learn a model of the world from visual observations? The goal of my lab is to answer these questions. Kanazawa AI Research (KAIR) members Postdoc Dr. Tyler Bonnen (with Jitendra Malik and Alexei Efros) Dr. Jiahui Lei (with Trevor Darrell) Graduate Students Vongani Maluleke (with Jitendra Malik) Justin Kerr (with Ken Goldberg) Brent Yi (with Yi Ma) Chung Min Kim (with Ken Goldberg) David McAllister Hongsuk Choi (with Jitendra Malik) Visiting PhD student: Haven (Haiwen) Feng Arthur Allshire (with Jitendra Malik and Pieter Abbeel) Isabella Yu (with Alexei Efros) 5th year MS students Anthony Zhang Alumni Former PhD Students Dr. Hang Gao, now at xAI Dr. Ruilong Li, at NVIDIA Dr. Ethan Weber, now at Meta Reality Labs Dr. Vickie Ye, now at Anthropic Dr. Evonne Ng (with Trevor Darrell), now at Meta Reality Labs Dr. Shubham Goel (with Jitendra Malik), now at Avataar Dr. Matt Tancik (with Ren Ng), now at Luma AI Former Postdocs Prof. Qianqian Wang, will be starting as assistant professor at Harvard University 2026 Prof. Aleksander Holynski, now an assistant professor at Columbia University Dr. Lea MÃ¼ller, now at Meta Prof. Georgios Pavlakos, now an assistant professor at UT Austin Former Visitors Dr. Songwei Ge (visiting PhD student from UMD), now at Reve Dr. Frederik Warburg (visiting PhD student from Technical University of Denmark Fall 2022 - Spring 2023), now at Tenton.ai Former MS/BS Students Jake Austin (5th yr MS), now PhD student at MIT Alex Yu, (undergraduate 2020-2022) co-founder @ Luma AI, now at OpenAI Micael Tchapmi, (visitor 2020-2021) now MS student @ Stanford Ze (Edward) Ma, (visitor 2020-2021) now MS Student @ Columbia Jason Y. Zhang (undergraduate 2019-2020) PhD student @ CMU, now at Google Xin Qin (undergraduate 2018-2019) PhD student @ USC, now Assistant Professor at CSULB Libraries Papers Please see google scholar for latest updates. Viser: Imperative, Web-based 3D Visualization in Python Brent Yi, Chung Min Kim, Justin Kerr, Gina Wu, Rebecca Feng, Anthony Zhang, Jonas Kulhanek, Hongsuk Choi, Yi Ma, Matthew Tancik, Angjoo Kanazawa ArXiv 2025 [Project Page] [Paper] [Code] [bibtex] Flow Matching Policy Gradients David McAllister*, Songwei Ge*, Brent Yi*, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, Angjoo Kanazawa ArXiv 2025 [Project Page] [Paper] [bibtex] Cameras as Relative Positional Encoding Ruilong Li*, Brent Yi*, Junchen Liu*, Hang Gao, Yi Ma, Angjoo Kanazawa NeurIPS 2025 [Project Page] [Paper] [Code] [bibtex] Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop Justin Kerr, Kush Hari, Ethan Weber, Chung Min Kim, Brent Yi, Tyler Bonnen, Ken Goldberg, Angjoo Kanazawa CoRL 2025 [Project Page] [Paper] [bibtex] Visual Imitation Enables Contextual Humanoid Control Arthur Allshire*, Hongsuk Choi*, Junyi Zhang*, David McAllister*, Anthony Zhang, Chung Min Kim, Trevor Darrell, Pieter Abbeel, Jitendra Malik, Angjoo Kanazawa CoRL 2025, Best Student Paper [Project Page] [Paper] [Code] [bibtex] PyRoki: A Modular Toolkit for Robot Kinematic Optimization Chung Min Kim*, Brent Yi*, Hongsuk Choi, Yi Ma, Ken Goldberg, Angjoo Kanazawa IROS 2025 [Project Page] [Paper] [Code] [bibtex] St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World Haiwen Feng*, Junyi Zhang*, Qianqian Wang, Yufei Ye, Pengcheng Yu, Michael J. Black, Trevor Darrell, Angjoo Kanazawa ICCV 2025 [Project Page] [Paper] [Code] [bibtex] Predict-Optimize-Distill: A Self-Improving Cycle for 4D Object Understanding Mingxuan Wu*, Huang Huang*, Justin Kerr, Chung Min Kim, Anthony Zhang, Brent Yi, Angjoo Kanazawa ICCV 2025 [Project Page] [Paper] [bibtex] Shape of Motion: 4D Reconstruction from a Single Video Qianqian Wang*, Vickie Ye*, Hang Gao*, Weijia Zeng*, Jake Austin, Zhengqi Li, and Angjoo Kanazawa ICCV 2025 [Project Page] [Code] [Paper] [Interactive Results] [bibtex] Segment Any Motion in Videos Nan Huang, Wenzhao Zheng, Chenfeng Xu, Kurt Keutzer, Shanghang Zhang, Angjoo Kanazawa, Qianqian Wang CVPR 2025 [Project Page] [Paper] [Code] [bibtex] Continuous 3D Perception Model with Persistent State Qianqian Wang*, Yifei Zhang*, Aleksander Holynski, Alexei A. Efros, and Angjoo Kanazawa CVPR 2025 [Project Page] [Paper] [Interactive Results] [bibtex] Decentralized Diffusion Models David McAllister, Matthew Tancik, Jiaming Song, and Angjoo Kanazawa CVPR 2025 [Blog] [Paper] [bibtex] Reconstructing People, Places, and Cameras Lea MÃ¼ller*, Hongsuk Choi*, Anthony Zhang, Brent Yi, Jitendra Malik, and Angjoo Kanazawa CVPR 2025 [Project Page] [Paper] [bibtex] Estimating Body and Hand Motion in an Ego-Sensed World Brent Yi, Vickie Ye, Maya Zheng, Lea MÃ¼ller, Georgios Pavlakos, Yi Ma, Jitendra Malik, and Angjoo Kanazawa CVPR 2025 [Project Page] [Code] [Paper] [Interactive Results] [bibtex] MegaSaM: Accurate, Fast and Robust Structure and Motion from Casual Dynamic Videos Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely CVPR 2025, Best Paper Honorable Mention [Project Page] [Interactive Results] [Paper] [bibtex] Is this video of mochi on a walk? Yes! MegaSam FTW Toon3D: Seeing Cartoons from New Perspectives Ethan Weber*, Riley Peterlinz*, Rohan Mathur, Frederik Warburg, Alexei A. Efros, and Angjoo Kanazawa ArXiv 2024 [Project Page] [Labeling Tool] [Paper] [bibtex] Fillerbuster: Multi-View Scene Completion for Casual Captures Ethan Weber, Norman MÃ¼ller, Yash Kant, Vasu Agrawal, Michael ZollhÃ¶fer, Angjoo Kanazawa, Christian Richardt ArXiv 2025 [Project Page] [Paper] [Code] [bibtex] Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos Gengshan Yang, Andrea Bajcsy, Shunsuke Saito, and Angjoo Kanazawa ICLR 2025 [Project Page] [Paper] [bibtex] Spatial Cognition f",
  "content_length": 24507,
  "method": "requests",
  "crawl_time": "2025-12-01 12:58:37"
}