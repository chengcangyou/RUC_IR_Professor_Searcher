{
  "name": "Yaoqing Yang",
  "homepage": "https://sites.google.com/site/yangyaoqingcmu",
  "status": "success",
  "content": "Yaoqing YangSearch this siteEmbedded FilesSkip to main contentSkip to navigationYaoqing YangAssistant ProfessorDepartment of CS, Dartmouth15 Thayer Drive, Hanover, NH 03755-4404Yaoqing.Yang AT dartmouth.eduMy current research focuses on diagnosing and mitigating failures in machine learning models. For example, I analyze shape and geometric features in high-dimensional spaces, such as loss landscapes, weight matrix spectral densities, and decision boundaries, to provide actionable insights for addressing common failure modes in these models. I also apply these techniques to applications such as 3D point clouds and graphs. My research draws inspiration from statistical learning and information theory.You are welcome to email me if you're interested in working with me. Please apply to our PhD program using the link below.PhD in Dartmouth CSMore information about me.Postdoc, RISE Lab, EECS, UC Berkeley.PhD, ECE, CMU.BS, EE, Tsinghua.Google Scholar  |  CV  |  LinkedInNews[Nov 2025] Accepted the invitation to serve as an Area Chair at ICML 2026.[Nov 2025] I am honored to serve as an Associate Editor for TPAMI.[Oct 2025] Our paper \"The false promise of zero-shot super-resolution in machine-learned operators\" is online. [Aug 2025] Accepted the invitation to serve as an Area Chair at ICLR 2026.[July 2025] Our paper \"From spikes to heavy tails\" is accepted by Transactions on Machine Learning Research.[June 2025] I am honored to receive a grant to study HT-SR theory for the quantification and evaluation of AI models.[May 2025] One paper is accepted by the findings of ACL 2025.[May 2025] We have two papers accepted by ICML 2025. See you in Vancouver.[April 2025] Accepted the invitation to serve as a Session Chair at ICLR 2025.[April 2025] I gave a talk at CMU CyLab.[Feb 2025] I gave a talk at Lawrence Berkeley National Laboratory. It was nice to go back and visit New Dumplings again (a low-profile Michelin one-star restaurant on San Pablo Avenue).[Feb 2025] Accepted the invitation to serve as an Area Chair at NeurIPS 2025.[Jan 2025] Our paper \"Mitigating memorization in language models\" is accepted by ICLR 2025 as a spotlight.[Dec 2024] We will organize a workshop on AI for Science at ICLR 2025. Stay tuned![Nov 2024] I gave a talk at Google Research and received helpful feedback to improve our work.[Nov 2024] I am honored to receive the Burke Research Initiation Award from Dartmouth.[Sep 2024] We have two papers accepted by NeurIPS 2024.[Aug 2024] Accepted the invitation to serve as an Area Chair at ICLR 2025.[Aug 2024] I am honored to receive a grant from DOE to study scientific foundation models.[Aug 2024] We uploaded a video to introduce our new ICML paper on model diagnosis.[July 2024] I am honored to receive a grant from DARPA.[July 2024] Two new papers are online. The first paper analyzes the heavy-tailed weight matrix spectrum from the feature learning perspective, and the second paper introduces a new ensemble learning method called SharpBalance.[June 2024] Accepted the invitation to serve as an Area Chair at NeurIPS 2024.[May 2024] Two papers accepted by ICML 2024. Stay tuned![Jan 2024] Our paper \"Teach LLMs to phish: stealing private information from language models\" is accepted by ICLR 2024.[Sep 2023] Our paper on \"Temperature balancing\" has been accepted by NeurIPS 2023 as a spotlight.[Sep 2023] Our paper \"When are ensembles really effective\" is accepted by NeurIPS 2023.Selected publicationsFrom spikes to heavy tails: unveiling the spectral evolution of neural networksVignesh Kothapalli, Tianyu Pang, Shenyang Deng, Zongmin Liu, Yaoqing YangTransactions on Machine Learning Research 2025Summary: This paper uncovers the mechanism behind the emergence of heavy-tailed empirical spectral densities (ESDs). We show that heavy-tailed ESDs arise from the interaction between an ESD spike, caused by feature learning, and the bulk, resulting from iid random weight initialization. This theory reveals several surprising facts: the emergence of a heavy-tailed spectrum (1) does not require SGD noise during training, (2) does not need the model to be overparameterized or sufficiently interpolated, and (3) does not require more than one spike in the ESD.Full paper  |  CodeEigenspectrum analysis of neural networks without aspect ratio biasYuanzhe Hu, Kinshuk Goel, Vlad Killiakov, Yaoqing YangICML 2025Summary: Our group has published several papers on using spectral analysis of weight matrices to identify critical \"under-trained\" layers in neural networks. This insight led to the development of methods like Temperature Balancing and AlphaPruning. However, we recently uncovered a key oversight in our own work: when two weight matrices have different aspect ratios—that is, different m/n ratios for an m-by-n matrix—it is inaccurate to compare their eigenspectra directly. The empirical spectral density is biased by this shape difference, a factor we had overlooked. Correcting this bias leads to consistent improvements across methods like Temperature Balancing and AlphaPruning.Full paper  |  CodeLIFT the veil for the truth: principal weights emerge after rank reduction for reasoning-focused supervised fine-tuningZihang Liu, Tianyu Pang, Oleg Balabanov, Chaoqun Yang, Tianjin Huang, Lu Yin, Yaoqing Yang, Shiwei LiuICML 2025Summary: This paper investigates parameter-efficient fine-tuning (PEFT) of LLMs for reasoning tasks. While PEFT is more efficient than full fine-tuning, it often sacrifices reasoning accuracy. We show how to bridge this gap using a sparse fine-tuning method that delivers both efficiency and accuracy. The key insight is striking: the most important weights for fine-tuning are those with large magnitudes after applying a low-rank approximation to the weight matrices. We call this method “LIFT” because it reveals the useful weights by removing the noisy low-rank components.Full paper  |  CodeWhy LLM safety guardrails collapse after fine-tuning: a similarity analysis between alignment and fine-tuning datasetsLei Hsiung, Tianyu Pang, Yung-Chen Tang, Linyue Song, Tsung-Yi Ho, Pin-Yu Chen, Yaoqing YangICML 2025 Workshop on Data in Generative Models - The Bad, the Ugly, and the GreatsSummary: LLMs are often vulnerable to jailbreak attacks, especially after downstream fine-tuning. Existing mitigation strategies overlook a key factor: the role of the original safety-alignment data. This paper investigates how safety guardrails degrade by examining the representation similarity between upstream alignment datasets and downstream fine-tuning tasks. Our experiments show that high similarity between the two significantly weakens guardrails and increases jailbreak risk, while low similarity leads to more robust models.Full paper  |  BlogMitigating memorization in language modelsMansi Sakarvadia, Aswathy Ajith, Arham Khan, Nathaniel Hudson, Caleb Geniesse, Kyle Chard, Yaoqing Yang, Ian Foster, Michael W. MahoneyICLR 2025Summary: Language models can memorize training data, encoding it in their weights so that inference-time queries trigger verbatim regurgitation. This raises concerns when the data are private or sensitive. In this work, we compare seventeen memorization mitigation methods: three regularizer-based, three fine-tuning-based, and eleven machine unlearning methods, five of which are novel contributions. We also introduce TinyMem, a suite of small, efficient LMs for fast development and evaluation of mitigation strategies. Our experiments reveal that regularizer-based methods are slow and ineffective, fine-tuning methods reduce memorization but are prohibitively expensive when accuracy matters, and unlearning methods are both faster and more effective, enabling precise removal of memorized content before inference. Notably, our proposed unlearning method, BalancedSubnet, outperforms all others in eliminating memorized information while preserving task performance.Full paper  |  Code  |  BlogLossLens: diagnostics for machine learning through loss landscape visual analyticsTiankai Xie, Jiaqing Chen, Yaoqing Yang, Caleb Geniesse, Ge Shi, Ajinkya Chaudhari, John Kevin Cava, Michael W. Mahoney, Talita Perciano, Gunther H. Weber, Ross MaciejewskiIEEE Computer Graphics & Applications 2024Summary: Modern machine learning relies on optimizing a neural network’s parameters via a loss function to learn complex features. Examining this loss function with respect to the network’s parameters—the loss landscape—can reveal key insights into the architecture and learning process. While local landscape structures near individual solutions are well-studied, the global structure, with its many local minima, remains challenging to understand and visualize. To address this, we introduce LossLens, a visual analytics framework for exploring loss landscapes across multiple scales. LossLens integrates local and global metrics into a unified visual representation, making it easy for practitioners to conduct model diagnostics. We showcase its utility through two case studies: visualizing the role of residual connections in ResNet-20, and analyzing how physical parameters affect a physics-informed neural network solving a convection PDE problem.Full paper  |  Code  |  VideoModel balancing helps low-data training and fine-tuningZihang Liu*, Yuanzhe Hu*, Tianyu Pang, Yefan Zhou, Pu Ren, Yaoqing YangEMNLP 2024Summary: As shown in our previous work Temperature Balancing, different layers of an LLM can be highly imbalanced—some layers are much better trained than others. Here, we reveal that this imbalance is even more pronounced at the start of training, especially during fine-tuning with limited downstream data. This paper is also the first in which we apply layer-balancing techniques to scientific machine learning tasks.Full paper  |  CodeAlphaPruning: using heavy-tailed self regularization theory for improved layer-wise pruning of large language modelsHaiquan Lu*, Yefan Zhou*, Shiwei Liu, Zhangyang Wang, Michael W. Mahoney, Yaoqing YangNeurIPS 2024Summa",
  "content_length": 24562,
  "method": "requests",
  "crawl_time": "2025-12-01 14:50:35"
}