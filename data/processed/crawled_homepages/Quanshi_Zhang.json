{
  "name": "Quanshi Zhang",
  "homepage": "http://qszhang.com",
  "status": "success",
  "content": "Quanshi Zhang | 张拳石 Quanshi Zhang Associate Professor 国家级海外高层次人才引进计划  ACM China新星奖 John Hopcroft Center for Computer Science, School of electronic information and electrical engineering, Shanghai Jiao Tong University Email: zqs1022 [AT] sjtu.edu.cn      [知乎] （Curriculum Vita） News. Serve as an Area Chair in NeurIPS 2024. 招生 Prospective Ph.D., Master, and undergraduate students: I am looking for highly motivated students to work together on the interpretability of neural networks, deep learning theory, and other frontier topics in machine learning and computer vision. Please read “写给学生” and send me your CV and transcripts. Research Interests My research mainly focuses on explainable AI, including XAI theories, designing interpretable neural networks, and explaining the representation power (e.g., the adversarial robustness and generalization power) of neural networks. In particular, I aim to build up a theoretic system based on game-theoretic interactions, which provides a new perspective to theoretically connect symbolic concepts encoded by a DNN with the DNN’s generalization power and robustness. I also use the game-theoretic interaction to prove the common mechanism shared by many recent heuristic deep-learning methods. Tutorials & invited talk in explainable AI VALSE 2022 Keynote [Website] IJTCS 2022, invited talk [Website] CVPR 2022 Workshop on the Art of Robustness, Invited talk [Website] 世界人工智能大会（WAIC）可信AI论坛 Panel Discussion [Website] VALSE 2021 Tutorial on Interpretable Machine Learning [Website] IJCAI 2021 Tutorial on Theoretically Unifying Conceptual Explanation and Generalization of DNNs [Website][Video] IJCAI 2020 Tutorial on Trustworthiness of Interpretable Machine Learning [Website] [Video] PRCV 2020 Tutorial on Robust and Explainable Artificial Intelligence [Website] ICML 2020 Online Panel Discussion: “Baidu AutoDL: Automated and Interpretable Deep Learning” A few selected studies 1. This paper (ICLR Oral paper receiving the top-5 score among all oral papers) proves a counterintuitive representation bottleneck shared by all DNNs, i.e., proving which types of interactions (concepts) are easy/hard for a DNN to learn. This paper proves that a DNN is more likely to encode both too simple interactions and too complex interactions, but is less likely to learn interactions of intermediate complexity. 2. This paper (Neurips 2021) provides a unified view to explain different adversarial attacks and defense methods, i.e., the view of multi-order interactions between input variables of DNNs. Based on the multi-order interaction, we discover that adversarial attacks mainly affect high-order interactions to fool the DNN. Furthermore, we find that the robustness of adversarially trained DNNs comes from category-specific low-order interactions. Our findings provide a potential method to unify adversarial perturbations and robustness, which can explain the existing defense methods in a principle way. 3. Interpretable Convolutional Neural Networks. We add additional losses to force each convolutional filter in our interpretable CNN to represent a specific object part. In comparisons, a filter in ordinary CNNs usually represents a mixture of parts and textures. We learn the interpretable CNN without any part annotations for supervision. Clear semantic meanings of middle-layer filters are of significant values in real applications. Activation regions of two convolutional filters in the interpretable CNN through different frames. 4. Explanatory Graphs for CNNs. We transform traditional CNN representations to interpretable graph representations, i.e., explanatory graphs, in an unsupervised manner. Given a pre-trained CNN, we disentangle feature representations of each convolutional filter into a number of object parts. We use graph nodes to represent the disentangled part components and use graph edges to encode the spatial relationship and co-activation relationship between nodes of different conv-layers. In this way, the explanatory graph encodes the potential knowledge hierarchy hidden inside middle layers of the CNN. 5. This paper (Neurips 2021) proposes a method to visualize the discrimination power of intermediate-layer visual patterns encoded by a DNN. Specifically, we visualize (1) how the DNN gradually learns regional visual patterns in each intermediate layer during the training process, and (2) the effects of the DNN using non-discriminative patterns in low layers to construct disciminative patterns in middle/high layers through the forward propagation. Based on our visualization method, we can quantify knowledge points (i.e., the number of discriminative visual patterns) learned by the DNN to evaluate the representation capacity of the DNN. Professional Activities Transactions on Machine Learning Research 担任Action Editor ACM China SigAI 2024 副秘书长 ACM TURC 2023 SIGAI China Symposium 程序委员会主席 Workshop Co-chair: VALSE Workshop on Interpretable Visual Models, 2021 (http://valser.org/2021/#/workshopde?id=8) ICML Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI, 2021 (https://icml2021-xai.github.io/) CVPR Workshop on Explainable AI, 2019 AAAI Workshop on Network Interpretability for Deep Learning, 2019 (http://networkinterpretability.org) CVPR Workshop on Language and Vision, 2018 (http://languageandvision.com/) CVPR Workshop on Language and Vision, 2017 (http://languageandvision.com/2017.html) Journal Reviewer: Nature Machine Intelligence, Nature Communications, IEEE Transactions on Pattern Analysis and Machine Intelligence, International Journal of Computer Vision, Journal of Machine Learning Research, IEEE Transactions on Knowledge and Data Engineering, IEEE Transactions on Multimedia, IEEE Transactions on Signal Processing, IEEE Signal Processing Letters, IEEE Robotics and Automation Letters, Neurocomputing News 2022-08-22 Keynote @ VALSE 2022 http://valser.org/2022/ 2022-08-15 Invited talk @IJTCS 2022-06-17 give an invited talk @ CVPR 2022 Workshop on The Art of Robustness: Devil and Angel in Adversarial Machine Learning (https://artofrobust.github.io/) 2022-04-22 give a talk @ 北洋智算论坛 2022-03-17 give a talk @ 青源talk 2022-01-12 panal discussion @ VALSE Webinar on SNN 2021-11-04 give a talk @ Fudan University 2021-10-23 give a tutorial @ ADL 121, 安全可信人工智能 2021-10-19 give a talk @ AI安全与隐私论坛 2021-10-16 give a talk @ Huawei Strategy and Technology Workshop. 2021-10-14 give a talk @ 第三届自动驾驶网络技术论坛，华为。 2021-10-09 give a tutorial @ VALSE 2021 Tutorial on Interpretable Machine Learning。 2021-08-15 give a talk on AI ethics and Governance @ Future Forum 2021-08-03 give a talk @ Huawei Inc. Songshan Lake 2021-07-31 give a talk @ ACM TURC 2021 2021-07-31 win ACM China Rising Star Award @ ACM TURC 2021 2021-07-24 give a online talk @ 2nd Trustworthy AI Seminar, Chinese Academy of Sciences 2021-07-15 give a talk @ Alibaba 2021-07-09 panel discussion on Trustworthy AI @ World Artificial Intelligence Conference (WAIC) 2021-05-31 give a talk @ Seminar for AI Robot Technology, Northwestern Polytechnical University 2021-01-12 give a talk @ Huawei Inc. Nanjing 2020-12-03 give a talk @ 1st Workshop on Safe and Trustworthy AI, Huawei Inc. Beijing 2020/10/19 Give an invited talk at IEEE XAI Seminar 2020/10/17 Give an invited talk at CCHI 2020 2020/09/26 Give an invited talk at CCF yocsef 2020/07/12 Online Panel Discussion: “Baidu AutoDL: Automated and Interpretable Deep Learning” in ICML 2020 2020/06/20 Give an invited talk at Machine Learning MINI Webinar 2020/04/25 Give a tutorial at 中国工程院院刊在线论坛 2020/01/09 Give an invited talk at the South China University of Technology 2020/01/07 Give an invited talk at the Forum of Turing Centers 2019/12/12 Give an invited talk at the Westlake University 2019/11/08 Give an tutorial at PRCV2019 2019/11/08 Give an tutorial at PRCV2019 2019/10/25 Give an invited talk at the CCF-CV Lecture @ China University Of Petroleum 2019/10/20 Give an invited talk at the Beijing University of Technology. 2019/09/27 Give an invited talk at RACV2019. 2019/06/16 Give an invited talk at UCLA Health. 2019/05/16 Give an invited talk at the GAMES Webinar. 2019/01/28 Give an invited talk “Feature Interpretability and Structural Interpretability of Deep Models” at AAAI-19 Workshop on Network Interpretability for Deep Learning. 2019/01/16 Give an invited talk at the VALSE Webinar. 2019/01/12 Chair of the Workshop on Interpretable AI @SJTU John Hopcroft Center. Give an invited talk. 2019/01/10 Give an invited talk at the 3rd workshop on Brain, Cognition, and Computing. 2018/12/29 Give an invited talk at the Forum of Turing Centers. http://jhc.sjtu.edu.cn/forum 2018/12/15 Organize the CVPR Workshop on Explainable AI, 2019. https://explainai.net/ 2018/11/21 Give an invited talk at the Department of Electronic Engineering, Tsinghua University. 2018/10/11 Give an invited talk at the Department of Computer Science and Technology, Tsinghua University. 2018/10/09 Give an invited talk at the Institute of Computing Technology, Chinese Academy of Sciences 2018/08/24 Give an invited talk at the Tongji University. 2018/08/10 Organize the AAAI-19 Workshop on Network Interpretability for Deep Learning, 2019. http://networkinterpretability.org 2018/07/28 Give an invited talk at the Seminar on Frontier of Deep Learning Theory, SJTU. 2018/07/25 Give a talk about the interpretability of neural networks at Microsoft Research Asia. Employed as an Associate Professor at the Shanghai Jiao Tong University. Organize the Language-and-Vision Workshop at CVPR 2018. http://languageandvision.com The code for “interpretable convolutional neural network” at CVPR 2018 is released. https://github.com/zqs1022/interpretableCNN/ A spotlight paper is accepted by CVPR 2018. The code for “Interpreting CNN Knowledge via an Explanatory Graph” at AAAI 2018 is released. https://github.com/zqs1022/explanatoryGraph Two papers are accepted by AAAI 2018. Organize the Language-and-Vision Workshop at CVPR 2017. http://languageandvis",
  "content_length": 10085,
  "method": "requests",
  "crawl_time": "2025-12-01 14:13:50"
}