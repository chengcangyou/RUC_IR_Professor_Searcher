{
  "name": "Adam Dziedzic",
  "homepage": "https://adam-dziedzic.com",
  "status": "success",
  "content": "Adam Dziedzic Adam Dziedzic I am a Tenure Track Faculty Member at CISPA, where I co-lead the SprintML group with a research focus on Secure, Private, Robust, INterpretable, and Trustworthy Machine Learning. We design robust and reliable machine learning methods for training and inference of ML models while preserving data privacy and model confidentiality. Befor joining CISPA, I was a Postdoctoral Fellow at the Vector Institute and the University of Toronto, a member of the CleverHans Lab, advised by Prof. Nicolas Papernot. I earned my PhD in computer science at the University of Chicago, where I was advised by Prof. Sanjay Krishnan and worked on input and model compression for adaptive and robust neural networks. I obtained my Bachelor's and Master's degrees in computer science from Warsaw University of Technology in Poland. I was also studying at DTU (Technical University of Denmark) and carried out research at EPFL, Switzerland. I also worked at CERN (Geneva, Switzerland), Barclays Investment Bank in London (UK), Microsoft Research (Redmond, USA) and Google (Madison, USA). Hiring: we are searching for ambitious students who would like to work with us in our SprintML group at CISPA. Please, feel free to email me if you are interested in this opportunity.Email: adam.dziedzic@sprintml.com (my public PGP key)Address: CISPA Helmholtz Center for Information Security, Stuhlsatzenhaus 5, 66123 Saarbrücken, Germany Selected Publications Please also check the Full List of my publications and the Google Scholar profile. BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models Louis Kerner, Michel Meintz, Bihe Zhao, Franziska Boenisch, Adam Dziedzic In The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS) 2025 Paper TL;DR Abstract BibTeX We introduce a bitwise, radioactive watermark for the Infinity image generative model and show that it remains robust against various attacks while preserving high image quality and generation speed. State-of-the-art text-to-image models like Infinity generate photorealistic images at an unprecedented speed. These models operate in a bitwise autoregressive manner over a discrete set of tokens that is practically infinite in size. However, their impressive generative power comes with a growing risk: as their outputs increasingly populate the Internet, they are likely to be scraped and reused as training data-potentially by the very same models. This phenomenon has been shown to lead to model collapse, where repeated training on generated content, especially from the models’ own previous versions, causes a gradual degradation in performance. A promising mitigation strategy is watermarking, which embeds human-imperceptible yet detectable signals into generated images-enabling the identification of generated content. In this work, we introduce BitMark, a robust bitwise watermarking framework for Infinity. Our method embeds a watermark directly at the bit level of the token stream across multiple scales (also referred to as resolutions) during Infinity’s image generation process. Our bitwise watermark subtly influences the bits to preserve visual fidelity and generation speed while remaining robust against a spectrum of removal techniques. Furthermore, it exhibits high radioactivity, i.e., when watermarked generated images are used to train another image generative model, this second model’s outputs will also carry the watermark. The radioactive traces remain detectable even when only fine-tuning diffusion or image autoregressive models on images watermarked with our BitMark. Overall, our approach provides a principled step toward preventing model collapse in image generative models by enabling reliable detection of generated outputs. @inproceedings{kerner2025BitMark, title = {BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models}, author = {Kerner, Louis and Meintz, Michel and Zhao, Bihe and Boenisch, Franziska and Dziedzic, Adam}, year = {2025}, booktitle = {The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS)} } Memorization in Graph Neural Networks Adarsh Jamadandi, Jing Xu, Adam Dziedzic, Franziska Boenisch In The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS) 2025 Paper TL;DR Abstract BibTeX We propose the first label memorization framework for node classification in GNNs and investigate the relationship between memorization and graph/node properties. Deep neural networks (DNNs) have been shown to memorize their training data, yet similar analyses for graph neural networks (GNNs) remain largely under-explored. We introduce NCMemo (Node Classification Memorization), the first framework to quantify label memorization in semi-supervised node classification. We first establish an inverse relationship between memorization and graph homophily, i.e the property that connected nodes share similar labels/features. We find that lower homophily significantly increases memorization, indicating that GNNs rely on memorization to learn less homophilic graphs. Secondly, we analyze GNN training dynamics. We find that the increased memorization in low homophily graphs is tightly coupled to the GNNs’ implicit bias on using graph structure during learning. In low homophily regimes, this structure is less informative, hence inducing memorization of the node labels to minimize training loss. Finally, we show that nodes with higher label inconsistency in their feature-space neighborhood are significantly more prone to memorization. Building on our insights into the link between graph homophily and memorization, we investigate graph rewiring as a means to mitigate memorization. Our results demonstrate that this approach effectively reduces memorization without compromising model performance. Moreover, we show that it lowers the privacy risk for previously memorized data points in practice. Thus, our work not only advances understanding of GNN learning but also supports more privacy-preserving GNN deployment. @inproceedings{Jamadandi2025memorizationGNNs, title = {Memorization in Graph Neural Networks}, author = {Jamadandi, Adarsh and Xu, Jing and Dziedzic, Adam and Boenisch, Franziska}, year = {2025}, booktitle = {The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS)} } Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models Jamie Hayes, Adam Dziedzic, A. Feder Cooper, Christopher A. Choquette-Choo, Franziska Boenisch, Georgios Kaissis, Igor Shilov, Ilia Shumailov, Katherine Lee, Matthew Jagielski, Matthieu Meeus, Meenatchi Sundaram Muthu Selva Annamalai, Niloofar Mireshghallah, Yves-Alexandre Montjoye, Milad Nasr In The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS) 2025 Paper TL;DR Abstract BibTeX We scale strong membership inference attacks to large pre-trained language models and find that, while such attacks can succeed, their effectiveness remains limited in practical settings and their relationship to standard privacy metrics is more complex than previously thought. State-of-the-art membership inference attacks (MIAs) typically require training many reference models, making it difficult to scale these attacks to large pre-trained language models (LLMs). As a result, prior research has either relied on weaker attacks that avoid training reference models (e.g., fine-tuning attacks), or on stronger attacks applied to small-scale models and datasets. However, weaker attacks have been shown to be brittle - achieving close-to-arbitrary success - and insights from strong attacks in simplified settings do not translate to today’s LLMs. These challenges have prompted an important question: are the limitations observed in prior work due to attack design choices, or are MIAs fundamentally ineffective on LLMs? We address this question by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures ranging from 10M to 1B parameters, training reference models on over 20B tokens from the C4 dataset. Our results advance the understanding of MIAs on LLMs in three key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their effectiveness, however, remains limited (e.g., AUC<0.7) in practical settings; and, (3) the relationship between MIA success and related privacy metrics is not as straightforward as prior work has suggested. @inproceedings{hayes2025strongMIALLMt, title = {Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models}, author = {Hayes, Jamie and Dziedzic, Adam and Cooper, A. Feder and Choquette-Choo, Christopher A. and Boenisch, Franziska and Kaissis, Georgios and Shilov, Igor and Shumailov, Ilia and Lee, Katherine and Jagielski, Matthew and Meeus, Matthieu and Annamalai, Meenatchi Sundaram Muthu Selva and Mireshghallah, Niloofar and de Montjoye, Yves-Alexandre and Nasr, Milad}, booktitle = {The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS)}, year = {2025} } Privacy Attacks on Image AutoRegressive Models Antoni Kowalczuk, Jan Dubiński, Franziska Boenisch, Adam Dziedzic In Forty-Second International Conference on Machine Learning (ICML) 2025 Paper TL;DR Abstract BibTeX Code We design new methods to assess the privacy leakage from the image autoregressive models and show that they provide better performance, however, also leak more private information than diffusion models. Image AutoRegressive generation has emerged as a new powerful paradigm with image autoregressive models (IARs) matching state-of-the-art diffusion models (DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for a higher generation speed. However, the privacy risks associated with IARs remain unexplored, raising concerns regarding their responsible deployment. To address this gap, we conduct a comprehensive privacy analysis of IARs, comparing their privacy risks to the ones of DMs as reference",
  "content_length": 91686,
  "method": "requests",
  "crawl_time": "2025-12-01 12:29:00"
}