{
  "name": "Jon Barker",
  "homepage": "https://www.sheffield.ac.uk/dcs/people/academic/jon-barker",
  "status": "success",
  "content": "Professor Jon Barker | Computer Science | The University of Sheffield Skip to main content Search sheffield.ac.uk Close menu × School of Computer Science School of Computer Science Menu Professor Jon Barker PhD School of Computer Science Personal Chair School Ethics Lead Member of the Speech and Hearing (SpandH) research group Open staff member portrait in a modal window j.p.barker@sheffield.ac.uk Regent Court (DCS) Full contact details Professor Jon Barker School of Computer Science Regent Court (DCS) 211 Portobello Sheffield S1 4DP Orcid ID:0000-0002-1684-5660 Google Scholar Personal website Profile Professor Jon Barker is a member of the Speech and Hearing Research Group. He has a first degree in Electrical and Information Sciences from Cambridge University, UK. After receiving a PhD from the University of Sheffield in 1999, he worked for some time at GIPSA-lab, Grenoble and IDIAP research institute in Switzerland before returning to Sheffield where he has had a permanent post since 2002.His research interests lie in noise-robust speech processing. Key application areas include distant-microphone speech recognition, speech intelligibility prediction and improved speech processing for hearing-aid users. Research interests Professor Barker’s research interests are focused around machine listening and the computational modelling of human hearing. A recent focus has been on modelling speech intelligibility, ie can we predict whether or not a speech signal will be intelligible to a given listener?This understanding will help us produce better signal processing for application such as hearing aids and cochlear implants. Another strand of his work is about taking insights gained from human auditory perception and using them to engineer robust automatic speech processing systems. Publications Journal articles Yue Z, Loweimi E, Cvetkovic Z, Barker J & Christensen H (2026) Raw acoustic-articulatory multimodal dysarthric speech recognition. Computer Speech & Language, 95, 101839-101839. Roa-Dabike G, Akeroyd MA, Bannister S, Barker JP, Cox TJ, Fazenda B, Firth J, Graetzer S, Greasley A, Vos RR & Whitmer WM (2025) The First Cadenza Challenges: Using Machine Learning Competitions to Improve Music for Listeners With a Hearing Loss. IEEE Open Journal of Signal Processing, 6, 722-734. Roa G, Bannister S, Firth JL, Graetzer S, Vos R, Akeroyd MA, Barker JP, Cox TJ, Fazenda B, Greasley A & Whitmer WM (2025) The second Cadenza machine learning challenge (CAD2): Improving music for people with hearing loss. The Journal of the Acoustical Society of America, 157(4_Supplement), A321-A321. Leglaive S, Fraticelli M, ElGhazaly H, Borne L, Sadeghi M, Wisdom S, Pariente M, Hershey JR, Pressnitzer D & Barker JP (2025) Objective and subjective evaluation of speech enhancement methods in the UDASE task of the 7th CHiME challenge. Computer Speech & Language, 89, 101685-101685. Roa Dabike G, Cox TJ, Miller AJ, Fazenda BM, Graetzer S, Vos RR, Akeroyd MA, Firth J, Whitmer WM, Bannister S , Greasley A et al (2024) The cadenza woodwind dataset: Synthesised quartets for music information retrieval and machine learning. Data in Brief, 57, 111199-111199. Whitmer WM, McShefferty D, Akeroyd MA, Bannister S, Barker JP, Cox TJ, Roa G, Fazenda B, Firth JL, Graetzer S , Greasley A et al (2024) Lyric intelligibility of musical segments for older individuals with hearing loss. The Journal of the Acoustical Society of America, 156(4_Supplement), A121-A121. Akeroyd MA, Bannister S, Barker JP, Cox TJ, Roa G, Fazenda B, Firth JL, Graetzer S, Greasley A, Vos R & Whitmer WM (2024) Development of the 2nd Cadenza challenge for improving music listening for people with a hearing loss. The Journal of the Acoustical Society of America, 155(3_Supplement), A277-A277. Bannister S, Greasley AE, Cox TJ, Akeroyd MA, Barker J, Fazenda B, Firth J, Graetzer SN, Roa Dabike G, Vos RR & Whitmer WM (2024) Muddy, muddled, or muffled? Understanding the perception of audio quality in music by hearing aid users. Frontiers in Psychology, 15. View this article in WRRO Firth JL, Cox TJ, Greasley A, Barker JP, Whitmer WM, Fazenda B, Bannister S, Graetzer S, Vos R, Roa G & Akeroyd MA (2023) A systematic review of measurements of real-world interior car noise for the “Cadenza” machine-learning project. The Journal of the Acoustical Society of America, 153(3_supplement), A332-A332. Akeroyd MA, Firth JL, Naylor G, Barker JP, Culling J, Cox TJ, Bailey W, Graetzer S, Viveros Muñoz R, Porter E & Griffiths H (2023) Results of the second “clarity” enhancement challenge for hearing devices. The Journal of the Acoustical Society of America, 153(3_supplement), A48-A48. Graetzer S, Akeroyd MA, Barker J, Cox TJ, Culling JF, Naylor G, Porter E & Viveros-Muñoz R (2022) Dataset of British English speech recordings for psychoacoustics and speech processing research: The clarity speech corpus. Data in Brief, 41. Akeroyd MA, Barker JP, Cox TJ, Culling J, Graetzer S, Naylor G, Porter E & Viveros Muñoz R (2020) Launching the first “Clarity” Machine Learning Challenge to revolutionise hearing device processing. The Journal of the Acoustical Society of America, 148(4_Supplement), 2711-2711. Graetzer S, Akeroyd M, Barker JP, Cox TJ, Culling JF, Naylor G, Porter E & Muñoz RV (2020) Clarity: Machine Learning Challenges to Revolutionise Hearing Device Processing. Cooke M, Garcia Lecumberri ML, Barker J & Marxer R (2019) Lexical frequency effects in English and Spanish word misperceptions. Journal of the Acoustical Society of America, 145(2), EL136-EL141. View this article in WRRO Alghamdi N, Maddock S, Marxer R, Barker J & Brown GJ (2018) A corpus of audio-visual Lombard speech with frontal and profile views. Journal of the Acoustical Society of America, 143(6), 523-529. View this article in WRRO Marxer R, Barker JP, Alghamdi N & Maddock S (2018) The impact of the Lombard effect on audio and visual speech recognition systems. Speech Communication, 100, 58-68. View this article in WRRO Alghamdi N, Maddock S, Barker J & Brown GJ (2017) The impact of automatic exaggeration of the visual articulatory features of a talker on the intelligibility of spectrally distorted speech. Speech Communication, 95, 127-136. View this article in WRRO Vincent E, Watanabe S, Nugraha AA, Barker J & Marxer R (2017) An analysis of environment, microphone and data simulation mismatches in robust speech recognition. Computer Speech & Language, 46, 535-557. View this article in WRRO Barker JP (2017) Evaluation of scene analysis using real and simulated acoustic mixtures: Lessons learnt from the CHiME speech recognition challenges. The Journal of the Acoustical Society of America, 141(5_Supplement), 3693-3693. Barker J, Marxer R, Vincent E & Watanabe S (2017) Guest Editorial for the special issue on Multi-Microphone Speech Recognition in Everyday Environments. Computer Speech & Language, 46, 386-387. View this article in WRRO Gonzalez JA, Gómez AM, Peinado AM, Ma N & Barker J (2017) Spectral Reconstruction and Noise Model Estimation Based on a Masking Model for Noise Robust Speech Recognition. Circuits, Systems, and Signal Processing, 36, 3731-3760. View this article in WRRO Barker J, Marxer R, Vincent E & Watanabe S (2016) The third 'CHiME' speech separation and recognition challenge: Analysis and outcomes. Computer Speech and Language. View this article in WRRO Marxer R, Barker J, Cooke M & Garcia Lecumberri ML (2016) A corpus of noise-induced word misperceptions for English. Journal of the Acoustical Society of America, 140(5), EL458-EL463. Vincent E, Barker J, Watanabe S, Le Roux J, Nesta F & Matassoni M (2013) The second 'CHiME' speech separation and recognition challenge: An overview of challenge systems and outcomes. 2013 IEEE Workshop on Automatic Speech Recognition and Understanding Asru 2013 Proceedings, 162-167. Carmona JL, Barker J, Gomez AM & Ma N (2013) Speech spectral envelope enhancement by HMM-based analysis/resynthesis. IEEE Signal Processing Letters, 20(6), 563-566. Cooke M, Barker J & Lecumber MLG (2013) An Overview, 137-172. González JA, Peinado AM, Ma N, Gómez AM & Barker J (2013) MMSE-based missing-feature reconstruction with temporal modeling for robust speech recognition. IEEE Transactions on Audio Speech and Language Processing, 21(3), 624-635. Barker J & Vincent E (2012) Special issue on speech separation and recognition in multisource environments. Computer Speech and Language. Barker J, Vincent E, Ma N, Christensen H & Green P (2012) The PASCAL CHiME speech separation and recognition challenge. Computer Speech and Language. Ma N, Barker J, Christensen H & Green P (2012) A hearing-inspired approach for distant-microphone speech recognition in the presence of multiple sources. Computer Speech and Language. Ma N, Barker J, Christensen H & Green P (2012) Combining speech fragment decoding and adaptive noise floor modelling.. IEEE Transactions on Audio, Speech and Language Processing, 20, 818-827. Cooke M, Barker J, Lecumberri MLG & Wasilewski K (2011) Crowdsourcing for word recognition in noise. Proceedings of the Annual Conference of the International Speech Communication Association Interspeech, 3049-3052. Barker J, Ma N, Coy A & Cooke M (2010) Speech fragment decoding techniques for simultaneous speaker identification and speech recognition. COMPUT SPEECH LANG, 24(1), 94-111. Barker J & Shao X (2009) Energetic and Informational Masking Effects in an Audiovisual Speech Recognition System. IEEE T AUDIO SPEECH, 17(3), 446-458. Christensen H, Ma N, Wrigley SN & Barker J (2008) Improving source localisation in multi-source, reverberant conditions: exploiting local spectro-temporal location cues.. Abstract for Acoust. Soc. Am. mtg. Shao X & Barker J (2008) Stream weight estimation for multistream audio-visual speech recognition in a multispeaker environment. SPEECH COMMUN, 50(4), 337-353. Cooke M, Garcia Lecumberri ML & Barker J (2008) The foreign language cocktail party problem: Energetic and informational masking effe",
  "content_length": 43190,
  "method": "requests",
  "crawl_time": "2025-12-01 13:33:25"
}