{
  "name": "Anh Nguyen 0002",
  "homepage": "https://anhnguyen.me",
  "status": "success",
  "content": "Artificial Intelligence Research | Anh Totti Nguyen Skip to content I'm interested in (1) understanding the weaknesses, behaviors, and inner-workings of AIs; and (2) making these systems more trustworthy and explainable. Below are selected projects in Vision, NLP, Explainable AI, and HCI where I'm a lead contributor (the first or senior author). Publication list on G Scholar. Computer Vision Vision Language Models are Biased Large language models (LLMs) memorize a vast amount of prior knowledge from the Internet that help them on downstream tasks... 2025 | | Computer Vision Explainable AI TAB: Transformer Attention Bottlenecks enable User Intervention and Debugging in Vision-Language Models Multi-head self-attention (MHSA) is a key component of Transformers, a widely popular architecture in both language and vision. Multiple heads... 2025\t\t\t\t\t\t\tICCV | | Explainable AI Human-Computer Interaction NLP Interpretable LLM-based Table Question Answering Interpretability for Table Question Answering (Table QA) is critical, particularly in high-stakes industries like finance or healthcare. Although recent approaches... 2025\t\t\t\t\t\t\tTMLR | | Computer Vision Improving zero-shot object-level change detection by incorporating visual correspondence Detecting object-level changes between two images across possibly different views is a core task in many applications that involve visual... 2025\t\t\t\t\t\t\tWACV | | Explainable AI Human-Computer Interaction NLP HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate nonfactual statements. A response mixed of factual... 2025 | | Computer Vision Understanding Generative AI Capabilities in Everyday Image Editing Tasks Generative AI (GenAI) holds significant promise for automating everyday image editing tasks, especially following the recent release of GPT-4o on... 2025 | | NLP B-score: Detecting biases in large language models using response history Large language models (LLMs) often exhibit strong biases, e.g, against women or in favor of the number 7. We investigate... 2025\t\t\t\t\t\t\tICML | | Computer Vision Explainable AI Fast and Interpretable Face Identification for Out-Of-Distribution Data Using Vision Transformers Most face identification approaches employ a Siamese neural network to compare two images at the image embedding level. Yet, this... 2024\t\t\t\t\t\t\tWACV | | Computer Vision Explainable AI PCNN: Probable-Class Nearest-Neighbor Explanations Improve Fine-Grained Image Classification Accuracy for AIs and Humans Nearest neighbors (NN) are traditionally used to compute final decisions, e.g., in Support Vector Machines or k-NN classifiers, and to... 2024\t\t\t\t\t\t\tTMLR | | Computer Vision Vision Language Models Are Blind While large language models with vision capabilities (VLMs), e.g., GPT-4o and Gemini-1.5 Pro, are powering various image-text applications and scoring... 2024\t\t\t\t\t\t\tACCV | | Computer Vision Explainable AI NLP PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck * Equal contribution. CLIP-based classifiers rely on the prompt containing a {class name} that is known to the text encoder.... 2024\t\t\t\t\t\t\tNAACL | | Computer Vision GlitchBench: Can large multimodal models detect video game glitches? Large multimodal models (LMMs) have evolved from large language models (LLMs) to integrate multiple input modalities, such as visual inputs.... 2024\t\t\t\t\t\t\tCVPR | | NLP PiC: A Phrase-in-Context Dataset for Phrase Understanding and Semantic Search Since BERT (Devlin et al., 2018), learning contextualized word embeddings has been a de-facto standard in NLP. However, the progress... 2023\t\t\t\t\t\t\tEACL | | Computer Vision ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of Zoom and Spatial Biases in Image Classification Image classifiers are information-discarding machines, by design. Yet, how these models discard information remains mysterious. We hypothesize that one way... 2023\t\t\t\t\t\t\tNeurIPS | Computer Vision Explainable AI DeepFace-EMD: Re-ranking Using Patch-wise Earth Mover’s Distance Improves Out-Of-Distribution Face Identification Face identification (FI) is ubiquitous and drives many high-stake decisions made by law enforcement. State-of-the-art FI approaches compare two images... 2022\t\t\t\t\t\t\tCVPR | | Explainable AI NLP Double Trouble: How to not explain a text classifier’s decisions using counterfactuals synthesized by masked language models Explaining how important each input feature is to a classifier’s decision is critical in high-stake applications. An underlying principle behind... 2022\t\t\t\t\t\t\tAACL | | Computer Vision Explainable AI gScoreCAM: What objects is CLIP looking at? Large-scale, multimodal models trained on web data such as OpenAI’s CLIP are becoming the foundation of many applications. Yet, they... 2022\t\t\t\t\t\t\tACCV | | Computer Vision Explainable AI Visual correspondence-based explanations improve AI robustness and human-AI team accuracy Explaining artificial intelligence (AI) predictions is increasingly important and even imperative in many high-stakes applications where humans are the ultimate... 2022\t\t\t\t\t\t\tNeurIPS | | Computer Vision Explainable AI How explainable are adversarially-robust CNNs? Three important criteria of existing convolutional neural networks (CNNs) are (1) test-set accuracy; (2) out-of-distribution accuracy; and (3) explainability. While... 2022 | Computer Vision Inverting Adversarially Robust Networks for Image Synthesis Recent research in adversarially robust classifiers suggests their representations tend to be aligned with human perception, which makes them attractive... 2021\t\t\t\t\t\t\tACCV | | Computer Vision Explainable AI The effectiveness of feature attribution methods and its correlation with automatic evaluation scores Explaining the decisions of an Artificial Intelligence (AI) model is increasingly critical in many real-world, high-stake applications. Hundreds of papers... 2021\t\t\t\t\t\t\tNeurIPS | | Uncategorized baller2vec: A Multi-Entity Transformer For Multi-Agent Spatiotemporal Modeling Multi-agent spatiotemporal modeling is a challenging task from both an algorithmic design and computational complexity perspective. Recent work has explored... 2021 | | NLP Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks? Do state-of-the-art natural language understanding models care about word order – one of the most important characteristics of a sequence?... 2021\t\t\t\t\t\t\tACL Findings | Computer Vision Explainable AI Human-Computer Interaction Explaining image classifiers by removing input features using generative models Interpretability methods often measure the contribution of an input feature to an image classifier’s decisions by heuristically removing it via... 2020\t\t\t\t\t\t\tACCV | | Computer Vision Explainable AI The shape and simplicity biases of adversarially robust ImageNet-trained CNNs * All authors contributed equally. Adversarial training has been the topic of dozens of studies and a leading method for... 2020 | Computer Vision Explainable AI SAM: The Sensitivity of Attribution Methods to Hyperparameters * Equal contributions. Attribution methods can provide powerful insights into the reasons for a classifier’s decision. We argue that a... 2020\t\t\t\t\t\t\tCVPR | | Computer Vision Strike (with) a Pose: Neural networks are easily fooled by strange poses of familiar objects Despite excellent performance on stationary test sets, deep neural networks (DNNs) can fail to generalize to out-of-distribution (OoD) inputs, including... 2019\t\t\t\t\t\t\tCVPR | | Computer Vision A cost-effective method for improving and re-purposing large, pre-trained GANs by fine-tuning their class-embeddings Large, pre-trained generative models have been increasingly popular and useful to both the research and wider communities. Specifically, BigGANs a... 2019\t\t\t\t\t\t\tACCV | | Computer Vision Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning Motion-sensor cameras in natural habitats offer the opportunity to inexpensively and unobtrusively gather vast amounts of data on animals in... 2018\t\t\t\t\t\t\tPNAS | | Computer Vision VectorDefense: Vectorization as a Defense to Adversarial Examples Training deep neural networks on images represented as grids of pixels has brought to light an interesting phenomenon known as... 2018 | | NewsOh wow, this VLM benchmark is pure evil, and I love it!\"Vision Language Models are Biased\" by @an_vo12, @taesiri, @anh_ng8, etal.Also really good idea to have one-click copy-paste of images and prompts, makes trying it super easy. pic.twitter.com/cxlJ7VoEBp— Lucas Beyer (bl16) (@giffmana) August 8, 2025 Our research in the news 2024: Our findings on the blindspots of Vision Language Models are covered on ArsTechnica and TechCrunch . More articles ...",
  "content_length": 8933,
  "method": "requests",
  "crawl_time": "2025-12-01 12:58:39"
}