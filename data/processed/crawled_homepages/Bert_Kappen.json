{
  "name": "Bert Kappen",
  "homepage": "http://www.snn.ru.nl/~bertk",
  "status": "success",
  "content": "Bert Kappen Bert (HJ) Kappen Research interests Quantum machine learning Path integral control theory page. Publications A short biography Videolectures Google scholar citations Weekly SNN seminars Current group members Former group members Teaching Master Projects Recent meetings organized Organizations and collaborations Externally funded projects Applications Contact information Kappenball ITunes App Liefde en het brein Bert (HJ) Kappen is professor of physics at the Department of Biophysics, Radboud University, Nijmegen Address: Department of Neurophysics, Donders Center for Neuroscience, Heyendaalseweg 135, 6525 AJ Nijmegen, The Netherlands Email: b.kappen@science.ru.nl The physics of machine learning Keywords: Bayesian inference, learning and reasoning, stochastic control theory, neural networks, statistical physics, quantum machine learning My research focusess on the design of efficient computational methods AI and machine learning using ideas and methods from statistical physics and quantum physics. In addition, I am interested in how intelligence and consciousness may arise in the brain. Here, I give a high level overview of my research interests, both past and present. For details consult my Google scholar page. Bayesian Inference Due to the essential roles that noise and uncertainty play in perception and learning, a useful way to model intelligence is to use probability models. In the mid 90s, the fields of analog and digital computing as separate approaches to model intelligence, have begun to merge using the idea of Bayesian inference: One can generalize the logic of digital computation to a probabilistic calculus, embodied in a so-called graphical model. Similarly, one can generalize dynamical systems to stochastic dynamical systems that allow for a probabilistic description in terms of a Markov process. The Bayesian paradigm has greatly helped to integrate different schools of thought in particular in the field of artificial intelligence and machine learning but also provides a computational paradigm for neuroscience. A typical Bayesian computation, whether in the context of a complex data analysis problem or in a stochastic neural network, is to compute an expectation value, which is referred to as Bayesian inference. Bayesian inference is intractable, which means that computation time and memory use scale exponentially with the problem size. However, many methods exist to compute these quantities approximately. Most of these methods origin from statistical physics, such as the mean field method, belief propagation or Monte Carlo sampling. Application of these methods to machine learning problems is challenging and an active field of research to which I have made several contributions. Control theory Control theory is a theory from engineering that gives a formal description of how a system, such as a robot or animal, can move from a current state to a future state at minimal cost, where cost can mean time spent, or energy spent or any other quantity. Control theory is used traditionally to control industrial plants, airplanes or missiles, but is also the natural framework to model intelligent behavior in animals or robots. The mathematical formulation of deterministic control theory is very similar to classical mechanics. In fact, classical mechanics can be viewed as a special case of control theory. Stochastic control theory uses the language of stochastic differential equations. For a certain class of stochastic control problems, the solution is described by a linear partial differential equation that can be solved formally as a path integral. This so-called path integral control method provides a deep link between control, inference and statistical physics. This statistical physics view of control theory shows that qualitative different control solutions exist for different noise levels separated by phase transitions. The control solutions can be computed using efficient approximate inference methods such as Monte Carlo sampling or deterministic approximation methods. The path integral control theory is successfully being used by leading research groups in robotics world wide. For more information see the path integral control theory page. Quantum Machine Learning Current successes in machine learning has ignited interesting new connections between machine learning and quantum physics, loosely referred to as quantum machine learning. Machine learning methods are finding useful applications in quantum physics, such as characterizing the ground state of a quantum Hamiltonian or to learn different phases of matter. Since 2018, I am interested in how the quantum formalism can be used to advance machine learning. Recent work: Quantum Boltzmann Machine One line of work is the quantum Boltzmann machine, which is a method to learn a quantum Hamiltonian from classical or quantum data. The learning rule requires the computation of quantum spin expectation values which is intractable, as in the classical case article. In this paper we propose a new method to accellerate learning using a quantum circuit article. Adiabatic quantum computing Another line of work is to explore the possibility of quantum advantage using adiabatic quantum computing. In particular, we generalize the well-known quadratic speedup of Grover search to general optimization problems. We show that this is in principle possible, but that in practice this faces two serious obstacles. The speedup is achievable using an optimized annealing schedule that requires the exact value of annealing parameter at the phase transition. Computation of this number is intractable in general. Secondly, the value needs to be specified with with a numerical precision that increases exponentially with the problem size article. Computing with atoms Since 2020 we have an intense new collaboration with the scanning tunneling microscopy group of professor Alex Khajetoorians. In this collaboration, we have shown the possibility to realize a stochastic neural network at atomic scale. The spins in this network are bi-stable atoms that stochastically switch between two states (up and down). Each spin or neuron is characterized by the asymmetry (the probability to be in the up state minus down state) and mean residence time (the mean time between switches). Residence times of different spins can differ many orders of magnitude. We proposed that fast spins encode the firing or non-firing of neurons and slow spins encode binary learning elements, ie. synapses. In this way, a physical substrate can implement learning as the long term change of the slow variables. article. Identification of missing persons through DNA We built in 2010 a software system, called Bonaparte, for the identification of missing persons on the basis of their DNA. The method matches individuals DNA to pedigrees of relatives using a Bayesian network. The method is currently used by the NFI, the Australian police force on the entire continent, the Interpol I-Familia system and the identification of victims from the Spanish Civil War . See Bonaparte for further details. Medical diagnosis The idea to assist medical doctors to diagonse patients based on their symptoms is one of the oldest ideas of the use of artificial intelligence. However, up to today, building such systems with high accuracy has proven surprisingly difficult. In collaboration with the Erasmus MC in Rotterdam, we are building such an expert system for the diagnosis of internal medicine related diseases as they occur in the emergency department. The system is based on a Bayesian network that is specified on the basis of the knowledge of medical experts and textbooks. Teaching Inleiding Machine Learning BA NWI physics and math Statistical Machine Learning MA NWI computer science and FSS AI Advanced computational Neuroscience MA NWI physics and math and MA Donders research Machine Learning MA NWI physics and math and MA Donders research Advanced machine Learning MA NWI physics and math and MA Donders research Short course on control theory, ACNS Short course on machine learning, Pompeu Fabra spring 2003 Short course on control theory, Madrid fall 2010 Short course on control theory, UCL 2011 Short course on control theory, Madrid 2012 Tutorial ICTP Summer school Machine Learning, Trieste 2012 Short course on control theory, Madrid 2013 Tutorial ICTP Spring College on Physics of Complex systems, Trieste May-June 2013 Short course on control theory, Madrid 2014 Short course on control theory, CWI, Amsterdam 2025 Information, Physics, and Computation MA 2014 Introduction to Biophysics BA (discontinued) Neural networks and Information theory BA (discontinued, content moved to Inleiding Machine Learning BA from 2012-2013) Neurophysics BA (discontinued, content moved to Computational Neuroscience MA from 2013-2014) Neurophysics MA (discontinued) Computational Physics MA (discontinued) Master Projects Quantum machine learning There is an exciting possibility to use a quantum mechanical wave function to represent a probability distribution. While classically the probability distribution p(x) is computed for each x separately, the quantum physics computes all 'compoonents' of the wave function simulatenously and in parallel. This implies that the computation of statistics (means, correlations) of high dimensional distribution, which requires exponentially long computation times using classical machines, could be computed in constant time on a quantum device. My recent work focusses on learning such quantum systems. The learning step requires the estimation of the above statistics and is done classically using Monte Carlo sampling. The long term aim is to replace this step by a quantum computer. The use of the quantum formalism for learning also yields novel quantum statistics for purely classical data analysis. These statistics signal entanglement in classical data. The research focuses on 1) developing fast approximate inference method",
  "content_length": 18506,
  "method": "requests",
  "crawl_time": "2025-12-01 13:04:36"
}