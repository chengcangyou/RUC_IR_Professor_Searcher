{
  "name": "Libin Liu 0002",
  "homepage": "http://libliu.info",
  "status": "success",
  "content": "ï»¿ Libin Liu [About Me] Libin Liu | åå©æ Assistant Professor School of Intelligence Science and Technology Peking University Email: libin.liu [at] pku.edu.cn I am an assistant professor at the School of Intelligence Science and Technology, Peking University. Before joining Peking University, I was the Chief Scientist of DeepMotion Inc. I was a postdoctoral research fellow at Disney Research and the University of British Columbia. I received my Ph.D. in computer science and B.S. degree in mathematics and physics from Tsinghua University. I am interested in character animation, physics-based simulation, motion control, and related areas such as reinforcement learning, deep learning, and robotics. I put a lot of work into realizing various agile human motions on simulated characters and robots. [News] 2025/08/11: two papers accepted to ACM SIGGRAPH Asia 2025 2025/07/06: one paper accepted to ACM Multimedia 2025 2025/03/11: I am invited to give a talk on The 1st Workshop on Humanoid Agents at CVPR 2025 2024/07/30: one paper accepted to ACM SIGGRAPH Asia 2024 2024/04/30: two papers accepted to ACM SIGGRAPH 2024 2024/02/14: one paper accepted to Eurographics 2024 2024/01/27: I am now serving as an Associate Editor for IEEE Transactions on Visualization and Computer Graphics (TVCG) 2023/07/06: our paper GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents received SIGGRAPH 2023 Honorable Mention Papers [news] 2023/05/20: I am invited to give a talk on workshop Toward Natural Motion Generation at RSS 2023 2023/05/05: one paper accepted to SIGGRAPH 2023 2022/12/06: our paper Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings received SIGGRAPH Asia 2022 Best Paper Award [Projects] Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents Zeyi Zhang, Yanju Zhou, Heyuan Yao, Tenglong Ao, Xiaohang Zhan, Libin Liuâ Social Agent is a framework for generating realistic co-speech nonverbal behaviors in dyadic conversations. Combining an LLM-driven agentic system with a dual-person gesture generation model, it produces synchronized, context-aware gestures and adaptive interactions. User studies show improved naturalness, coordination, and responsiveness in conversational nonverbal behaviors. ACM SIGGRAPH Asia 2025 Conference Track. [Project Page] [Paper] SRBTrack: Terrain-Adaptive Tracking of a Single-Rigid-Body Character Using Momentum-Mapped Space-Time Optimization Hanyang Cao, Heyuan Yao, Libin Liu, Taesoo Kwon We introduce SRBTrack, a terrain-adaptive motion tracking framework for virtual characters. By combining a single-rigid-body controller with momentum-mapped space-time optimization, it enables robust, real-time, and physically plausible full-body motions. Trained on unstructured data, SRBTrack generalizes across unseen terrains, disturbances, and versatile motion styles without retraining. ACM SIGGRAPH Asia 2025 Conference Track. [Project Page] Learning Uniformly Distributed Embedding Clusters of Stylistic Skills for Physically Simulated Characters. Nian Liu, Zilong Zhang, Zi Wang, Tengyu Liu, Hongzhao Xie, Xinyi Tong, Libin Liu, Yaodong Yang, Zhaofeng He ACM Multimedia 2025. [Project Page] CPoser: An Optimization-after-Parsing Approach for Text-to-Pose Generation Using Large Language Models. Yumeng Li, Bohong Chen, Zhong Ren, Yao-Xiang Ding, Libin Liu, Tianjia Shao, Kun Zhou ACM Transactions on Graphics, Vol 43 Issue 6 (SIGGRAPH Asia 2024). [Project Page] MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete Representations Heyuan Yao, Zhenhua Song, Yuyang Zhou, Tenglong Ao, Baoquan Chen, Libin Liuâ We present MoConVQ, a uniform framework enabling simulated avatars to acquire diverse skills from large, unstructured datasets. Leveraging a rich and scalable discrete skill representation, MoConVQ supports a broad range of applications, including pose estimation, interactive control, text-to-motion generation, and, more interestingly, integrating motion generation with Large Language Models (LLMs). ACM Transactions on Graphics, Vol 43 Issue 4, Article 144 (SIGGRAPH 2024). [Project Page] [Paper] [Code] Semantic Gesticulator: Semantics-aware Co-speech Gesture Synthesis Zeyi Zhang*, Tenglong Ao*, Yuyao Zhang*, Qingzhe Gao, Chuan Lin, Baoquan Chen, Libin Liuâ We introduce Semantic Gesticulator, a novel framework designed to synthesize realistic co-speech gestures with strong semantic correspondence. Semantic Gesticulator fine-tunes an LLM to retrieve suitable semantic gesture candidates from a motion library. Combined with a novel, GPT-style generative model, the generated gesture motions demonstrate strong rhythmic coherence and semantic appropriateness. ACM Transactions on Graphics, Vol 43 Issue 4, Article 136 (SIGGRAPH 2024). [Project Page] [Paper] [Video (YouTube)] [Dataset] [Code] Cinematographic Camera Diffusion Model Hongda Jiang, Xi Wang, Marc Christie, Libin Liu, Baoquan Chen We present a cinematographic camera diffusion model using a transformer-based architecture to handle temporality and exploit the stochasticity of diffusion models to generate diverse and qualitative trajectories conditioned by high-level textual descriptions. Computer Graphics Forum, Vol 43 Issue 2 (Eurographics 2024). [Paper] [Code] MuscleVAE: Model-Based Controllers of Muscle-Actuated Characters Yusen Feng, Xiyan Xu, Libin Liuâ We present a novel framework for simulating and controlling muscle-actuated characters. This framework generates biologically plausible motion and accounts for fatigue effects using model-based generative controllers. ACM SIGGRAPH Asia 2023 Conference Track. [Project Page] [Paper] [Video (YouTube|BiliBili)] [Code] Neural Novel Actor: Learning a Generalized Animatable Neural Representation for Human Actors Yiming Wang*, Qingzhe Gao*, Libin Liuâ , Lingjie Liuâ , Christian Theobalt, Baoquan Chenâ (*: equal comtribution, â : corresponding author) We propose a new method for learning a generalized animatable neural human representation from a sparse set of multi-view imagery of multiple persons. IEEE Transactions on Visualization and Computer Graphics, 2023 [Project Page] [Paper] [Video] [Code] MotionBERT: A Unified Perspective on Learning Human Motion Representations Wentao Zhu, Xiaoxuan Ma, Zhaoyang Liu, Libin Liu, Wayne Wu, Yizhou Wangâ We present a unified perspective on tackling various human-centric video tasks by learning human motion representations from large-scale and heterogeneous data resources. ICCV 2023 [Project Page] [Paper] GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents Tenglong Ao, Zeyi Zhang, Libin Liuâ We introduce GestureDiffuCLIP, a CLIP-guided, co-speech gesture synthesis system that creates stylized gestures in harmony with speech semantics and rhythm using arbitrary style prompts. Our highly adaptable system supports style prompts in the form of short texts, motion sequences, or video clips and provides body part-specific style control. ACM Transactions on Graphics, Vol 42 Issue 4, Article 40 (SIGGRAPH 2023). (SIGGRAPH 2023 Honorable Mention Award [news]) [Project Page] [Paper] [Video (YouTube|BiliBili)] Control VAE: Model-Based Learning of Generative Controllers for Physics-Based Characters Heyuan Yao, Zhenhua Song, Baoquan Chen, Libin Liuâ We introduce Control VAE, a novel model-based framework for learning generative motion control policies, which allows high-level task policies to reuse various skills to accomplish downstream control tasks. ACM Transactions on Graphics, Vol 41 Issue 6, Article 183 (SIGGRAPH Asia 2022). [Project Page] [Paper] [Video (YouTube|BiliBili)] [Code] Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings Tenglong Ao, Qingzhe Gao, Yuke Lou, Baoquan Chen, Libin Liuâ We present a novel co-speech gesture synthesis method that achieves convincing results both on the rhythm and semantics. ACM Transactions on Graphics, Vol 41 Issue 6, Article 209 (SIGGRAPH Asia 2022). (SIGGRAPH Asia 2022 Best Paper Award) [Project Page] [Paper] [Video (YouTube|BiliBili)] [Code] [Explained (YouTube (English)|ç¥ä¹ (Chinese))] Neural3Points: Learning to Generate Physically Realistic Full-body Motion for Virtual Reality Users Yongjing Ye, Libin Liuâ , Lei Hu, Shihong Xiaâ We present a method for real-time full-body tracking using three VR trackers provided by a typical VR system: one HMD (head-mounted display) and two hand-held controllers. Computer Graphics Forum, Vol 41 Issue 8, Page 183-194 (SCA 2022). [Project Page] [Paper] [Video (YouTube|BiliBili)] Learning to Use Chopsticks in Diverse Gripping Styles Zeshi Yang, KangKang Yin, Libin Liuâ We propose a physics-based learning and control framework for using chopsticks. Robust hand controls for multiple hand morphologies and holding positions are first learned through Bayesian optimization and deep reinforcement learning. For tasks such as object relocation, the low-level controllers track collision-free trajectories synthesized by a high-level motion planner. ACM Transactions on Graphics, Vol 41 Issue 4, Article 95 (SIGGRAPH 2022). [Project Page] [Paper] [Video (YouTube|BiliBili)] [Code] Camera Keyframing with Style and Control Hongda Jiang, Marc Christie, Xi Wang, Libin Liu, Bin Wang, Baoquan Chen We present a tool that enables artists to synthesize camera motions following a learned camera behavior while enforcing user-designed keyframes as constraints along the sequence. ACM Transactions on Graphics, Vol 40 Issue 6, Article 209 (SIGGRAPH Asia 2021). [Project Page] [Paper 20.1MB] [Video] [Code] Learning Skeletal Articulations With Neural Blend Shapes Peizhuo Li, Kfir Aberman, Rana Hanocka, Libin Liu, Olga Sorkine-Hornung, Baoquan Chen We present a technique for articulating 3D characters with pre-defined skeletal structure and high-quality deformation, using neural blend shapes â corrective, pose-dependent, shapes that improve deformation quality in joint regio",
  "content_length": 15720,
  "method": "requests",
  "crawl_time": "2025-12-01 13:47:06"
}