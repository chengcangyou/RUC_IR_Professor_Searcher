{
  "name": "Khoa D. Doan",
  "homepage": "https://khoadoan.me",
  "status": "success",
  "content": "Khoa D Doan MAIL Research aims to develop Machine Learning Algorithms that Make Sense in constrained and large-scale, societal applications in Advertising, Healthcare, Sustainability (Remote Sensing, Computing, Agricultural)… [more about our research] [SAIL Research ] [photos ] [open office hour ] VinUni-Illinois Smart Health Center (VISHC) – VISHC is open to collaborate with all researchers and industry practioners inside/oustide of Vietnam to solve healthcare related challenges with translational and innovative research, together. Please reach out via [email] for collaboration. [more about VISHC]. Center for Environmental Intelligence (CEI) – MAIL-Research is a member of CEI. Please reach out via [email] for collaboration on environmental monitoring. [more about CEI]. Selected Press Coverage: khoahocphattrien , Thanh Nien, VnExpress, BaoDauTu, DanTri, Vietnam.vn, Vietnam.vn, Yahoo Finance, Benzinga, Macau Business, Taiwan News, TNGlobal, VinGroup… Khoa D Doan Assistant Professor -- College of Engineering & Computer Science, VinUniversity Associate Director -- VinUni-Illinois Smart Health Center Tweets by khoaddoan We're open to research/industry collaborations in ML/CV/NLP: Immediate Positions: Postdoc (1-2), working on responsible (safe/secure) ML and mental-health NLP (co-advised with either (a) Nitesh Chawla, or (b) Heng Ji and Dilek Hakkani-Tür, and VinUni Faculty), please also fill in the form here. Deadline: Until Filled! Immediate Positions: Master by Research (2), PhD (2), Research Assistants (3-4) also working on LLM/RL Fine-tuning, responsible (safe/secure) ML, mental-health NLP, and ML for Science (co-advised with either (A) Nitesh Chawla or (B) Heng Ji/Dilek Hakkani-Tür, or (C) Chee Seng Chan/Kuan-Hao Huang and VinUni Faculty), please also fill in the form here. Deadline: December 31st (positions are filling up quickly)! Other collaboration? Please reach out via email. news [11/2025] Will serve as Invited Area Chair for ICML 2026. [11/2025] Gave a talk at National Taiwan University on novel problem solving and LLMs. [11/2025] Accepted paper - [AAAI’26-a] on clean backdoor attacks with data distillation. [09/2025] Accepted papers - [NeurIPS’25-a] on fixing DPO/IPO’s overfitting, [NeurIPS’25-b] on parameter attributions in and inference control fo diffusion models, [NeurIPS’25-c] efficient inference with token merging in 3D Point Cloud, and [NeurIPS-W’25-d] on clean backdoor attacks with data distillation. [08/2025] Will serve as Invited Area Chair for AISTATS and ICLR 2026. [07/2025] Will serve on the Editorial Board of ACM AI Letters, a premier venue focusing on impactful, concise, and timely communications in AI. [07/2025] Congrats several MAIL members for Grad Acceptances: Chau H Tran and Nguyen K Hoang and Tuan M Nguyen → UIUC, Hieu N Nguyen → Penn State, Quan H Do → Institut Polytechnique de Paris [06/2025] Accepted papers - [ICML’25-a] (Oral) on evaluating novel equation discovery of LLM-based symbolic regression methods and [ICML-W’25-b] on fragment-aware, structure-guided graph transformer. [04/2025] Gave a talk at ICLR’25 ML for Science on Low-resource Machine Learning and Opportunities in LMICs. [04/2025] Attend Global AI Summit on Africa and Grand Challenges AI Community Convening in Kigali, Rwanda. [03/2025] Our DIG-BUGS workshop (w. Franziska Boenisch, Adam Dziedzic, Aniruddha Saha, Bo Li, Viet-Anh Nguyen, Zhenting Wang, and Heather Zheng on Responsibly managed training data for Gen Models accepted to ICML’2025. [02/2025] Will serve as Invited Area Chair for NeurIPS 2025. [11/2024] Will serve as Invited Area Chair for ICML 2025. [10/2024] We received ~$340K USD AWS funding from AWS Health Equity Initiative (HEI) Program. [09/2024] Will serve as Invited Area Chair for AISTATS 2025. selected publications [full list] Majority of work done or significant contribution by MAIL/SAIL members!! NeurIPS How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need? Tuan A Tran, Duy MH Nguyen, Chau H Tran, and others In Advances in Neural Information Processing Systems 2025 Abs Bib Recent advances in 3D point cloud transformers have led to state-of-the-art results in tasks such as semantic segmentation and reconstruction. However, these models typically rely on dense token representations, incurring high computational and memory costs during training and inference. In this work, we present an efficient token merging strategy that drastically reduces the token count by up to 90–95\\% while preserving competitive performance. Our approach estimates token importance by leveraging spatial structures within the 3D point cloud, enabling aggressive token reduction with minimal degradation in accuracy. This finding challenges the prevailing assumption that more tokens inherently yield better performance and highlights that many current models are over-tokenized and under-optimized for scalability. We validate our method across multiple 3D vision tasks and show consistent improvements in computational efficiency. Our ongoing work will release code and detailed benchmarks to support reproducibility and further system-level exploration of efficient foundation models for 3D data. @inproceedings{nguyen2025merging3d, title = {How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?}, author = {Tran, Tuan A and Nguyen, Duy MH and Tran, Chau H and others}, booktitle = {Advances in Neural Information Processing Systems}, year = {2025}, bibtex_show = {true}, abbr = {NeurIPS}, selected = {true} } NeurIPS Mitigating Reward Over-optimization in Direct Alignment Algorithms with Adaptive Importance Sampling Phuc M Nguyen, Ngoc-Hieu Nguyen, Binh T Nguyen, and Khoa D Doan In Advances in Neural Information Processing Systems 2025 Abs Bib PDF Code Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO) have emerged as alternatives to the standard Reinforcement Learning from Human Feedback (RLHF) for aligning large language models (LLMs) with human values. However, these methods are more susceptible to over-optimization, in which the model drifts away from the reference policy, leading to degraded performance as training progresses. This paper proposes a novel importance-sampling approach to mitigate the over-optimization problem of offline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective with an importance ratio that accounts for the reference policy distribution. IS-DAAs additionally avoid the high variance issue associated with importance sampling by clipping the importance ratio to a maximum value. Our extensive experiments demonstrate that IS-DAAs can effectively mitigate over-optimization, especially under low regularization strength, and achieve better performance than other methods designed to address this problem. @inproceedings{nguyen2025ais, title = {Mitigating Reward Over-optimization in Direct Alignment Algorithms with Adaptive Importance Sampling}, author = {Nguyen, Phuc M and Nguyen, Ngoc-Hieu and Nguyen, Binh T and Doan, Khoa D}, booktitle = {Advances in Neural Information Processing Systems}, year = {2025}, bibtex_show = {true}, abbr = {NeurIPS}, pdf = {https://arxiv.org/abs/2506.08681}, selected = {true}, code = {https://github.com/mail-research/AIS-Sampling4DAAs} } NeurIPS Unveiling Concept Attribution in Diffusion Models Quang H Nguyen, Phan Hoang, and Khoa D Doan In Advances in Neural Information Processing Systems 2025 Abs Bib PDF Code Diffusion models have shown remarkable abilities in generating realistic and high-quality images from text prompts. However, a trained model remains black-box; little do we know about the role of its components in exhibiting a concept such as objects or styles. Recent works employ causal tracing to localize layers storing knowledge in generative models without showing how those layers contribute to the target concept. In this work, we approach the model interpretability problem from a more general perspective and pose a question: \\textit{``How do model components work jointly to demonstrate knowledge?''}. We adapt component attribution to decompose diffusion models, unveiling how a component contributes to a concept. Our framework allows effective model editing, in particular, we can erase a concept from diffusion models by removing positive components while remaining knowledge of other concepts. Surprisingly, we also show there exist components that contribute negatively to a concept, which has not been discovered in the knowledge localization approach. Experimental results confirm the role of positive and negative components pinpointed by our framework, depicting a complete view of interpreting generative models. Our code is available at https://github.com/mail-research/CAD-attribution4diffusion @inproceedings{nguyen2025cad, title = {Unveiling Concept Attribution in Diffusion Models}, author = {Nguyen, Quang H and Hoang, Phan and Doan, Khoa D}, booktitle = {Advances in Neural Information Processing Systems}, year = {2025}, bibtex_show = {true}, abbr = {NeurIPS}, pdf = {https://arxiv.org/abs/2412.02542}, selected = {true}, code = {https://github.com/mail-research/CAD-attribution4diffusion} } ICML ORAL LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models Parshin Shojaee, Ngoc-Hieu Nguyen, Kazem Meidani, Amir Barati Farimani, Khoa D Doan, and Chandan K Reddy In International Conference on Machine Learning 2025 Abs Bib PDF Code Data Submission History Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflate",
  "content_length": 64314,
  "method": "requests",
  "crawl_time": "2025-12-01 13:42:48"
}