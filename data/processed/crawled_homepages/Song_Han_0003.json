{
  "name": "Song Han 0003",
  "homepage": "https://hanlab.mit.edu/songhan",
  "status": "success",
  "content": "Song Han - Associate Professor, MIT EECS Song HanAssociate Professor, MIT EECSAboutSong Han is an associate professor with tenure at MIT EECS. He earned his PhD from Stanford, pioneering efficient AI computing techniques including âDeep Compressionâ (pruning, quantization) and the âEfficient Inference Engine,â which first introduced weight sparsity to modern AI chips, making it one of the top-5 most cited papers in the 50-year history of ISCA (1953-2023). His innovations, including TinyML and hardware-aware neural architecture search (Once-for-All Network), have advanced AI model deployment on resource-constrained devices. His recent work on LLM quantization and acceleration (SmoothQuant, AWQ, StreamingLLM) has improved efficiency in LLM inference, adopted by NVIDIA TensorRT-LLM. Song received best paper awards at ICLR'16, FPGA'17, and MLSys'24, the NSF CAREER Award, â35 Innovators Under 35,â IEEE âAIâs 10 to Watch,â and the Sloan Research Fellowship. He developed the open lecture series EfficientML.ai to share advances in efficient ML research.âRecent work:Â accelerating LLM and generative AI [slides]LLM Quantization: AWQ, TinyChat enables on-device LLMÂ inference with 4bit quantization (best paper award at MLSys'24), with 19 million downloads on HuggingFace. SmoothQuant is a training-free and accuracy-preserving 8-bit post-training quantization (PTQ) solution for LLMs. QServe speeds up the large scale LLMÂ serving with W4A8KV4 quantization (4-bit weights, 8-bit activations, and 4-bit KV cache). COAT enables memory efficient FP8 training.Long Context LLM:Â StreamingLLM enables LLMs to generate infinite-length texts with a fixed memory budget by preserving the \"attention sinks\" in the KV-cache. StreamingVLM introduceda streaming-aware KV cache with attention sinks to enable real-time understanding of infinite video streams.Quest leverages query-aware sparsity in long-context KV cache to boost inference throughput. DuoAttention reduces both LLM's decoding and pre-filling memory and latency with retrieval and streaming heads. LServe accelerates long-context LLM serving with hardware-aware unified sparse attention framework.Sparse Attention: SpAtten invented cascade KV cache pruning and head pruning. XAttention accelerate long-context prefilling with block sparse attention and anti-diagnol scoring. Sparse VideoGen introduced an online profiling strategy to identify spatial-temporal sparsity and a hardware-efficient layout transformation. Radial Attention identified the Spatiotemporal Energy Decay phenomenon and proposed a corresponding O(n log n) sparse attention mechanism. Sparse VideoGen2 introduced semantic-aware permutation and efficient dynamic block size attention kernels.Efficient Visual Generation: HARTÂ is an autoregressive visual generation model capable of directly generating 1024Ã1024 images on a laptop. SANA enables 4K image synthesis under low computation, using deep compression auto-encoder (DC-AE) and linear diffusion transformer. SANA-1.5 explores efficient training scaling and inference scaling for diffusion models. SANA-Sprint is a one-step distilled diffusion model enabling real-time generation. SVDQuant further enables 4-bit diffusion models (W4A4)Â by absorbing the outliers with low-rank components. SANA-Video introduced the Linear Diffusion Transformer and a constant-memory KV cache. DC-VideoGen introduced a chunk-causal Deep Compression Video Autoencoder and the AE-Adapt-V adaptation strategy.Efficient Visual Language Models: VILA, VILA-U, LongVILA are a family of efficient visual language models for both understanding and generation. LongVILA efficiently scales to 6K frames of video.TwitterGoogle ScholarGitHubYouTubeInstagramLinkedinHANÂ Lab WebsiteResearch InterestsEfficient Generative AIGenerative AI models are significantly larger (>1000x) than traditional predictive AI, presenting new computational challenges. We innovated in key areas of quantization, parallelization, KV cache optimization, long-context learning, and multi-modal representation learning to minimize GenAI costs.SANA-VideoDC-VideoGenTLTRadial AttentionSANA-SprintDC-ARDC-AE 1.5StreamingVLMDC-GenJet-NemotronXAttentionSANA-1.5QServeSVDQuantCOATSANADuoAttentionDC-AELongVILAVILA-UHARTFastComposerDistriFusionCANVILAQuestStreamingLLMAWQLongLoRAEfficientViTSpAtten-ChipSmoothQuantSIGEAnycostGANDiffAugmentGAN CompressionLite TransformerHATModel Compression and TinyMLI pioneered the area of model compression that can shrink neural networks by >10x without hurting accuracy. By pruning, quantization, neural architecture search, we can fit neural networks in micro-controllers (MCUs). We also enable on-device training with 1000x less memory on MCUs.LEGOTinyML-ProjectsTinyML-MagazinePockEngineBEVFusionOn-Device TrainingLitePoseNetAugNAASMCUNet-v2TinyTLGCN-RL Circuit DesignerMCUNetAPQOFAParkTSMHAQProxylessNASL2DCAMCDeep CompressionPruningAccelerating AI with SparsitySparsity in neural networks arises where not all neurons are connected. I designed the first hardware accelerator EIE to exploit weight sparsity. I identify new sources of sparsity in modern AI: sparse attention, token pruning, point cloud, and implement efficient systems and accelerators to efficiently exploit sparsity.LServeSparseRefineFlatFormerEIE RetrospectiveTorchSparseDGAPointAccSemAlignSpAttenSPVNASSpArchPVCNNDGCEIETeachingTinyML and Efficient Deep Learning Computing6.5940 â¢ Fall â¢ 2024 â¢ https://efficientml.aiLive Streaming: Time: The course will not be offered in Fall 2025 due to Prof. Han is on sabbatical Location: 34-101More About CourseLecture 1: IntroductionLecture 12: Transformer and LLM (I)Lecture 13: Transformer and LLM (II)Lecture 16: Diffusion ModelResearchTab 1Tab 2Efficient AI AlgorithmThe incredible potential of large models in Artificial Intelligence Generated Content (AIGC), including cutting-edge technologies like Large Language Models (LLMs) and Diffusion Models, have revolutionized a wide range of applications, spanning natural language processing, content generation, creative arts, and more. However, large model size, and high memory and computational requirements present formidable challenges. We aim to tackle these hurdles head-on and make these advanced AI technologies more practical, democratizing access to these future-changing technologies for everyone.Efficient AI Hardware & SystemEfficiency improvements in deep learning often start with refining algorithms, but these theoretical gains, like reducing FLOPs and model size, don't always easily lead to practical speed and energy savings. The demand arises for specialized hardware and software systems to bridge this gap. These specialized software and hardware systems create a fresh design dimension independent of the algorithm space. This opens up opportunities for holistic optimization by co-designing both the algorithm and the software/hardware systems.Industry ImpactOur efficient ML research has influenced and landed in many industry products, thanks to the close collaboration with our sponsors: Intel OpenVino, Intel Neural Compressor, Apple Neural Engine, NVIDIA Sparse Tensor Core, NVIDIA FasterTransformer, AMD-Xilinx Vitis AI, Qualcomm AI Model Efficiency Toolkit (AIMET), Amazon AutoGluon, Microsoft NNI, SONY Neural Architecture Search Library, SONY Model Compression Toolkit, Â ADI MAX78000/MAX78002 Model Training and Synthesis Tool, Ford Trailer Backup Assist.Open source projects with over 1K GitHub stars:SANA-Video: Efficient Video Generation with Block Linear Diffusion TransformerCodeSANA-Sprint: One-Step Diffusion with Continuous-Time Consistency DistillationCodeSANA-1.5: Efficient Scaling of Training-Time andâ¨Inference-Time Compute in Linear Diffusion TransformerCodeSVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion ModelsCodeSANA: Efficient High-Resolution Image Synthesis with Linear Diffusion TransformersCodeDeep Compression Autoencoder for Efficient High-Resolution Diffusion ModelsCodeLongVILA: Scaling Long-Context Visual Language Models for Long VideosCodeCondition-Aware Neural Network for Controlled Image GenerationCodeEfficient Streaming Language Models with Attention SinksCodeAWQ: Activation-aware Weight Quantization for LLM Compression and AccelerationCodeLongLoRA: Efficient Fine-tuning of Long-Context Large Language ModelsCodeTiny Machine Learning ProjectsCodeEfficientViT: Multi-Scale Linear Attention for High-Resolution Dense PredictionCodeBEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View RepresentationCodeQuantumNAS: Noise-Adaptive Search for Robust Quantum CircuitsCodeDifferentiable Augmentation for Data-Efficient GAN TrainingCodeGAN Compression: Efficient Architectures for Interactive Conditional GANsCodeOnce-for-All: Train One Network and Specialize it for Efficient DeploymentCodeTSM: Temporal Shift Module for Efficient Video UnderstandingCodeProxylessNAS: Direct Neural Architecture Search on Target Task and HardwareCodeMore About Research ProjectsHonors and AwardsDate (Newest to Oldest)Date (Oldest to Newest)5/1/20232023 Sloan Research Fellowship5/1/20235/1/20222022 Red Dot Award5/1/20225/1/20212021 Samsung Global Research Outreach (GRO) Award5/1/20215/1/20212021 NVIDIA Academic Partnership Award5/1/20215/1/20202020 NVIDIA Academic Partnership Award5/1/20205/1/20202020 IEEE \"AIs 10 to Watch: The Future of AI\" Award5/1/20205/1/20202020 NSF CAREER Award5/1/20205/1/20192019 MIT Technology Review list of 35 Innovators Under 355/1/20195/1/20202020 SONY Faculty Award5/1/20205/1/20172017 SONY Faculty Award5/1/20175/1/20182018 SONY Faculty Award5/1/20185/1/20182018 Amazon Machine Learning Research Award5/1/20185/1/20192019 Amazon Machine Learning Research Award5/1/20195/1/20192019 Facebook Research Award5/1/20196/13/20246/13/2024Best Paper Award of MLSys 2024 AWQ6/15/20236/15/2023Top 5 cited papers in 50 years of ISCA of EIE Retrospective5/15/20175/15/2017Best Paper Award of FPGA 2017 5/15/20165/15/2016Bes",
  "content_length": 42540,
  "method": "requests",
  "crawl_time": "2025-12-01 14:29:26"
}