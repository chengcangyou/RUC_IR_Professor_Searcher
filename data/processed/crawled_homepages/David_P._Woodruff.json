{
  "name": "David P. Woodruff",
  "homepage": "http://www.cs.cmu.edu/~dwoodruf",
  "status": "success",
  "content": "David P. Woodruff David P. Woodruff Professor Theory Group, Department of Computer Science, Carnegie Mellon University Research interests : algorithms, data streams, machine learning, numerical linear algebra, sketching, and sparse recovery Contact: dwoodruf (at) cs (dot) cmu (dot) edu I am also part-time in the Google Omega team. For Google-related matters email woodruffd (at) google (dot) com We are currently running an LLM-based feedback experimental program for STOC here . Let us know if you have feedback! I am currently the chair of CATCS . Please check it out - we welcome any suggestions. I was the PC chair of SODA, 2024 . The accepted papers are here. An alternative talk schedule format is here. A news article is here. I was the PC chair of ICALP, 2022 . The accepted papers are here. Copyright: Persons copying the material below should adhere to the terms of each author's copyright. Book Sketching as a Tool for Numerical Linear Algebra, Foundations and Trends in Theoretical Computer Science, vol 10, issue 1-2, pp. 1-157, 2014. You can download a free copy (for personal use only) here Simons Institute Foundations of Data Science: program page Teaching at CMU: I am honored to receive the Herbert Simon Award for teaching in computer science Fall 2017: 15859 - Algorithms for Big Data Spring 2018: 15451/651 - Algorithms Spring 2019: 15451/651 - Algorithms Fall 2019: 15859 - Algorithms for Big Data Spring 2020: 15451/651 - Algorithms Fall 2020: 15859 - Algorithms for Big Data Spring 2021: 15451/651 - Algorithms School of Computer Science, Executive Education, Online Course Algorithms and Data Structures Fall 2021: 15451/651 - Algorithms Fall 2021: 15859 - Algorithms for Big Data Fall 2022: 15451/651 - Algorithms Fall 2022: 15859 - Algorithms for Big Data Spring 2024: 15451/651 - Algorithms Spring 2024: 15851 - Algorithms for Big Data Spring 2025: 15451/651 - Algorithms Spring 2025: 15851- Algorithms for Big Data My Amazing Students and Postdocs Students: Ainesh Bakshi, co-advised with Pravesh Kothari, Ainesh graduated, was a postdoc at MIT, and is an assistant professor at NYU Rajesh Jayaram , Raj has graduated, and has joined Google Research NYC as a Research Scientist Praneeth Kacham , Praneeth has graduated, and has joined Google Research NYC as a Resesarch Scientist Honghao Lin Hoai-An Nguyen co-advised with Yang Liu Hai Pham co-advised with Barnabas Poczos. Hai has graduated and joined Reka AI Madhusudhan Pittu co-advised with Anupam Gupta Kijun Shin Taisuke Yasuda , Tai has graduated, and has joined the Voleon Group as a Member of the Research Staff Hongyang Zhang, co-advised with Nina Balcan. Hongyang has graduated, was a postdoc at TTIC, and is an assistant professor at the University of Waterloo Postdocs: William Swartworth Will will join the Voleon Group as a Member of the Research Staff Samson Zhou Samson was a postdoc at UC Berkeley/Rice and will join Texas A & M 2017 Course on Sketching as a Tool for Numerical Linear Algebra All slides for 12 1-hour lectures slides l1LowRankSlidesÂ  weightedLowRankSlides 2016 Summer School Course Slides (Sketching as a Tool for Numerical Linear Algebra) allLectures.pptx allLectures.pdf. The other slides for day 4 are regressionM and lowRankM and weighted Lecture Notes (MADALGO and BASICS) Here are three lectures, slight variants of which were given at the MADALGO summer school on streaming 2015 as well as the BASICS summer school on communication complexity 2015. The first lecture is an introduction to information theory for data streams, the second contains direct sum theorems for data streams, and the third covers multiplayer communication complexity. Lecture 1 Lecture 2 Lecture 3 Publications 2026 SODA, Online Learning with Limited Information in the Sliding Window Model with Vladimir Braverman, Sumegha Garg, Chen Wang, and Samson Zhou SODA, L_p Sampling in Distributed Data Streams with Applications to Adversarial Robustness with Honghao Lin, Zhao Song, Shenghao Xie, and Samson Zhou PODS, Finding Heavy-Hitters with Optimal State Changes with William Swartworth 2025 NeurIPS, Nearly-Linear Time and Massively Parallel Algorithms for k-Anonymity with Kevin Aydin, Honghao Lin, and Peilin Zhong NeurIPS, Query-Efficient Locally Private Hypothesis Selection via the Scheffe Graph with Gautam Kamath, Alireza F. Pour, and Matthew Regehr FOCS, Perfect L_p Sampling with Polylogarithmic Update Time with William Swartworth and Samson Zhou FOCS, Root Ridge Leverage Score Sampling for $\\ell_p$ Subspace Approximation with Taisuke Yasuda APPROX, Multipass Linear Sketches for Geometric LP-Type Problems with Efe Cekirge and William Gay ICML, On Fine-Grained Distinct Element Estimation with Ilias Diakonikolas, Daniel Kane, Jasper C.H. Lee, Thanasis Pittas, and Samson Zhou ICML, Maximum Coverage in Turnstile Streams with Aline Ene, Alessandro Epasto, Vahab Mirrokni, Hoai-An Nguyen, Huy Nguyen, and Peilin Zhong ICML, On Differential Privacy for Adaptively Solving Search Problems via Sketching with Shiyuan Feng, Ying Feng, George Li, Zhao Song, and Lichen Zhang Selected for Oral Presentation (top 1% of submissions) ICML, Robust Sparsification via Sensitivity with Chansophea Wathanak In, Yi Li, and Xuan Wu ICML, Understanding the Kronecker Matrix-Vector Complexity of Linear Algebra with Raphael Meyer and William Swartworth ICALP, Guessing Efficiently for Constrained Subspace Approximation with Aditya Bhaskara, Sepideh Mahabadi, Madhusudhan Pittu, and Ali Vakilian Full version on arXiv ICALP, Tight Bounds for Heavy-Hitters and Moment Estimation in the Sliding Window Model with Shiyuan Feng and William Swartworth Full version on arXiv PODS, Perfect Sampling in Turnstile Streams Beyond Small Moments with Shenghao Xie and Samson Zhou Full version on arXiv SoCG, Range Counting Oracles for Geometric Problems with Anne Driemel, Morteza Monemizadeh, Eunjin Oh, and Frank Staals Full version on arXiv STOC, Lifting Linear Sketches: Optimal Bounds and Adversarial Robustness with Elena Gribelyuk, Honghao Lin, Huacheng Yu, and Samson Zhou Full version on arXiv ICLR, Streaming Algorithms for lp Flows and lp Regression with Amit Chakrabarti, Jeffrey Jiang, and Taisuke Yasuda Full version on OpenReview Seleted for Spotlight Presentation ICLR, Time, Space, and Streaming Efficient Algorithm for Heavy Attentions with Ravindran Kannan, Chiranjib Bhattacharyya, and Praneeth Kacham Full version on arXiv ICLR, Beyond Worst-Case Dimensionality Reduction for Sparse Vectors with Sandeep Silwal and Qiuyi Zhang Full version on arXiv ITCS, Space Complexity of Minimum Cut Problems in Single-Pass Streams with Matthew Ding, Alexandro Garces, Jason Li, Honghao Lin, Jelani Nelson, and Vihan Shah Full version on arXiv SODA, Tight Sampling Bounds for Eigenvalue Approximation with William Swartworth Full version on arXiv iScience, Learning-Augmented Sketching Offers Improved Performance for Privacy Preserving and Secure GWAS with Junyan Xu, Kaiyuan Zhu, Jieling Cai, Can Kockan, Natnatee Dokmai, Hyunghoon Cho, and Cenk Sahinalp Full version on biorxiv 2024 NeurIPS, Communication Bounds for the Distributed Experts Problem with Zhihao Jia, Qi Pang, Trung Tran, Zhihao Zhang, and Wenting Zheng Full version on arXiv NeurIPS, Approximating the Top Eigenvector in Random Order Streams with Praneeth Kacham Selected for Spotlight Presentation Full version on arXiv NeurIPS, Even Sparser Graph Transformers with Hamed Shirzad, Honghao Lin, Balaji Venkatachalam, Ameya Velingker, and Danica J. Sutherland Full version on arXiv NeurIPS, On Socially Fair Low-Rank Approximation and Column Subset Selection with Zhao Song, Ali Vakilian, and Samson Zhou Full verison on arXiv NeurIPS, John Ellipsoids via Lazy Updates with Taisuke Yasuda Full version on arXiv NeurIPS, Adversarially Robust Dense-Sparse Tradeoffs via Heavy-Hitters with Samson Zhou Full version on arXiv EMNLP, GRASS: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients with Aashiq Muhamed, Oscar Li, Mona T. Diab, and Virginia Smith Full version on arXiv FOCS, A Strong Separation for Adversarially Robust $\\ell_0$ Estimation for Linear Sketches with Elena Gribelyuk, Honghao Lin, Huacheng Yu, and Samson Zhou Full version on arXiv RANDOM, Faster Algorithms for Schatten-p Low Rank Approximation Full version on arXiv with Praneeth Kacham ICML, Sensitivity Sampling for Coreset-Based Data Selection Full version on arXiv with Kyriakos Axiotis, Vincent Cohen-Addad, Monika Henzinger, Sammy Jerome, Vahab Mirrokni, David Saulpic, and Michael Wunder ICML, High-Dimensional Geometric Streaming for Nearly Low Rank Data Full version on arXiv with Hossein Esfandiari, Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong ICML, Fast White-Box Adversarial Streaming Without a Random Oracle Full version on arXiv with Ying Feng and Aayush Jain ICML, Learning Multiple Secrets in Mastermind Full version on arXiv with Milind Prabhu ICML, Fast Sampling-Based Sketches for Tensors Full version on arXiv with William Swartworth Selected for Spotlight Presentation ICML, Dimension-Free Coresets for Multiple $\\ell_p$ Regression Full version on arXiv with Taisuke Yasuda ICML, Reweighted Solutions for Weighted Low Rank Approximation Full version on arXiv with Taisuke Yasuda STOC, A New Information Complexity Measure for Multi-Pass Streaming with Applications Full version on arXiv with Mark Braverman, Sumegha Garg, Qian Li, Shuo Wang, and Jiapeng Zhang STOC, Optimal Communication Bounds for Classic Functions in the Coordinator Model and Beyond Full version on arXiv with Hossein Esfandiari, Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong STOC, Improving the Bit Complexity of Communication for Distributed Convex Optimization Full version on arXiv with Mehrdad Ghadiri, Yin Tat Lee, Swati Padmanabhan, William Swartworth, and Guanghao Ye ICLR, HyperAttention: Long-context Attention in Near-Linear Time Full version on arXiv with Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, and Amir Zandieh ICLR, O",
  "content_length": 47497,
  "method": "requests",
  "crawl_time": "2025-12-01 12:59:53"
}