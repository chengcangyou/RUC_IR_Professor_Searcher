{
  "name": "Ross Anderson 0001",
  "homepage": "https://www.cl.cam.ac.uk/~rja14",
  "status": "success",
  "content": "Ross Anderson's Home Page Ross Anderson passed away in March 2024. (Obituaries) We preserve here the content of his personal web space. If you notice any problems, please contact pagemaster@cl.cam.ac.uk. Ross Anderson [Research] [Blog] [Videos] [Politics] [My Book] [Music] [Seminars] [Contact Details] Machine Learning needs Better Randomness Standards: Randomised Smoothing and PRNG-based attacks shows that the randomness tests long used to check random number generators for use in cryptographic key generation are inadequate for machine learning, where some applications make heavy use of random inputs about which very specific assumptions are made (accepted for Usenix 2024) Defacement Attacks on Israeli Websites is a measurement study of attacks by Palestinian sympathisers on Israeli websites since the Hamas attack on Israel (CW blog). Getting Bored of Cyberwar is a similar study of how pro-Ukrainian hackers responded to the Russian invasion of their country by attacking Russian websites, and pro-Russian hackers then responded (AP SC Magazine The Record) No Easy Way Out: the Effectiveness of Deplatforming an Extremist Forum to Suppress Hate and Harassment is a measurement study of the industry attempt to take down Kiwi Farms in 2022-23. This holds a number of practical lessons for people interested in online censorship, as well as raising legal and philosophical issues with the approach taken by the UK's Online Safety Bill (The Register; accepted for Oakland 2024) The Curse of Recursion: Training on Generated Data Makes Models Forget asks what will happen to GPT-{n} once most of the content online is generated by previous models. We show that the use of model-generated content in training leads to irreversible defects in subsquent model generations as the tails of the original distributions disappear, leading to model collapse (The Atlantic, Wall Street Journal, New Scientist, Venture Beat, Business Insider blog) One Protocol to Rule Them All? On Securing Interoperable Messaging analyses the EU DMA mandate for messaging systems interoperability. This will vastly increase the attack surface at every level in the stack (blog Register Schneier). Threat Models over Space and Time: A Case Study of E2EE Messaging Applications shows how Signal Desktop and WhatsApp Desktop are insecure; an opponent with temporary access to your laptop, such as a border guard or an intimate partner, can make this access persistent. Chat Control or Child Protection debunks the arguments used by the intelligence community that \"because children\" we needed the Online Safety Bill which gave Ofcom the power to mandate snooping software in your phone (blog). The same arguments were used to support the so-called Child Sex Abuse Regulation which thankfully failed in the European Parliament (blog evidence video) – our big policy win of 2023. Cambridge forced me to retire in September 2023 when I turned 67, a policy of unlawful age discrimination against which we are campaigning. I am now 20% at Edinburgh and (officially) 20% at Cambridge. I'm teaching a course in Security Engineering at Edinburgh to masters students and fourth-year undergrads, and the lecture videos are now all online (as are the lecture videos and notes for my first-year undergrad course on Software and Security Engineering at Cambridge). timeline ... Research The research students I advise are Bill Marino, Eleanor Clifford, Lawrence Piao, Jenny Blessing, Nicholas Boucher, Anh Viet Vu, and David Khachaturov. My RAs are Richard Clayton and Hridoy Dutta. I also work with Robert Brady. My former RAs are Sergei Skorobogatov, Lydia Wilson, Franck Courbon, Maria Bada, Yi Ting Chua, Ben Collier, Helen Oliver, Ildiko Pete, Daniel Thomas, Alice Hutchings, Sergio Pastrana, David Modic, Sven Übelacker, Julia Powles, Ramsey Faragher, Sophie van der Zee, Mike Bond, Vashek Matyas, Steven Murdoch, Andrei Serjantov and Alex Vetterl. My former students Jong-Hyeon Lee, Frank Stajano, Fabien Petitcolas, Harry Manifavas, Markus Kuhn, Ulrich Lang, Jeff Yan, Susan Pancho-Festin, Mike Bond, George Danezis, Sergei Skorobogatov, Hyun-Jin Choi, Richard Clayton, Jolyon Clulow, Hao Feng, Andy Ozment, Tyler Moore, Shishir Nagaraja, Robert Watson, Hyoungshick Kim, Shailendra Fuloria, Joe Bonneau, Wei-Ming Khoo, Rubin Xu, Laurent Simon, Kumar Sharad, Shehar Bano, Dongting Yu, Khaled Baqer, Alex Vetterl, Mansoor Ahmed and Ilia Shumailov have earned PhDs. I'm teaching three Cambridge courses in 2023-24: the undergraduate course in Software and Security Engineering and graduate courses in Computer Security and Cybercrime. I also organise our security seminars and help run the Cambridge Cybercrime Centre. My research topics include: Machine learning and signal processing – from adversarial machine learning to side channels Sustainability of security – from software patching through energy management to fighting wildlife crime Economics, psychology and criminology of information security – from dependability to deception detection Peer-to-Peer and social network systems – including the Eternity Service, cocaine auctions and suicide bombing Reliability of security systems – including bank fraud and hardware hacking Robustness of cryptographic protocols – including API attacks Cryptography – including why quantum crypto security proofs based on entanglement are convincing Security of clinical information systems – including ethics, genomic privacy and the care.data scandal Privacy and freedom issues – including chat control, \"Keys under Doormats\" and the Online Safety Act Machine learning and signal processing The detection and manipulation of patterns, both overt and covert, has many applications, and the field is being refreshed by the recent revolution in neural networks. Machine Learning needs Better Randomness Standards: Randomised Smoothing and PRNG-based attacks AI Security shows that the randomness tests long used to check random number generators for use in cryptographic key generation are inadequate for machine learning, where some applications make heavy use of random inputs about which very specific assumptions are made. The Curse of Recursion: Training on Generated Data Makes Models Forget asks what will happen to GPT-{n} once most of the content online is generated by previous models. We show that the use of model-generated content in training leads to irreversible defects in subsquent model generations as the tails of the original distributions disappear, leading to model collapse (The Atlantic, Wall Street Journal, New Scientist, Venture Beat, Business Insider) Talking Trojan describes what we learned from trying to get industry to fix the Trojan Source vulnerability, which broke almost all computer languages, and the related Bad Characters vulnerability, which broke almost all NLP models. What parts of the disclosure ecosystem work, and which are broken? (blog) When Vision Fails then showed that the \"obvious\" defence to the Bad Characters attack, namely rendering text and then OCRing it, doesn't really work that well; and Boosting Big Brother: Attacking Search Engines with Encodings showed that the same techniques could be used for search engine optimisation and poisoning (blog). Trojan Source: Invisible Vulnerabilities shows how adversarial coding can make source code look different to a compiler and to a human reviewer. This enables supply-chain attacks to hide in plain sight (website blog). Bad Characters: Imperceptible NLP Attacks shows how the systems used for common natural-language processing tasks such as machine translation and toxic content filtering can be broken easily by inputs with adversarial coding. This can enable bad actors to hide in plain sight (website code). Markpainting: Adversarial Machine Learning meets Inpainting shows how to defeat inpainters – machine-learning tools that make it easy to edit or even forge images. Adversarial machine-learning tricks can be used to make images tamper-evident, or to add copyright marks that are extremely difficult for inpainters to remove (blog). Situational Awareness and Machine Learning – Robots, Manners and Stress argues that manners are a new frontier for research in robotics and machine learning. ML models find it really hard to interact with multiple humans, for example when an autonomous vehicle is trying to turn across traffic; this is related to situational awareness (blog). Data Ordering Attacks enable you to poison or backdoor a machine-learning system without changing the training data; you only have to manipulate the order in which the training samples are presented. For example, you can train a credit-scoring algorithm to be sexist by starting its training with ten rich men and ten poor women; but it's very much more general than that (blog). Sponge Examples: Energy-Latency Attacks on Neural Networks describes how to find inputs to neural networks that make them take a lot of time, or burn a lot of energy. They can be used to distract or to jam machine learning systems in a wide range of applications (blog press Schneier). Blackbox Attacks on Reinforcement Learning Agents Using Approximated Temporal Information demonstrates delayed-action attacks on reinforcement learning agents; some might be used as time bombs. Nudge Attacks on Point-Cloud DNNs disturb a small number of input points to a DNN to change how it classifies a 3-d object, and may therefore cause an autonomous vehicle or other robot to misunderstand its environment. We show two ways to generate them. The Taboo Trap is a mechanism we invented to block adversarial machine learning attacks on energy-constrained devices. An older version of paper was the subject of my invited talk at AISEC 2019. It emerged from earlier work on neural network compression, which appeared at SysML. The Taboo Trap work also led to further papers on transferability and adversarial reinforcement learning. Hey Alexa what did I just type? Decoding smartphone sounds with a voice assistant show",
  "content_length": 95064,
  "method": "requests",
  "crawl_time": "2025-12-01 14:21:08"
}