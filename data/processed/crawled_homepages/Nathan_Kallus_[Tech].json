{
  "name": "Nathan Kallus [Tech]",
  "homepage": "https://nathankallus.com",
  "status": "success",
  "content": "Nathan Kallus Nathan Kallus    [nah-tahn kah-loosh]Associate ProfessorCornell Tech, Cornell UniversityField member: ORIE, CS, Econ, Stats, CAMDirector Machine Learning & Inference Research, Netflix           kallus@cornell.edu   GitHub   Google Scholar Causal ML Book Applied Causal Inference Powered by ML and AIV. Chernozhukov, C. Hansen, N. Kallus, M. Spindler, V. SyrgkanisBook website.arXiv.Abstract: An introduction to the emerging fusion of machine learning and causal inference. The book presents ideas from classical structural equation models (SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and structural causal models (SCMs), and covers Double/Debiased Machine Learning methods to do inference in such models using modern predictive tools. Research group Current Antonia (Miruna) Oprescu Brian Cho Alumni (and first position) Graduated PhD students Kaiwen Wang (OpenAI, Safety Research) Masatoshi Uehara (Assistant Professor at University of Wisconsin-Madison, Computer Science Department) Yichun Hu (Assistant Professor at Cornell University, Johnson School of Business) Andrew Bennett (Machine Learning Researcher at Morgan Stanley) Angela Zhou (Assistant Professor at University of Southern California, Marshall School of Business; Research Fellow at Simons Institute) Xiaojie Mao (Assistant Professor at Tsinghua University, Management Science and Engineering) Postdocs Su Jia (Amazon, Research Scientist) Michele Santacatterina (Assistant Professor at NYU Langone, Biostatistics Division) Brenton Pennicooke (Assistant Professor at Washington University in St. Louis, School of Medicine) Always recruiting motivated and talented PhD students for our research group. See here for more information. Publications and working papers Filter by type:AllJournalConferenceFilter by venue: AllPreprintNeurIPSICMLAISTATSManagement ScienceJRSS:BOperations ResearchWWWJASAJMLRFAccTCOLTKDDJournal of Causal InferenceStatistical ScienceMathematical ProgrammingCODEBiometrikaStatistica SinicaStatistics in MedicineStatistics and Public PolicyMathematics of Operations ResearchAnnual Review of Statistics and Its ApplicationIJOOProduction and Operations ManagementJournal of Clinical NeuroscienceDiabetes CareAcademic RadiologyICLRCVPRECRecSysWSDMALTCIKMRLCEMNLPMachine LearningFilter by topics: AllSequential Decision-MakingCausal InferenceOptimization and PersonalizationFairnessApplied MLShowing 146 of 146 recordsJApplied ML|Optimization and Personalization|Causal Inference1The Value of Personalized Recommendations: Evidence from Netflix, with K. Zielnicki, G. Aridor, A. Bibaut, A. Tran, and W Chou.arXiv November 2025.Abstract: Personalized recommendation systems shape much of user choice online, yet their targeted nature makes separating out the value of recommendation and the underlying goods challenging. We build a discrete choice model that embeds recommendation-induced utility, low-rank heterogeneity, and flexible state dependence and apply the model to viewership data at Netflix. We exploit idiosyncratic variation introduced by the recommendation algorithm to identify and separately value these components as well as to recover model-free diversion ratios that we can use to validate our structural model. We use the model to evaluate counterfactuals that quantify the incremental engagement generated by personalized recommendations. First, we show that replacing the current recommender system with a matrix factorization or popularity-based algorithm would lead to 4% and 12% reduction in engagement, respectively, and decreased consumption diversity. Second, most of the consumption increase from recommendations comes from effective targeting, not mechanical exposure, with the largest gains for mid-popularity goods (as opposed to broadly appealing or very niche goods).CSequential Decision-Making|Optimization and Personalization1Inverse Reinforcement Learning Using Just Classification and a Few Regressions, with L. van der Laan and A. Bibaut.arXiv September 2025.Abstract: Inverse reinforcement learning (IRL) aims to explain observed behavior by uncovering an underlying reward. In the maximum-entropy or Gumbel-shocks-to-reward frameworks, this amounts to fitting a reward function and a soft value function that together satisfy the soft Bellman consistency condition and maximize the likelihood of observed actions. While this perspective has had enormous impact in imitation learning for robotics and understanding dynamic choices in economics, practical learning algorithms often involve delicate inner-loop optimization, repeated dynamic programming, or adversarial training, all of which complicate the use of modern, highly expressive function approximators like neural nets and boosting. We revisit softmax IRL and show that the population maximum-likelihood solution is characterized by a linear fixed-point equation involving the behavior policy. This observation reduces IRL to two off-the-shelf supervised learning problems: probabilistic classification to estimate the behavior policy, and iterative regression to solve the fixed point. The resulting method is simple and modular across function approximation classes and algorithms. We provide a precise characterization of the optimal solution, a generic oracle-based algorithm, finite-sample error bounds, and empirical results showing competitive or superior performance to MaxEnt IRL.CApplied ML|Sequential Decision-Making1Entropy After </think> for Reasoning Model Early Exiting, with X. Wang, J. McInerney, and L. Wang.arXiv September 2025.Abstract: Large reasoning models show improved performance with longer chains of thought. However, recent work has highlighted (qualitatively) their tendency to overthink, continuing to revise answers even after reaching the correct solution. We quantitatively confirm this inefficiency by tracking Pass@1 for answers averaged over a large number of rollouts and find that the model often begins to always produce the correct answer early in the reasoning, making extra reasoning a waste of tokens. To detect and prevent overthinking, we propose a simple and inexpensive novel signal -- Entropy After (EAT) -- for monitoring and deciding whether to exit reasoning early. By appending a stop thinking token () and monitoring the entropy of the following token as the model reasons, we obtain a trajectory that decreases and stabilizes when Pass@1 plateaus; thresholding its variance under an exponential moving average yields a practical stopping rule. Importantly, our approach enables adaptively allocating compute based on the EAT trajectory, allowing us to spend compute in a more efficient way compared with fixing the token budget for all questions. Empirically, on MATH500 and AIME2025, EAT reduces token usage by 13 - 21% without harming accuracy, and it remains effective in black box settings where logits from the reasoning model are not accessible, and EAT is computed with proxy models.CSequential Decision-Making|Optimization and Personalization1DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning, with H. Zhao, D. Liang, W. Tang, and D. Yao.arXiv October 2025.Abstract: We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified framework for training masked diffusion large language models (dLLMs) to reason not only better (furious), but also faster via reinforcement learning (RL). We first unify the existing baseline approach such as d1 by proposing to train surrogate policies via off-policy RL, whose likelihood is much more tractable as an approximation to the true dLLM policy. This naturally motivates a more accurate and informative two-stage likelihood approximation combined with importance sampling correction, which leads to generalized RL algorithms with better sample efficiency and superior task performance. Second, we propose a new direction of joint training efficient samplers/controllers of dLLMs policy. Via RL, we incentivize dLLMs' natural multi-token prediction capabilities by letting the model learn to adaptively allocate an inference threshold for each prompt. By jointly training the sampler, we yield better accuracies with lower number of function evaluations (NFEs) compared to training the model only, obtaining the best performance in improving the Pareto frontier of the inference-time compute of dLLMs. We showcase the effectiveness of our pipeline by training open source large diffusion language models over benchmark math and planning tasks.CSequential Decision-Making|Causal Inference1Experimentation Under Non-stationary Interference, with S. Jia, P. Frazier, and C. Lee Yu.arXiv November 2025.Abstract: We study the estimation of the ATE in randomized controlled trials under a dynamically evolving interference structure. This setting arises in applications such as ride-sharing, where drivers move over time, and social networks, where connections continuously form and dissolve. In particular, we focus on scenarios where outcomes exhibit spatio-temporal interference driven by a sequence of random interference graphs that evolve independently of the treatment assignment. Loosely, our main result states that a truncated Horvitz-Thompson estimator achieves an MSE that vanishes linearly in the number of spatial and time blocks, times a factor that measures the average complexity of the interference graphs. As a key technical contribution that contrasts the static setting we present a fine-grained covariance bound for each pair of space-time points that decays exponentially with the time elapsed since their last ‘‘interaction''. Our results can be applied to many concrete settings and lead to simplified bounds, including where the interference graphs (i) are induced by moving points in a metric space, or (ii) follow a dynamic Erdos-Renyi model, where each edge is created or removed independently in each time period.CApplied ML|Sequential Decision-Making|Optimization and Personalization1Rank-GRPO: Tra",
  "content_length": 218786,
  "method": "requests",
  "crawl_time": "2025-12-01 14:03:45"
}