{
  "name": "Marc Peter Deisenroth",
  "homepage": "https://deisenroth.cc",
  "status": "success",
  "content": "Marc DeisenrothMarc DeisenrothGoogle DeepMind Chair of Machine Learning and Artificial IntelligenceUniversity College LondonBiographyProfessor Marc Deisenroth is the Google DeepMind Chair of Machine Learning and Artificial Intelligence at University College London. He is also a Research Director at Google DeepMind. Marc co-leads the Sustainability and Machine Learning Group at UCL. His research interests center around data-efficient machine learning, probabilistic modeling and autonomous decision making with applications in climate/weather science, nuclear fusion, and robotics.Marc was Program Chair of EWRL 2012, Workshops Chair of RSS 2013, EXPO Chair at ICML 2020, Tutorials Chair at NeurIPS 2021, Program Chair at ICLR 2022, and Program Chair at NeurIPS 2026. He serves on the Scientific Advisory Boards of the National Oceanography Centre as well as the United Nations University Global AI Network. He received Paper Awards at ICRA 2014, ICCAS 2016, ICML 2020, AISTATS 2021, and FAccT 2023. In 2019, Marc co-organized the Machine Learning Summer School in London.In 2018, Marc received The President’s Award for Outstanding Early Career Researcher at Imperial College. He is a recipient of a Google Faculty Research Award and a Microsoft PhD Grant.In 2018, Marc spent four months at the African Institute for Mathematical Sciences (Rwanda), where he taught a course on Foundations of Machine Learning as part of the African Masters in Machine Intelligence. He is co-author of the book Mathematics for Machine Learning, published by Cambridge University Press.Research ExpertiseMachine Learning: Data-efficient machine learning, Gaussian processes, reinforcement learning, Bayesian optimization, approximate inference, deep probabilistic models, geo-spatial modelsEnvironment and Sustainability: Data assimilation, data-driven forecasting models, renewablesRobotics and Control: Robot learning, legged locomotion, planning under uncertainty, imitation learning, adaptive control, robust control, learning control, optimal controlSignal Processing: Nonlinear state estimation, Kalman filtering, time-series modeling, dynamical systems, system identification, stochastic information processingKey PublicationsDaniel Augusto De Souza, Yuchen Zhu, Harry Jake Cunningham, Yuri Saporito, Diego Mesquita, Marc P. Deisenroth2025-12-04 Advances in Neural Information Processing Systems (NeurIPS)Infinite Neural Operators: Gaussian Processes on FunctionsA variety of infinitely wide neural architectures (e.g., dense NNs, CNNs, and transformers) induce Gaussian process (GP) priors over their outputs. These relationships provide both an accurate characterization of the prior predictive distribution and enable the use of GP machinery to improve the uncertainty quantification of deep neural networks. In this work, we extend this connection to neural operators (NOs), a class of models designed to learn mappings between function spaces. Specifically, we show conditions for when arbitrary-depth NOs with Gaussian-distributed convolution kernels converge to function-valued GPs. Based on this result, we show how to compute the covariance functions of these NO-GPs for two NO parametrizations, including the popular Fourier neural operator (FNO). With this, we compute the posteriors of these GPs in realistic scenarios. This work is an important step towards uncovering the inductive biases of current FNO architectures and opens a path to incorporate novel inductive biases for use in kernel-based operator learning methods.CiteFabian Paischer, Lukas Hauzenberger, Thomas Schmied, Benedikt Alkin, Marc P. Deisenroth, Sepp Hochreiter2025-12-04 Advances in Neural Information Processing Systems (NeurIPS)Parameter Efficient Fine-tuning via Explained Variance AdaptationFoundation models (FMs) are pre-trained on large-scale datasets and then fine-tuned for a specific downstream task. The most common fine-tuning method is to update pretrained weights via low-rank adaptation (LoRA). Existing initialization strategies for LoRA often rely on singular value decompositions (SVD) of gradients or weight matrices. However, they do not provably maximize the expected gradient signal, which is critical for fast adaptation. To this end, we introduce Explained Variance Adaptation (EVA), an initialization scheme that uses the directions capturing the most activation variance, provably maximizing the expected gradient signal and accelerating fine-tuning. EVA performs incremental SVD on minibatches of activation vectors and selects the right-singular vectors for initialization once they converged. Further, by selecting the directions that capture the most activation-variance for a given rank budget, EVA accommodates adaptive ranks that reduce the number of trainable parameters, while maintaining or improving downstream performance. We apply EVA to a variety of fine-tuning tasks as language generation and understanding, image classification, and reinforcement learning. EVA exhibits faster convergence than competitors and achieves the highest average score across a multitude of tasks per domain while reducing the number of trainable parameters through rank redistribution.CiteVignesh Gopakumar, Ander Gray, Lorenzo Zanisi, Timothy Nunn, Daniel Giles, Matt Kusner, Stanislas Pamela, Marc Peter Deisenroth2025-07-13 Proceedings of the International Conference on Machine Learning (ICML)Calibrated Physics-Informed Uncertainty QuantificationNeural PDEs have emerged as inexpensive surrogate models for numerical PDE solvers. While they offer efficient approximations, they often lack robust uncertainty quantification (UQ), limiting their practical utility. Existing UQ methods for these models typically have high computational demands and lack guarantees. We introduce a novel framework for calibrated physics-informed uncertainty quantification to address these limitations. Our approach leverages physics residual errors as a nonconformity score within a conformal prediction (CP) framework. This enables data-free, model-agnostic, and statistically guaranteed uncertainty estimates. Our framework utilises convolutional layers as finite difference stencils for gradient estimation, our framework provides inexpensive coverage bounds for the violation of conservation laws within model predictions. In our experiments, we utilise CP to obtain marginal coverage for each cell and joint coverage over the entire prediction domain of various PDEs.PDF Cite Code URLDenis Hadjivelichkov, Sicelukwanda N. T. Zwane, Marc P. Deisenroth, Lourdes Agapito, Dimitrios Kanoulas2025-05-19 Proceedings of the International Conference on Robotics and Automation (ICRA)Semantic Cross-Pose Correspondence from a Single ExampleThis article focuses on predicting how an object can be transformed to a semantically meaningful pose relative to another object, given only one or few examples. Current pose correspondence methods rely on vast 3D object datasets and do not actively consider semantic information, which limits the objects to which they can be applied. We present a novel method for learning cross-object pose correspondence. The proposed method detects interacting object parts, performs one-shot part correspondence, and uses geometric and visual-semantic features. Given one example of two objects posed relative to each other, the model can learn how to transfer the demonstrated relations to unseen object instances.CiteJoel Oskarsson, Tomas Landelius, Marc P. Deisenroth, Fredrik Lindsten2024-12-11 Advances in Neural Information Processing Systems (NeurIPS)Probabilistic Weather Forecasting with Hierarchical Graph Neural NetworksIn recent years, machine learning has established itself as a powerful tool for high-resolution weather forecasting. While most current machine learning models focus on deterministic forecasts, accurately capturing the uncertainty in the chaotic weather system calls for probabilistic modeling. We propose a probabilistic weather forecasting model called Graph-EFM, combining a flexible latent-variable formulation with the successful graph-based forecasting framework. The use of a hierarchical graph construction allows for efficient sampling of spatially coherent forecasts. Requiring only a single forward pass per time step, Graph-EFM allows for fast generation of arbitrarily large ensembles. We experiment with the model on both global and limited area forecasting. Ensemble forecasts from Graph-EFM achieve equivalent or lower errors than comparable deterministic models, with the added benefit of accurately capturing forecast uncertainty.PDF Cite CodeSee all publications Recent Publications Daniel Augusto De Souza, Yuchen Zhu, Harry Jake Cunningham, Yuri Saporito, Diego Mesquita, Marc P. Deisenroth (2025). Infinite Neural Operators: Gaussian Processes on Functions. Advances in Neural Information Processing Systems (NeurIPS).Cite Fabian Paischer, Lukas Hauzenberger, Thomas Schmied, Benedikt Alkin, Marc P. Deisenroth, Sepp Hochreiter (2025). Parameter Efficient Fine-tuning via Explained Variance Adaptation. Advances in Neural Information Processing Systems (NeurIPS).Cite Vignesh Gopakumar, Ander Gray, Lorenzo Zanisi, Timothy Nunn, Daniel Giles, Matt Kusner, Stanislas Pamela, Marc Peter Deisenroth (2025). Calibrated Physics-Informed Uncertainty Quantification. Proceedings of the International Conference on Machine Learning (ICML).PDF Cite Code URL Denis Hadjivelichkov, Sicelukwanda N. T. Zwane, Marc P. Deisenroth, Lourdes Agapito, Dimitrios Kanoulas (2025). Semantic Cross-Pose Correspondence from a Single Example. Proceedings of the International Conference on Robotics and Automation (ICRA).Cite Joel Oskarsson, Tomas Landelius, Marc P. Deisenroth, Fredrik Lindsten (2024). Probabilistic Weather Forecasting with Hierarchical Graph Neural Networks. Advances in Neural Information Processing Systems (NeurIPS).PDF Cite CodeSee all publications ContactCite × Copy Download",
  "content_length": 9937,
  "method": "requests",
  "crawl_time": "2025-12-01 13:51:05"
}