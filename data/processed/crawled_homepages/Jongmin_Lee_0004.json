{
  "name": "Jongmin Lee 0004",
  "homepage": "https://www.jmlee.kr",
  "status": "success",
  "content": "Jongmin Lee (이종민)Search this siteEmbedded FilesSkip to main contentSkip to navigationJongmin LeeI am an assistant professor in AI at Yonsei University. Before joining Yonsei, I was a postdoctoral researcher at UC Berkeley, advised by Pieter Abbeel. I received my PhD from KAIST, where I was fortunate to be advised by Kee-Eung Kim.E-mail: jongminlee [at] yonsei.ac.kr / jongmin.lee012 [at] gmail.com[Google Scholar]Education2017. 03.  - 2022. 02: PhD, School of Computing, KAIST, Korea (Advisor: Kee-Eung Kim)Thesis: Algorithms for Safe Reinforcement Learning2015. 03. - 2017. 02.: MS, School of Computing, KAIST, Korea (Advisor: Kee-Eung Kim)Thesis: Constrained Bayesian Reinforcement Learning via Approximate Linear Programming2009. 03. - 2014. 02.: BS, Department of Computer Science and Engineering, Seoul National University, KoreaExperienceMar 2025 - Current: Assistant professor in AI at Yonsei UniversityMay 2022 - Feb 2025: Postdoc at UC Berkeley (Advisor: Pieter Abbeel)Apr 2021 - Aug 2021: Research Scientist Intern at DeepMind (Host: Arthur Guez)NoticeJoin us! We are actively recruiting research interns and graduate students (M.S./Ph.D.) who are passionate about AI, sequential-decision making, and reinforcement learning. If you are interested, feel free to reach out via email with your CV and academic transcript: jongminlee [at] yonsei.ac.krPublications(*: equal contribution, ^: equal advising)International[C27] FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement LearningWoosung Kim*, Jinho Lee*, Jongmin Lee^, Byung-Jun Lee^ (*: equal contribution, ^: co-corresponding)NeurIPS 2025[C26] SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction EstimationJongmin Lee*, Meiqi Sun*, Pieter Abbeel (*: equal contribution)ICLR 2025[C25] Mitigating Covariate Shift in Behavioral Cloning via Robust Distribution Correction EstimationSeokin Seo, Byung-Jun Lee, Jongmin Lee, HyeongJoo Hwang, Hongseok Yang, Kee-Eung KimNeurIPS 2024[C24] ROIDICE: Offline Return on Investment Maximization for Efficient Decision MakingWoosung Kim*, Hayeong Lee*, Jongmin Lee^, Byung-Jun Lee^ (*: equal contribution, ^: co-corresponding)NeurIPS 2024[C23] Body Transformer: Leveraging Robot Embodiment for Policy Learning [paper] [website]Carmelo Sferrazza, Dun-Ming Huang, Fangchen Liu, Jongmin Lee, Pieter AbbeelCoRL 2024[C22] Kernel Metric Learning for In-Sample Off-Policy Evaluation of Deterministic RL Policies [paper]Haanvid Lee, Tri Wahyu Guntara, Jongmin Lee, Yung-Kyun Noh, Kee-Eung KimICLR 2024 (spotlight)[C21] AlberDICE: Addressing Out-Of-Distribution Joint Actions in Offline Multi-Agent RL via Alternating Stationary Distribution Correction Estimation [paper] [code]Daiki E. Matsunaga*, Jongmin Lee*, Jaeseok Yoon, Stefanos Leonardos, Pieter Abbeel, Kee-Eung Kim (*: equal contribution)NeurIPS 2023[C20] SafeDICE: Offline Safe Imitation Learning with Non-Preferred Demonstrations [paper]Youngsoo Jang, Geon-Hyeong Kim, Jongmin Lee, Sungryull Sohn, Byoungjip Kim, Honglak Lee, Moontae LeeNeurIPS 2023[C19] Tempo Adaptation in Non-stationary Reinforcement Learning [paper]Hyunin Lee, Yuhao Ding, Jongmin Lee, Ming Jin, Javad Lavaei, Somayeh SojoudiNeurIPS 2023[C18] LobsDICE: Offline Imitation Learning from Observation via Stationary Distribution Correction Estimation [paper]Geon-Hyeong Kim*, Jongmin Lee*, Youngsoo Jang, Hongseok Yang, Kee-Eung Kim (*: equal contribution)NeurIPS 2022 [C17] Local Metric Learning for Off-Policy Evaluation in Contextual Bandits with Continuous ActionsHaanvid Lee, Jongmin Lee, Yunseon Choi, Wonseok Jeon, Byung-Jun Lee, Yung-Kyun Noh, Kee-Eung KimNeurIPS 2022[C16] COptiDICE: Offline Constrained Reinforcement Learning via Stationary Distribution Correction Estimation [paper] [code]Jongmin Lee, Cosmin Paduraru, Daniel J. Mankowitz, Nicolas Heess, Doina Precup, Kee-Eung Kim, Arthur GuezICLR 2022 (spotlight)[C15] DemoDICE: Offline Imitation Learning with Supplementary Imperfect Demonstrations [paper] [code]Geon-Hyeong Kim, Seokin Seo, Jongmin Lee, Wonseok Jeon, HyeongJoo Hwang, Hongseok Yang, Kee-Eung KimICLR 2022[C14] GPT-Critic: Offline Reinforcement Learning for End-to-End Task-Oriented Dialogue Systems [paper]Youngsoo Jang, Jongmin Lee, Kee-Eung KimICLR 2022[C13,W4] OptiDICE: Offline Policy Optimization via Stationary Distribution Correction Estimation [paper] [code]Jongmin Lee*, Wonseok Jeon*, Byung-Jun Lee, Joelle Pineau, Kee-Eung Kim  (*: equal contribution)ICML 2021ICLR Workshop on Never-Ending RL, 2021[C12] Representation Balancing Offline Model-based Reinforcement Learning [paper] [code]Byung-Jun Lee, Jongmin Lee, Kee-Eung KimICLR 2021[C11] Monte-Carlo Planning and Learning with Language Action Value Estimates [paper] [code]Youngsoo Jang, Seokin Seo, Jongmin Lee, Kee-Eung KimICLR 2021[C10] Reinforcement Learning for Control with Multiple Frequencies [paper] [code]Jongmin Lee, Byung-Jun Lee, Kee-Eung KimNeurIPS 2020[C9] Batch Reinforcement Learning with Hyperparameter Gradients [paper] [code]Byung-Jun Lee*, Jongmin Lee*, Peter Vrancx, Dongho Kim, Kee-Eung Kim  (*: equal contribution)ICML 2020[C8] Monte-Carlo Tree Search in Continuous Action Spaces with Value Gradients [paper]Jongmin Lee, Wonseok Jeon, Geon-Hyeong Kim, Kee-Eung KimAAAI 2020[C7,W4] Bayes-Adaptive Monte-Carlo Planning and Learning for Goal-Oriented Dialogues [paper]Youngsoo Jang, Jongmin Lee, Kee-Eung KimAAAI 2020NeurIPS Workshop on Conversational AI, 2019[C6] Trust Region Sequential Variational Inference [paper]Geon-Hyeong Kim, Youngsoo Jang, Jongmin Lee, Wonseok Jeon, Hongseok Yang, and Kee-Eung KimACML 2019[C5] PyOpenDial: A Python-based Domain-Independent Toolkit for Developing Spoken Dialogue Systems with Probabilistic Rules [paper] [code]Youngsoo Jang*, Jongmin Lee*, Jaeyoung Park*, Kyeng-Hun Lee, Pierre Lison, and Kee-Eung Kim  (*: equal contribution)EMNLP 2019, System Demonstrations[C4] Monte-Carlo Tree Search for Constrained POMDPs [paper] [code]Jongmin Lee, Geon-Hyeong Kim, Pascal Poupart, and Kee-Eung KimNeurIPS 2018[W3] Monte-Carlo Tree Search for Constrained MDPs [paper]Jongmin Lee, Geon-Hyeong Kim, Pascal Poupart, and Kee-Eung KimICML Workshop on Planning and Learning (PAL-18), 2018[J1] Layered Behavior Modeling via Combining Descriptive and Prescriptive Approaches: a Case Study of Infantry Company Engagement [paper]Jang Won Bae, Junseok Lee, Do-Hyung Kim, Kanghoon Lee, Jongmin Lee, Kee-Eung Kim and Il-Chul MoonIEEE Transactions on System, Man, and Cybernetics: Systems, 2018[C3,W2] Constrained Bayesian Reinforcement Learning via Approximate Linear Programming [paper]Jongmin Lee, Youngsoo Jang, Pascal Poupart, and Kee-Eung KimIJCAI 2017Scaling-Up Reinforcement Learning Workshop at ECML PKDD (SURL), 2017[C2] Hierarchically-partitioned Gaussian Process Approximation [paper]Byung-Jun Lee, Jongmin Lee, and Kee-Eung KimAISTATS 2017[W1] Multi-View Automatic Lip-Reading using Neural Network [paper]Daehyun Lee, Jongmin Lee, and Kee-Eung KimACCV Workshop on Multi-view Lip-reading/Audio-visual Challenges, 2016[C1] Bayesian Reinforcement Learning with Behavioral Feedback [paper]Teakgyu Hong, Jongmin Lee, Kee-Eung Kim, Pedro A. Ortega, and Daniel LeeIJCAI 2016DomesticA Study on Efficient Multi-Task Offline Model-based Reinforcement LearningGeon-Hyeong Kim, Youngsoo Jang, Jongmin Lee, and Kee-Eung Kim한국소프트웨어종합학술대회, 2021A Study on Application of Efficient Lifelong Learning Algorithm to Model-based Reinforcement LearningByung-Jun Lee, Jongmin Lee, Yunseon Choi, Youngsoo Jang, and Kee-Eung Kim한국소프트웨어종합학술대회, 2020A Study on Monte-Carlo Tree Search in Continuous Action SpacesJongmin Lee, Geon-Hyeong Kim, and Kee-Eung Kim한국통신학회 하계종합학술발표회 논문집, 2019Case Studies on Planning and Learning for Large-Scale CGFs with POMDPs through Counterfire and Mechanized Infantry ScenariosJongmin Lee, Jungpyo Hong, Jaeyoung Park, Kanghoon Lee, Kee-Eung Kim, Il-Chul Moon, and Jae-Hyun ParkKIISE Transactions on Computing Practices, 2017A Case Study on Planning and Learning for Large-Scale CGFs with POMDPsJungpyo Hong, Jongmin Lee, Kanghoon Lee, Sanggyu Han, Kee-Eung Kim, Il-Chul Moon, and Jae-Hyeon Park한국정보과학회 학술발표논문집, 2016Awards and HonorsOutstanding Ph.D. Thesis Award, School of Computing at KAIST, 2022Qualcomm-KAIST Innovation Awards - Paper Competition, Qualcomm, 2019Society of Global Ph.D. Fellows Outstanding Presentation Award, 5th SGPF Annual Conference, 2018Global Ph.D. Fellowship, National Research Foundation of Korea, 2018 ~ 2020Naver Ph.D. Fellowship, NAVER, 2017ReviewerNeurIPS (2016, 2018, 2019, 2020, 2021, 2022)ICML (2019, 2020,2021, 2022)AAAI (2020,2021, 2022)ICLR (2020,2021, 2022)IJCAI (2021, 2022)ACML (2017, 2019, 2021)Machine Learning Journal (2017, 2019)Journal of Artificial Intelligence Research (2019)Transactions on Machine Learning Research (2022)Teaching ExperiencesKAIST-Samsung AI Expert Program: Introduction to Reinforcement Learning & Deep Reinforcement Learning TA, KAIST, 2020KAIST-Samsung AI Expert Program: Introduction to Tensorflow & Reinforcement Learning, TA, KAIST, 2019Data Structure (CS206), TA, KAIST, 2019Introduction to Programming (CS101), Head TA, KAIST, 2018Artificial Intelligence and Machine Learning (CS570), TA,  KAIST, Spring 2016Artificial Intelligence and Machine Learning, TA, KMOOC, Fall 2015Peer group seminar: Agile web development for non-majors (009.032), Seoul National University,  Fall 2012Extracurricular ActivitiesStudent Representative of School of Computing, KAIST, 2017. 1 ~ 2017. 12Vice Student Representative of School of Computing, KAIST,  2015. 3 ~ 2016. 2Google SitesReport abuseGoogle SitesReport abuse",
  "content_length": 9596,
  "method": "requests",
  "crawl_time": "2025-12-01 13:34:12"
}