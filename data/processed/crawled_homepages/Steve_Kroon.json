{
  "name": "Steve Kroon",
  "homepage": "http://www.cs.sun.ac.za/~kroon",
  "status": "success",
  "content": "Steve Kroon Steve Kroon Associate Professor: Computer Science, Stellenbosch University. My personal web pages are available here. Bio Bio last updated: July 2022 Prof. Steve Kroon obtained MCom (Computer Science) and PhD (Mathematical Statistics) degrees from Stellenbosch University, and joined the Stellenbosch University Computer Science department in 2008. He holds a C2 rating from South Africa's National Research Foundation. His PhD thesis considered aspects of statistical learning theory, and his current research interests include generative modeling, Bayesian methods, search and adversarial search, decision-making and planning under uncertainty, and machine learning. He has supervised and co-supervised 2 graduated and 1 current PhD students, 17 graduated and 2 current master's students, and has published 10 journal articles and 19 peer-reviewed conference and workshop articles. He has served as a reviewer/is on the programme committee for ICML, NeurIPS, JAIR, Algorithmica and JUCS. He holds a Diploma in Actuarial Techniques, and is a member of the Centre for Artificial Intelligence Research and an individual associate of the National Institute of Theoretical and Computational Sciences. Please note: The information below is unfortunately dated. I will try to update when I get time. Curriculum Vitae (last updated: 25 April 2019) Research Google Scholar profile Main research topics My research group primarily considers the topics below. Generally, I am more interested in understanding existing approaches and proposing novel approaches in these areas, rather than applications in these areas. Generative modeling Bayesian methods Neural networks Computational intelligence in games Potential students If you're interested in studying under my supervision, please contact me after carefully considering my notes for prospective graduate students. I generally only accept full-time students who have their own source of funding, unless I am specifically advertising a funded position. Research output Also see the page of our Decision-making research group. TABIA: A language to transform, combine, and reason over semi-structured tabular data (with Dirko Coetsee, McElory Hoffmann and Luc De Raedt), Proceedings of the 28th European Conference on Artificial Intelligence (ECAI 2025). IOS Press, 2025 (accepted). Proposes a representation and language for manipulating spreadsheets and other tabular data. Making Superhuman AI More Human in Chess (with Daniel Barrish and Brink van der Merwe), Advances in Computer Games (ACG) 2023 (held online). Part of the Lecture Notes in Computer Science book series (LNCS, volume 14528). Work on creating chess agents that more faithfully imitate strong human players. Topological Dynamics of Functional Neural Network Graphs During Reinforcement Learning (with Martin Muller and Stephan Chalup), International Conference on Neural Information Processing (ICONIP) 2023. An investigation of topological structures present in graphs derived from the activation patterns in the neural networks obtained when training reinforcement learning agents. Integrating Bayesian Network Structure into Residual Flows and Variational Autoencoders (with Jacobie Mouton). Transactions on Machine Learning Research. Combines Jacobie's two 2022 ICLR workshop papers with some additional work from her Master's thesis. SIReN-VAE: Leveraging Flows and Amortized Inference for Bayesian Networks (with Jacobie Mouton). 2022 ICLR Workshop on Deep Generative Models for Highly Structured Data. Jacobie's poster on this work also won a book prize at the 2022 Deep Learning Indaba. (ICLR workshop poster| Indaba poster) Graphical Residual Flows (with Jacobie Mouton). 2022 ICLR Workshop on Deep Generative Models for Highly Structured Data. (Poster) SplyCI: Integrating Spreadsheets by Recognising and Solving Layout Constraints (with Dirko Coetsee, McElory Hoffmann, and Luc de Raedt), 19th Symposium on Intelligent Data Analysis (IDA 2021), Porto, Portugal. Part of the Lecture Notes in Computer Science book series (LNCS, volume 12695); also part of the Information Systems and Applications, incl. Internet/Web, and HCI book sub series (LNISA, volume 12695). (Download link from KU Leuven.) Also accepted for oral and poster presentation at the 2021 ECMLPKDD Workshop on Automating Data Science (ADS2021). SplyCI is an early prototype for automatic merging of spreadsheets; Dirko wrote this blog post on the article. If Dropout Limits Trainable Depth, Does Critical Initialisation Still Matter? A Large-scale Statistical Analysis on ReLU Networks (with Arnu Pretorius, Elanvan Biljon, Benjamin van Niekerk, Ryan Eloff, Matthew Reynard, Steve James, Benjamin Rosman, and Herman Kamper), Pattern Recognition Letters, Volume 138, October 2020, pages 95--105. (Elsevier Sharelink valid until 17 September 2020.)) Signal propagation theory establishes a limit on training depth for ReLU networks in the presence of noise regularization. Here, we employ an experimental design inspired by randomized controlled trials; our empirical analysis compares the critical initialization strategy to other strategies that are viable given the depth limit resulting from noise regularization. On the Expected Behaviour of Noise Regularized Deep Neural Networks as Gaussian Processes (with Arnu Pretorius and Herman Kamper), Pattern Recognition Letters, Volume 138, October 2020, pages 75--81. (Elsevier Sharelink valid until end of August 2020.) This paper considers the impact of noise regularization (e.g. dropout) on the neural network Gaussian processes arising as the infinite-width limit of deep neural networks. Performance-Agnostic Fusion of Probabilistic Classifier Outputs (with Jordan Masakuna and Simukai Utete), 23rd International Conference on Information Fusion (FUSION 2020). This article presents a novel approach to once-off fusion of disparate classifier predictions to obtain a single consensus prediction. (Code repository) Stochastic Gradient Annealed Importance Sampling for Efficient Online Marginal Likelihood Estimation (with Scott Cameron and Hans Eggers), Entropy, Volume 21(11) (2019). (This was a special issue for extended versions of selected papers at MaxEnt 2019.) Proposes a new evidence estimation technique which can take advantage of mini-batching. This yields an online evidence estimation technique which is considerably faster than nested sampling and regular annealed importance sampling. (Version on arxiv.) Update (24 July 2020): Post-publication communication with Sam Power revealed that part of the approach used in this paper (and the original MaxEnt paper) corresponds closely to Nicolas Chopin's iterated batch importance sampling technique from 2002. Stabilising priors for robust Bayesian deep learning (with Felix McGregor, Arnu Pretorius and Johan du Preez). Extended abstract accepted for poster presentation at the NeurIPS 2019 Bayesian Deep Learning workshop in Vancouver, Canada. A Sequential Marginal Likelihood Approximation using Stochastic Gradients (with Scott Cameron and Hans Eggers), Proceedings, Volume 33(1) (2019). Paper accepted for poster presentation at MaxEnt 2019 in Garching, Germany, where it won second prize in the poster competition. Proposes a new evidence estimation technique based on a sequential decomposition of data, and using stochastic gradient MCMC approaches to estimate the factors required. This idea was extended and improved in \"Stochastic Gradient Annealed Importance Sampling for Efficient Online Marginal Likelihood Estimation\". Dropout Initialization (with Arnu Pretorius, Elan van Biljon, Ryan Eloff, Matthew Rynard, Benjamin van Niekerk, Steven James, Benjamin Rosman, and Herman Kamper). Poster presentation by Elan van Biljon at 2019 Deep Learning IndabaX South Africa, Durban A Coordinated Search Strategy for Solitary Robots (with Jordan Masakuna and Simukai Utete), Third IEEE International Conference on Robotic Computing (IRC), 2019, Naples. This paper proposes a strategy based on cellular decomposition of an unknown region for coordinating solitary robots, i.e. robots that mostly function alone, but interact with others when serendipitous encounters arise. The aim is for the strategy to reduce redundant search to enhance the search effectiveness of a group of such solitary robots. An extended version with more detail is available on the arxiv. (Code repository) Critical Initialisation for Deep Signal Propagation in Noisy Rectifier Neural Networks (with Arnu Pretorius, Elan van Biljon and Herman Kamper), Thirty-second Conference on Neural Information Processing Systems (NIPS), 2018, Montreal. (Accepted for poster presentation.) Poster based on preliminary version of this work, \"Deep signal propagation for noisy rectifier neural networks\", also presented by Arnu and Elan at the 2nd Deep Learning Indaba 2018 (Arnu was awarded US$1000 in Google Compute credits, while Elan was awarded an NVidia Titan V GPU for this poster). Poster also presented at Data, Learning and Inference (DALI), 2019, George, as well as the 2019 Machine Learning Summer School in Stellenbosch. Variational Autoencoders for Missing Data Imputation with Application to a Simulated Milling Circuit (with John McCoy and Lidia Auret), 5th IFAC Workshop on Mining, Mineral and Metal Processing, 2018, Shanghai. IFAC-Papersonline, Vol. 51 (21), pp. 141-146, 2018. (DOI: 10.1016/j.ifacol.2018.09.406). Poster based on this work presented by John at the 2nd Deep Learning Indaba 2018 (awarded US$1000 in Google Compute credits for the poster). Learning Dynamics of Linear Denoising Autoencoders (with Arnu Pretorius and Herman Kamper), 2018 International Conference on Machine Learning, 2018, Stockholm. (Oral presentation and poster. Link to paper and supplementary material on PMLR.) Preliminary work towards this presented as a poster, \"Learning dynamics of regularised linear neural networks\", by Arnu at the 1st Deep Learning Indaba 2017 - this poster won a book prize. No evidence ",
  "content_length": 21995,
  "method": "requests",
  "crawl_time": "2025-12-01 14:32:24"
}