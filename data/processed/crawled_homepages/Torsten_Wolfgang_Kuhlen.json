{
  "name": "Torsten Wolfgang Kuhlen",
  "homepage": "http://www.vr.rwth-aachen.de/person/1",
  "status": "success",
  "content": "Prof. Dr. Torsten Wolfgang Kuhlen - Virtual Reality and Immersive Visualization Home Contact Staff Research Publications Software Teaching Theses Jobs Hardware &Infrastructure Help our Research Services Impressum Profile Prof. Dr. Torsten Wolfgang Kuhlen Room K107 Phone: +49 241 80 24783 Fax: +49 241 80 22134 Email: kuhlen@vr.rwth-aachen.de Publications Audiovisual angle and voice incongruence do not affect audiovisual verbal short-term memory in virtual reality Cosima A. Ermert, Manuj Yadav, Jonathan Ehret, Chinthusa Mohanathasan, Andrea Bönsch, Torsten Wolfgang Kuhlen, Sabine Janina Schlittmeier, Janina Fels PLOS ONE Virtual reality (VR) environments are frequently used in auditory and cognitive research to imitate real-life scenarios. The visual component in VR has the potential to affect how auditory information is processed, especially if incongruences between the visual and auditory information occur. This study investigated how audiovisual incongruence in VR implemented with a head-mounted display (HMD) affects verbal short-term memory compared to presentation of the same material over traditional computer monitors. Two experiments were conducted with both these display devices and two types of audiovisual incongruences: angle (Exp 1) and voice (Exp 2) incongruence. To quantify short-term memory, an audiovisual verbal serial recall (avVSR) task was developed where an embodied conversational agent (ECA) was animated to speak a digit sequence, which participants had to remember. The results showed no effect of the display devices on the proportion of correctly recalled digits overall, although subjective evaluations showed a higher sense of presence in the HMD condition. For the extreme conditions of angle incongruence in the computer monitor presentation, the proportion of correctly recalled digits increased marginally, presumably due to raised attention, but the effect size was negligible. Response times were not affected by incongruences in either display device across both experiments. These findings suggest that at least for the conditions studied here, the avVSR task is robust against angle and voice audiovisual incongruences in both HMD and computer monitor displays. » Show BibTeX @article{ Ermert2025, doi = {10.1371/journal.pone.0330693}, author = {Ermert, Cosima A. AND Yadav, Manuj AND Ehret, Jonathan ANDMohanathasan, Chinthusa AND Bönsch, Andrea AND Kuhlen, Torsten W. ANDSchlittmeier, Sabine J. AND Fels, Janina}, journal = {PLOS ONE}, publisher = {Public Library of Science}, title = {Audiovisual angle and voice incongruence do not affectaudiovisual verbal short-term memory in virtual reality}, year = {2025}, month = {08}, volume = {20}, url = {https://doi.org/10.1371/journal.pone.0330693}, pages = {1-23}, number = {8},} Downloads: DOI 10.1371/journal.pone.0330693 Poster: Listening Effort In Populated Audiovisual Scenes Under Plausible Room Acoustic Conditions Cosima A. Ermert, Karin Loh, Karl Baylan, Konstantin Kühlem, Andrea Bönsch, Torsten Wolfgang Kuhlen, Janina Fels International Symposium on Auditory and Audiological Research (ISAAR) 2025 Listening effort in real-world environments is shaped by a complex interplay of factors, including time-varying background noise, visual and acoustic cues from both interlocutors and distractors, and the acoustic properties of the surrounding space. However, many studies investigating listening effort neglect both auditory and visual fidelity: static background noise is frequently used to avoid variability, talker visualization often disregards acoustic complexity, and experiments are commonly conducted in free-field environments without spatialized sound or realistic room acoustics. These limitations risk undermining the ecological validity of study outcomes. To address this, we developed an audiovisual virtual reality (VR) framework capable of rendering immersive, realistic scenes that integrate dynamic auditory and visual cues. Background noise included time-varying speech and non-speech sounds (e.g., conversations, appliances, traffic), spatialized in controlled acoustic environments. Participants were immersed in a visually rich VR setting populated with animated virtual agents. Listening effort was assessed using a heard-text-recall paradigm embedded in a dual-task design: participants listened to and remembered short stories told by two embodied conversational agents while simultaneously performing a vibrotactile secondary task. We compared three room acoustic conditions: a free-field environment, a room optimized for reverberation time, and an untreated reverberant room. Preliminary results from 30 participants (15 female; age range: 18–33; M = 25.1, SD = 3.05) indicated that room acoustics significantly affected both listening effort and short-term memory performance, with notable differences between free-field and reverberant conditions. These findings underscore the importance of realistic acoustic environments when investigating listening behavior in immersive audiovisual settings. Interactive Streaming of 3D Scenes to Mobile Devices using Dual-Layer Image Warping and Loop-based Depth Reconstruction Jens Koenen, Simon Oehrl, Torsten Wolfgang Kuhlen, Tim Gerrits 2025 International Conferences in Central Europe on Computer Graphics, Visualization and Computer Vision (WSCG 2025) While mobile devices have developed into hardware with advanced capabilities for rendering 3D gra-phics, they commonly lack the computational power to render large 3D scenes with complex lighting interactively. A prominent approach to tackle this is rendering required views on a remote server and streaming them to the mobile client. However, the rate at which servers can supply data is limited, e.g., by the available network speed, requiring image-based rendering techniques like image warping to compensate for the latency and allow a smooth user experience, especially in scenes where rapid user movement is essential. In this paper, we present a novel streaming approach designed to minimize arti-facts during the warping process by including an additional visibility layer that keeps track of occluded surfaces while allowing access to 360° views. In addition, we propose a novel mesh generation techni-que based on the detection of loops to reliably create a mesh that encodes the depth information requi-red for the image warping process. We demonstrate our approach in a number of complex scenes and compare it against existing works using two layers and one layer alone. The results indicate a significant reduction in computation time while achieving comparable or even better visual results when using our dual-layer approach. » Show BibTeX @ARTICLE{Koenen-2025-A05,author={Koenen,J. and Oehrl,S. and Kuhlen,T. and Gerrits,T.},title={Interactive Streaming of 3D Scenes to Mobile Devices using Dual-Layer Image Warping and Loop-based Depth Reconstruction},journal={Journal of WSCG},year={2025},volume={33},number = {1},pages={1-10},doi={10.24132/JWSCG.2025-1},publisher={Union Agency, Science Press},issn={1213-6972},document_type={Article},} Downloads: DOI 10.24132/JWSCG.2025-1 Exploring Gaze Dynamics: Initial Findings on the Role of Listening Bystanders in Conversational Interactions Jonathan Ehret, Valentin Dasbach, Jan-Nikjas Hartmann, Janina Fels, Torsten Wolfgang Kuhlen, Andrea Bönsch 2025 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW) - VHCIE This work-in-progress paper investigates how virtual listening bystanders influence participants’ gaze behavior and their perception of turn-taking during scripted conversations with embodied conversational agents (ECAs). 25 participants interacted with five ECAs – two speakers and three bystanders – across three conditions: no bystanders, bystanders exhibiting random gazing behavior, and social bystanders engaging in mutual gaze and backchanneling. Participants either observed the conversation or actively participated as speakers by reciting prompted sentences. The results indicated that bystanders reduced the participants’ attention to speakers, hindering their ability to anticipate turn changes and resulting in longer delays in shifting their gaze to the new speaker after an ECA yielded the turn. Random gazing bystanders were particularly noted for obscuring conversational flow. These findings underscore the challenges of designing effective and natural conversational environments, highlighting the need for careful consideration of ECA behaviors to enhance user engagement. » Show BibTeX @INPROCEEDINGS{Ehret2025, author={Ehret, Jonathan and Dasbach, Valentin and Hartmann, Jan-Nikjas andFels, Janina and Kuhlen, Torsten W. and Bönsch, Andrea}, booktitle={2025 IEEE Conference on Virtual Reality and 3D User InterfacesAbstracts and Workshops (VRW)}, title={Exploring Gaze Dynamics: Initial Findings on the Role of ListeningBystanders in Conversational Interactions}, year={2025}, volume={}, number={}, pages={748-752}, doi={10.1109/VRW66409.2025.00151}} Downloads: PDF DOI 10.1109/VRW66409.2025.00151 Geschichte(n) in Virtual Reality - Perspektiven der Informatik Torsten Wolfgang Kuhlen In Christian Kuchler und Kristopher Muckel (eds): Virtual Reality - Zukunft der Historischen Bildung? Wallstein Verlag, Göttingen, ISBN 978-3-8353-5826-3, pp. 54-63 Den Bau der Pyramiden von Gizeh beobachten – und dann gleich weiter ins antike Rom? Das und mehr soll mit Virtual Reality möglich werden. Doch was macht das mit unserem Verständnis von Geschichte? Virtual-Reality-Anwendungen mit historischem Inhalt haben Konjunktur. Sie versprechen virtuelle Zeitreisen und die Möglichkeit, endlich zeigen zu können, wie die Vergangenheit wirklich war. Daraus resultieren Formen des Umgangs mit Geschichte, die nicht nur die außerschulische Geschichtskultur und -vermittlung prägen, sondern auch zunehmend in den Geschichtsunterricht hineinwirken. Im Zentrum dieses Bandes steht daher die Frage: Was macht Virtual Reality mit Geschichte? Währ",
  "content_length": 364983,
  "method": "requests",
  "crawl_time": "2025-12-01 14:40:43"
}