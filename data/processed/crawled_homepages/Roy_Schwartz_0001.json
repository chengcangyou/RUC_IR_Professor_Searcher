{
  "name": "Roy Schwartz 0001",
  "homepage": "https://schwartz-lab-huji.github.io",
  "status": "success",
  "content": "Home Search Professor of Natural Language Processing The Hebrew University of Jerusalem Schwartz lab Roy Schwartz's lab at the School of Computer Science and Engineering at the The Hebrew University of Jerusalem studies Natural Language Processing (NLP). Our research is driven towards making text understanding technology widely accessible—to doctors, to teachers, to researchers or even to curious teenagers. To be broadly adopted, NLP technology needs to not only be accurate, but also reliable; models should provide explanations for their outputs; and the methods we use to evaluate them need to be convincing. Our lab also studies methods to make NLP technology more efficient and green, in order to decrease the environmental impact of the field, as well as lower the cost of AI research in order to broaden participation in it. Lab News Congrats to Tamer for successfully defending his Master’s thesis! Sep 2025 Congrats to Tamer and Micael for getting their SSM Pruning paper accepted to EMNLP 2025, and to Yuval and Guy for getting their Vocab diet paper accepted to Findings of EMNLP 2025! Aug 2025 Congrats to Guy for successfully defending his Master’s thesis! Jul 2025 Excited to contribute to the AI Environmental Impacts Act by Senators Markey and Heinrich! Feb 2024 An awesome lab event in Nahal Halilim! Nov 2022 Projects Biases in Datasets We analyze the datasets on which NLP models are trained. Looking carefully into these datasets, we uncover limitations and biases in the data collection process as well as the evaluation process. Our findings indicate that the recent success of neural models on many NLP tasks has been overestimated, and pave the way for the development of more reliable methods of evaluation. Green NLP The computations required for deep learning research have been doubling every few months. These computations have a surprisingly large carbon footprint. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research. Our lab studies tools to make NLP technology more efficient, and to enhance the reporting of computational budgets. Multimodality Humans learn about the world using input from multiple modalities. Machines can also leverage other modalities in order to improve their textual understanding. Our lab studies methods for combining textual information with data from images, sounds, videos and others, with the goal of making them more robust and allowing them to generalize better. Understanding NLP In recent years, deep learning became the leading machine learning technology in NLP. Despite its wide adoption in NLP, the theory of deep learning lags behind its empirical success, as many engineered systems are in commercial use without a solid scientific basis for their operation. Our research aims to bridge the gap between theory and practice. We devise mathematical theories that link deep neural models to classical NLP models, such as weighted finite-state automata. Recent Publications Quickly discover relevant content by filtering publications. Yuval Reif, Guy Kaplan, Roy Schwartz October 2025 arXiv:2510.17001 Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic Large language models (LLMs) were shown to encode word form variations, such as \"walk\"->\"walked\", as linear directions in embedding space. However, standard tokenization algorithms treat these variations as distinct tokens – filling the size-capped vocabulary with surface form variants (e.g., \"walk\", \"walking\", \"Walk\"), at the expense of less frequent words and multilingual coverage. We show that many of these variations can be captured by transformation vectors – additive offsets that yield the appropriate word's representation when applied to the base form word embedding – in both the input and output spaces. Building on this, we propose a compact reshaping of the vocabulary: rather than assigning unique tokens to each surface form, we compose them from shared base form and transformation vectors (e.g., \"walked\" = \"walk\" + past tense). We apply our approach to multiple LLMs and across five languages, removing up to 10% of vocabulary entries – thereby freeing space to allocate new, more diverse tokens. Importantly, we do so while also expanding vocabulary coverage to out-of-vocabulary words, with minimal impact on downstream performance, and without modifying model weights. Our findings motivate a foundational rethinking of vocabulary design, moving from string enumeration to a compositional vocabulary that leverages the underlying structure of language. PDF Cite Project Yufeng Du, Minyang Tian, Srikanth Ronanki, Subendhu Rongali, Sravan Bodapati, Aram Galstyan, Azton Wells, Roy Schwartz, Eliu A Huerta, Hao Peng October 2025 In Findings of EMNLP 2025 Context Length Alone Hurts LLM Performance Despite Perfect Retrieval Large language models (LLMs) often fail to scale their performance on long-context tasks performance in line with the context lengths they support. This gap is commonly attributed to retrieval failures – the models' inability to identify relevant information in the long inputs. Accordingly, recent efforts often focus on evaluating and improving LLMs' retrieval performance: if retrieval is perfect, a model should, in principle, perform just as well on a long input as it does on a short one – or should it? This paper presents findings that the answer to this question may be negative. Our systematic experiments across 5 open- and closed-source LLMs on math, question answering, and coding tasks reveal that, even when models can perfectly retrieve all relevant information, their performance still degrades substantially (13.9%–85%) as input length increases but remains well within the models' claimed lengths. This failure occurs even when the irrelevant tokens are replaced with minimally distracting whitespace, and, more surprisingly, when they are all masked and the models are forced to attend only to the relevant tokens. A similar performance drop is observed when all relevant evidence is placed immediately before the question. Our findings reveal a previously-unrealized limitation: the sheer length of the input alone can hurt LLM performance, independent of retrieval quality and without any distraction. They motivate our simple, model-agnostic mitigation strategy that transforms a long-context task into a short-context one by prompting the model to recite the retrieved evidence before attempting to solve the problem. On RULER, we observe a consistent improvement of GPT-4o up to 4% on an already strong baseline. PDF Cite Federico Marcuzzi, Xuefei Ning, Roy Schwartz, Iryna Gurevych September 2025 arXiv:2508.18088 How Quantization Shapes Bias in Large Language Models This work presents a comprehensive evaluation of how quantization affects model bias, with particular attention to its impact on individual demographic subgroups. We focus on weight and activation quantization strategies and examine their effects across a broad range of bias types, including stereotypes, toxicity, sentiment, and fairness. We employ both probabilistic and generated text-based metrics across nine benchmarks and evaluate models varying in architecture family and reasoning ability. Our findings show that quantization has a nuanced impact on bias: while it can reduce model toxicity and does not significantly impact sentiment, it tends to slightly increase stereotypes and unfairness in generative tasks, especially under aggressive compression. These trends are generally consistent across demographic categories and model types, although their magnitude depends on the specific setting. Overall, our results highlight the importance of carefully balancing efficiency and ethical considerations when applying quantization in practice. PDF Cite Amit Ben Artzy, Roy Schwartz July 2025 arXiv:2507.16323 SpeLLM: Character-Level Multi-Head Decoding Scaling LLM vocabulary is often used to reduce input sequence length and alleviate attention's quadratic cost. Yet, current LLM architectures impose a critical bottleneck to this procedure: the output projection layer scales linearly with vocabulary size, rendering substantial expansion impractical. We propose SpeLLM, a method that decouples input and output vocabularies by predicting character-level strings through multiple output heads. In SpeLLM, each of the k linear heads predicts a single character simultaneously, enabling the model to represent a much larger output space using smaller, independent linear heads. We present a self-distillation approach for converting a standard LLM to a SpeLLM. Our experiments with four pre-trained LLMs show their SpeLLM variants achieve competitive performance on downstream tasks while reducing runtime by 5.1% on average across models. Our approach provides a potential avenue for reducing LLM costs, while increasing support for underrepresented languages and domains. PDF Cite Code Michael Hassid, Gabriel Synnaeve, Yossi Adi, Roy Schwartz May 2025 arXiv:2505.17813 Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive \"thinking\" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge the assumption that long thinking chains results in better reasoning capabilities. We first demonstrate that shorter reasoning chains within individual questions are significantly more likely to yield correct answers - up to 34.5% more accurate than the longest chain sampled for the same question. Based on these results, we suggest short-m@k, a novel reasoning LLM inference method. Our method executes k independent generations in parallel and halts computation once the first m thinking processes are done.",
  "content_length": 11082,
  "method": "requests",
  "crawl_time": "2025-12-01 14:21:11"
}