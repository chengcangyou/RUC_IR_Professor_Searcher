{
  "name": "Kenneth P. Birman",
  "homepage": "http://www.cs.cornell.edu/ken",
  "status": "success",
  "content": "Professor Ken Birman Kenneth P. Birman N. Rama Rao Professor of Computer Science 435 Gates Hall, Cornell University Ithaca, New York 14853 W: 607-255-9199; M: 607-227-0894; F: 607-255-9143 Email: ken@cs.cornell.edu CV: Jun 2025 Breaking News. Fall 2025 will be my last semester teaching, and while I plan to run my group until all the current PhD and MS students wrap up, I am no longer recruiting new students. This decision is personal... Cornell has been the right place for me, and would remain so going forward were it not for the lure of family out in Seattle. In addition to continuing my Cornell research, I'm also doing some consulting for Microsoft's new Copilot Tuning product group. Current Research (full publications list). Cascade, Vortex. This pair of systems is my current main focus, although the effort splits between work on Vortex (which is new), Cascade itself (closer to finished) and Derecho, described below.  Cascade centers on the observation that data movement is a huge overhead in modern AI and ML applications.  How can we run these systems at \"peak possible speed\" if we have this data movement barrier?    Cutting to the chase, Cascade is often 5x, 10x, and sometimes 20x or 100x faster than other platforms when running identical AI logic!  We gain these huge speedups through a few innovations.Vortex extends Cascade to include a bunch of specialized features in support of RAG LLM systems where vector databases are a major component (these need to support approximate search). We haven't published anything on Vortex yet. We maintain a full project web site here, and our GitHub site is here. I'll limit myself to a summary on this page, but you can find links to some papers at the bottom of the Cascade project page and also on our publications list.One is to offer ways to move the user's AI or ML code into our storage server, so that the code runs right where data is located and can access objects via pointers with no copying needed.  A second idea is to use a mixture of scheduling and planned placement for computing (and for objects created during computation), so that when a computation is needed, the data it requires is collocated at the node where we schedule the AI program to run.  This pays off because the models used by AI programs (ML training results in big parameter vectors called model objects) can be enormous.  The win is even larger for applications doing computer vision, because photos and videos are huge, too.  One puzzle seen with this example is that it departs from the widely prevalent cloud computing model in so many ways.  Yet there are reasons to believe that applications of this inevitably depart from cloud computing as we do it today.  For example,  in settings where the AI or ML will access sensitive data, it is often important to do that \"close\" to where the data is gathered and then to either discard the data after computing without ever storing it, or store it only where the user \"lives\". The European focus on privacy could mandate this architecture... and the cloud isn't very well-prepared for it today.  Cascade could offer an answer.  A second consideration centers on the wide-area internet link needed to upload data from a camera to a service on a cloud: often, uploads of this kind can be the slowest step.  We solve these problems by running Cascade directly on hardware close to the edge: a cluster of computers that might sit right in a hospital computing center, or on a factory floor, or in an airplane servicing center. Then we can also leverage shared memory to reduce data movement between the user's logic and the Cascade data storage layer, and leverage hardware accelerated communication for node-to-node communication.  That last idea uses Derecho, discussed below.  And we do a lot of work on scheduling, to ensure that compute and storage tend to be collocated on the same nodes.When people talk about storage, it is common for them to mean \"in a scalable file system\".  Cascade is very flexible in this sense.  It can be used as a file system through POSIX file system APIs, but can also be used as a key-value storage layer (like MemCached), or treated like a pub-sub system (similar to Kafka).  In fact our APIs are often identical to standard tools in those different areas.  Use whichever storage abstraction layer you prefer!When configured this way edge cameras can be connected directly to the same machines where the user's AI logic is running.  But we also want Cascade to look very transparent to the AI designer: platforms like PyTorch, Tensor Flow, Julia, Apache Spark/Databricks, MXNET and so forth are very popular, and we want to be fully compatible with them.  That leads to the view that Cascade should have a second hosting option, as a service on a normal cloud, able to run the user's AI and ML through a function (lambda) model, or in containers.  In work we hope to do during 2024, we'll connect these two options into a single service that would be perceived as a cloud service and yet might manage resources right on the cloud edge. Read a paper about Cascade here. Or check out my slide deck here. A Vortex-centric slide deck is here.  By the way, this first link is not yet a published paper: we do have a bunch of papers in the publication pipeline, but are only just starting to see them come out.Students interested in joining the Cascade or Vortex effort should reach out to me directly. Derecho. Cascade is actually built using Derecho, a project that was very active from 2017 through 2019, but continues at a lower pace today (notably the DCCL work mentioned below).  We maintain a full project web site here, and our GitHub site is here. I'll limit myself to a summary on this page.Derecho looks at ways of leveraging remote DMA (RDMA) technologies to move large data objects at wire speeds, and modern storage technologies to persist data.  Recently we ported Derecho to run over DPDK too (a pure software solution... not quite as fast, but we still set records), and we support normal TCP as well (slowest of all).   Learn more from our ACM TOCS paper here and our two DSN papers, here and here.  A paper on optimizations for small objects based on a methodology called Spindle is here. Our newest work on Derecho centers on an implementation of the Collective Communication Library (MapReduce/AllReduce) APIs, sometimes called the CCLs. Weijia Song completed a Derecho CCL (DCCL) and it substantially outperforms alternatives, notably beating the Open MPI CCL \"in its own home stadium\", namely on clusters configured as HPC systems!  We get as much as a 2x speedup for AllReduce, for example.  Weijia has not yet written the work up, but you can already use it in the most current Derecho release. We plan to do a deep integration of DCCL into Cascade soon.Same comment applies here: if you are a student with a distributed systems, networking or \"low level\" focus, the Derecho work could be a great opportunity for you to pursue your passion while being relevant to the modern AI-centric world.  And we have lots of opportunities for pushing the work forward.  Some center on a mix of PL, verification and theory, while others are very practical. Again, reach out to Ken. Derecho Secure Audit Log / BlockChain. Edward Tremel is extending Derecho to include a novel BFT layer over the object store.  It could be used much like a permissioned BlockChain.  Details soon... but I should note that Edward leads on this and is now a faculty member at University of Augusta.  We are collaborating on this work, but he is the person with the real vision on where to take it. Using all of this technology for IoT applications, notably in the smart power grid, healthcare, and industrial settings (IIoT). My group generally has application areas in mind, and in recent years the bulk electric power grid has been a rich source of ideas.  We've also been branching out and thinking about other kinds of environments that are rich in sensors and actuators, such as healthcare and industrial automation (sometimes called \"digital twin\" systems).   PhD students Alicia Yang, Tiancheng Yuan and Yifan Wang are leading this work, in collaboration with Siemens Corporate Research and in a smart farming setting (a dairy). Teaching: I teach two courses, in the fall only: CS4414 and CS5416. The first is cs4414: Systems Programming. This course introduces students to the challenges of developing and optimizing systems software on modern Linux platforms, using C++ and other tools. The second is cs5416: This course, which shares some lectures with cs4414 but has its own large project and recitation lectures, looks at Cloud and ML Systems Programming. Video links: I keep some videos and pptx files about our work here. SOSP '15 History Day talk on fault-tolerance and consistency, the CATOCS controversy, and the modern-day CAP conjecture.  My video is here and an accompanying essay is here. Robbert van Renesse and me discussing how we got into this area of research: here. My Textbook (last revised in 2012): Guide to Reliable Distributed Systems: Building High-Assurance Applications and Cloud-Hosted Services.   Click here to get to my cloud computing course, which has slide sets and other materials that include some lectures strongly tied to content from the book.  You are welcome to use these in your own courses if you like. The 2018 slide set is quite new and was one of the outcomes of my 2016-2017 sabbatical during which I visited widely and hopefully, came home with an updated appreciation of the contemporary perspectives seen in industry. But this means that by now, I've departed significantly from the treatment in the book; earlier slide sets that are closer to the book treatment can be found in http://www.cs.cornell.edu/courses/cs5412/XXXXsp, where XXXX would be the year. There was no 2013 or 2017 offering. The bad news is that the material evolves at a breathtaking pace, which is why I keep revising the slides. Natually, this also m",
  "content_length": 16740,
  "method": "requests",
  "crawl_time": "2025-12-01 13:41:49"
}