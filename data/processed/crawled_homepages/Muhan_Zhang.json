{
  "name": "Muhan Zhang",
  "homepage": "https://muhanzhang.github.io",
  "status": "success",
  "content": "Muhan Zhang's Homepage Research Publications Software Muhan Zhang (张牧涵) Email: muhan \"at\" pku \"dot\" edu \"dot\" cn Google Scholar, Github, Lab Homepage, Lab Github Biography Muhan is a tenure-track assistant professor and PhD advisor in the Institute for Artificial Intelligence of Peking University. Before coming back to China, he was a research scientist in Facebook AI (now Meta AI) working on large-scale graph learning systems and problems (2019-2021). He received his PhD degree in computer science from Washington University in St. Louis (2015-2019), advised by Prof. Yixin Chen. Before WashU, he obtained a bachelor degree from Shanghai Jiao Tong University as a member of the IEEE pilot class, where he worked with Prof. Ya Zhang and Prof. Wenjun Zhang. 张牧涵博士，北京大学人工智能研究院助理教授、研究员、博士生导师、院长助理。首届国家级青年人才项目（海外）获得者，北京大学博雅青年学者、未名青年学者。主持多项国自然、科技部、地方政府项目和课题，包括科技创新2030重大项目课题、国自然面上项目等，内容涵盖图神经网络、电路设计、药物分子设计、通用视觉、知识图谱、智慧司法等。研究受到华为、腾讯、阿里、蚂蚁、百度、理想、灵均等企业资助，涵盖大语言模型高效微调、推理、加速、图基础模型等。常年担任NeurIPS、ICML、ICLR、CVPR等顶级学术会议的领域主席。主要研究方向包括图机器学习、大语言模型架构和推理、智慧司法等，在国际上处于领先地位。Google Scholar总引用量超过11000次，其中两篇一作文章引用量分别达到2900+和2200+次，连续多年入选Elsevier全球前2%顶尖科学家（生涯影响力榜单）。担任北大通用人工智能实验班（通班）23级班主任，主讲北京大学《人工智能引论》（本科）和《机器学习》（本研）课程。张牧涵博士于2015年本科毕业于上海交通大学IEEE试点班，2019年获得美国华盛顿大学（圣路易斯）计算机科学博士学位，曾于2019至2021年担任Facebook AI（现Meta AI）研究科学家，负责十亿用户级别大规模图机器学习系统的开发和研究。 欢迎访问MμLab实验室主页！ Research Interests Graph machine learning. Many real-world problems are inherently graph-structured, e.g., social networks, biological networks, World Wide Web, molecules, circuits, brain, road networks, knowledge graphs. Many machine learning algorithms are also defined on graphs, such as neural networks and graphical models. In this field, I develop algorithms and theories for learning over graphs, and apply them to problems like link prediction, graph classification, graph structure optimization, and knowledge graph reasoning. I am also interested in practical applications of graph neural networks, including brain modeling, drug discovery, circuit design, and healthcare applications. Large language models. Compared to machine, human posesses extreme flexibility in handling unseen tasks in a few-shot/zero-shot way, much of which is attributed to human's system-II intelligence for complex logical reasoning, task planning, causal reasoning, and inductive generalization. Large language models (LLMs) have shown unprecedented improvement on such abilities, while still fail in some top-human-level tasks, such as scientific innovation, software engineering, super-long writing, autonomous agents, etc. In this field, we aim to 1) design next-generation LLM architectures with long-term memory, human-like learning mechanisms, fast training/inference, and superior long-context abilities, 2) understand and improve LLMs' reasoning ability, ultimately matching or outperforming human on the most challenging tasks, and 3) explore LLMs' integration with other modalities, such as graph, code, relational database (RDB), image, video, etc. Prospective Students I am looking for highly motivated PhD/undergraduate students who are interested in doing machine learning research with me. A successful AI researcher usually possesses the following properties: 1) creativity and passion for research, 2) solid coding skills, 3) solid math skills, 4) strong motivation for success. Please shoot me an email if you are passionate about doing ground-breaking research with me, and are willing to exercise your every necessary ability towards it. I will do my best to provide support for your success, including detailed guidance, plenty of computation resources, and research freedom for senior PhDs. You are especially welcome if you have interdisciplinary backgrounds (such as maths/physics/chemistry/biology) while can write code. For students in Peking University, you can schedule one on one chats with me at my office. Due to the large number of applicants, the competition is intense every year and I may not be able to respond to every email. Hope you understand! For potential PhD students: I am mainly affiliated with the Insitute for AI (人工智能研究院), which is based in the main campus (燕园) of PKU. Your office will also be there and you don't need to go to the Changping (昌平) campus. News 11/26/2025: Two papers accepted at LoG-25! Congrats to Xiaohui and Yuanshuo! 11/26/2025: TPLA accepted at ASPLOS-25! Congrats to Xiaojuan! 11/12/2025: Our paper wins the CIKM-25 Best Full Paper Award! Congrats to Weishuo, Yanbo and Xiyuan! Check out the news here. 9/19/2025: Four papers accepted at NeurIPS-25! Congrats to Fanxu, Pingzhi, Juntong, Yi, Shijia and Yanan! 9/19/2025: LooGLEv2 and PHYBench accepted at NeurIPS-25 Datasets and Benchmarks track! Congrats to Ziyuan, Yuxuan, Jiaqi and Shi! 9/15/2025: HD-PiSSA is accepted at EMNLP-25 as an oral presentation! Congrats to Yiding and Fanxu! 8/5/2025: One paper accepted at CIKM-25! Congrats to Weishuo! 5/1/2025: Three papers accepted at ICML-25! Congrats to Fanxu, Yanbo and Zian! 1/23/2025: Three papers accepted at ICLR-25! Congrats to Lecheng, Haotong and Zian! 1/20/2025: Collaborated with RedNote and released RedStar, a long-chain-of-thought O1-like model that can perform complex mathematical, code and general reasoning. See the preprint. 12/18/2024: Released LIFT, a new paradigm to address long context problems of LLMs by fine-tuning the long input into model parameters. The LIFT framework can \"lift\" any short-context model by enabling them to handle arbitrary long contexts. See the preprint. 11/7/2024: Why do SOTA LLMs tend to think 9.11 > 9.9? Do they really understand numbers? We open-sourced NUPA studying the Numerical Understanding and Processing Abilities of LLMs, which contains a benchmark of 4 numerical representations and 17 distinct tasks. 9/27/2024: I wrote a draft on the lexical invariance problem on multisets and graphs, proving the necessary and sufficient conditions for a lexical invariant function on multisets and graphs, respectively. 9/26/2024: Four papers accepted at NeurIPS-24! Congrats to Fanxu, Cai, Xiaojuan and Yanbo! 7/12/2024: We released GOFA, the Generative One For All model towards tackling all tasks on all kinds of graphs. 6/24/2024: We proposed an efficient neural common neighbor method for temporal graph link prediction, which achieves three new SoTA results on TGB. 5/17/2024: 1 paper accepted at KDD-24! Congrats to Zuoyu! 5/16/2024: 2 papers accepted at ACL-24! Congrats to Jiaqi and Xiaojuan! 5/2/2024: 3 papers accepted at ICML-24! Congrats to Yi, Xiyuan and Yanbo! 4/13/2023: Invited by Huawei to give a talk on PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models and Case-Based or Rule-Based: How Do Transformers Do the Math? \". 4/3/2024: We proposed a parameter-efficient fine-tuning method called PiSSA, which surpasses the fine-tuning effectiveness of the widely used LoRA on mainstream datasets and models without additional cost. A free lunch for PEFT! See preprint at PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models . 2/1/2024: Invited by Alibaba Cloud and Tongyi Lab to give a talk on \"One for All: Towards Training One Graph Model for All Classification Tasks\". 1/17/2024: 5 papers accepted at ICLR-24! Congrats to Hao, Xiyuan, Yinan, Zehao, and Ling! 10/17/2023: We introduce a theoretical framework that views complex answer generation in large language models as a hierarchical \"template-content\" structure, which can explain how LLMs decompose tasks and perform complex reasoning. See preprint at Explaining the Complex Task Reasoning of Large Language Models with Template-Content Structure . 9/22/2023: 5 papers accepted at NeurIPS-23! Congrats to Lecheng, Jiarui, Zian, Junru and Cai! 6/5/2023: We proposed (k,t)-FWL+, a subgraph GNN achieving new state-of-the-art results on ZINC. See preprint at Towards Arbitrarily Expressive GNNs in O(n2) Space by Rethinking Folklore Weisfeiler-Lehman . 5/29/2023: We introduced code prompting, a neural-symbolic prompting method that uses code as intermediate steps to improve LLMs' reasoning ability. See preprint at Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models . 5/24/2023: We systematically tested LLMs' inductive, deductive, and abductive reasoning abilities, and found that their performance dropped a lot when replacing natural language with symbols. See preprint at Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners . 5/31/2023: Gave a talk on \"Is Distance Matrix Enough for Geometric Deep Learning?\" at Amazon Web Services AI Shanghai Lablet. 5/8/2023: We constructed BREC: a new comprehensive dataset for evaluating GNN expressiveness. The arxiv link of our manuscript is https://arxiv.org/pdf/2304.07702.pdf. The dataset and evaluation code are at https://github.com/GraphPKU/BREC. We welcome suggestions, contributions or collaborations to improve BREC. 4/25/2023: One paper accepted at ICML-23! From Relational Pooling to Subgraph GNNs. Congrats to Cai and Xiyuan! 4/20/2023: We provided a complete theory of using graph neural networks (GNNs) for multi-node representation learning with labeling tricks, extending our original labeling trick paper. The arxiv link of our manuscript is https://arxiv.org/pdf/2304.10074.pdf 4/18/2023: We proposed Neural Common Neighbor (NCN) for link prediction. The preprint can be found at Neural Common Neighbor with Completion for Link Prediction . 1/21/2023: Two papers accepted at ICLR-23! I2-GNNs and Circuit Graph Neural Network. I2-GNNs is the first linear-time GNN model to count 6-cycles. Congrats to Yinan and Zehao! 11/24/2022: Two papers accepted at LoG-22! Graph Neural Network with Local Frame and Subgraph-aware Weisfeiler-Lehman. Congrats to Xiyuan and Zhaohui! 11/18/2022: Invited by BioMap to give a talk on 3DLinker! 9/15/2022: Three papers accepted at NeurIPS-22! Rethinking KG evaluation, K-hop GNN, and Geodesic GNN. Rethinking KG evaluation is a",
  "content_length": 31627,
  "method": "requests",
  "crawl_time": "2025-12-01 14:02:10"
}