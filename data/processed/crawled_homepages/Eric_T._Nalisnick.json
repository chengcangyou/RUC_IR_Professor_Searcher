{
  "name": "Eric T. Nalisnick",
  "homepage": "https://enalisnick.github.io",
  "status": "success",
  "content": "Eric Nalisnick | Johns Hopkins University Eric Nalisnick Assistant Professor Department of Computer Science Johns Hopkins University Email     |     Bio     |     CV     |     GitHub |     Google Scholar I am interested in building safe and robust intelligent systems with a human-centered design. To accomplish this, my research develops novel machine learning techniques, which are often rooted in probabilistic modeling and computational statistics. Questions I am particularly interested in are: how can we incorporate a human's prior knowledge?, how can we detect when the system is failing?, and how to combine human and machine decision making? Some applications of importance to me are: healthcare, moderation of online content, and sign language processing. JHU Affiliations: Institute for Assured Autonomy, Mathematical Institute for Data Science, Data Science and AI Institute Selected Publications Do Deep Gen. Models Know What They Don't Know? Calibrated Learning to Defer with One-vs-All Classifiers Learning to Defer to Multiple Experts Learning to Defer to a Population Fast yet Safe: Early-Exiting with Risk Control Bounding Box Uncertainties via Conformal Prediction Do Bayesian Neural Nets Need To Be Fully Stochastic? Predictive Complexity Priors Selected Talks Learning to Defer to One, Multiple, or a Pop. of Expert(s) Towards Anytime Uncertainty in Early-Exit NNs Detecting Distribution Shift with Deep Gen. Models Uncertainty Quantification for Predictive Models [video] Research Group PhD Students / Candidates: Chi Zhang Matthew Renze Xi Wang Andrea Wynn Rajeev Verma Alexander Timans Metod Jazbec Mona Schirmer Dharmesh Tailor Putra Manggala Former Members: James Allingham, now at Google DeepMind Mrinank Sharma, now at Anthropic Saba Amiri, now at Netherlands eScience Center Urja Khurana, now at TU Delft Software Lightning UQ Box: Implements various uncertainty quantification techniques for neural networks. [JMLR article] Learning to Defer: A lightweight, unified implementation of various loss functions for the learning to defer framework for human-AI collaboration. [Jupyter Notebook Demo] Courses Taught Deep Learning (Graduate, 2025) Human-in-the-Loop Machine Learning (Graduate, 2023 - present) Machine Learning I (Graduate, 2023) Leren: Introduction to Machine Learning (Undergraduate, 2020 - 2022) Module on Bayesian Deep Learning, Deep Learning II (Graduate, 2022 - 2023) Venues Workshop on Bayesian Deep Learning Seminar on Anomaly Detection for Scientific Discovery Workshop on Tractable Probabilistic Modeling Entropy: Probabilistic Methods for Deep Learning",
  "content_length": 2603,
  "method": "requests",
  "crawl_time": "2025-12-01 13:07:12"
}