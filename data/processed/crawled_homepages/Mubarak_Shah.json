{
  "name": "Mubarak Shah",
  "homepage": "https://www.crcv.ucf.edu/person/mubarak-shah",
  "status": "success",
  "content": "Mubarak Shah – Center for Research in Computer Vision Skip to main content UCF Trustee Chair Professor, Director CRCV | Center for Research in Computer Vision University of Central Florida 4328 Scorpius St. HEC 245D Orlando, FL 32816-2365 Phone: (407) 823-5077 Fax: (407) 823-0594 Email: shah@crcv.ucf.edu UCF Graduate Faculty Profile: Click here Biography Dr. Mubarak Shah, Trustee Chair Professor of Computer Science, is the founding director of the Center for Research in Computer Vision at UCF. His research interests include: video surveillance, visual tracking, human activity recognition, visual analysis of crowded scenes, video registration, UAV video analysis, etc. Dr. Shah is a fellow of the National Academy of Inventors, IEEE, AAAS, IAPR and SPIE. In 2006, he was awarded a Pegasus Professor award, the highest award at UCF. He is ACM distinguished speaker. He was an IEEE Distinguished Visitor speaker for 1997-2000 and received IEEE Outstanding Engineering Educator Award in 1997. He received the Harris Corporation's Engineering Achievement Award in 1999, the TOKTEN awards from UNDP in 1995, 1997, and 2000; Teaching Incentive Program award in 1995 and 2003, Research Incentive Award in 2003 and 2009, Millionaires' Club awards in 2005 and 2006, University Distinguished Researcher award in 2007, honorable mention for the ICCV 2005 Where Am I? Challenge Problem, and was nominated for the best paper award in ACM Multimedia Conference in 2005. He is an editor of international book series on Video Computing; editor in chief of Machine Vision and Applications journal, and an associate editor of ACM Computing Surveys journal. He was an associate editor of the IEEE Transactions on PAMI, and a guest editor of the special issue of International Journal of Computer Vision on Video Computing. Publications You can access the CRCV Publications Page for enhanced search capabilities. 2025 Gupta, Animesh; Parmar, Jay; Dave, Ishan Rajendrakumar; Shah, MubarakFrom Play to Replay: Composed Video Retrieval for Sports Highlights Conference The Thirty-Ninth Annual Conference on Neural Information Processing Systems, 2025.Abstract | BibTeX | Links: @conference{Gupta2025, title = {From Play to Replay: Composed Video Retrieval for Sports Highlights}, author = {Animesh Gupta and Jay Parmar and Ishan Rajendrakumar Dave and Mubarak Shah}, url = {https://neurips.cc/virtual/2025/poster/121717 https://animesh-007.github.io/TF-CoVR-WEBSITE/}, year = {2025}, date = {2025-11-30}, publisher = {The Thirty-Ninth Annual Conference on Neural Information Processing Systems}, abstract = {Composed Video Retrieval (CoVR) retrieves a target video given a query video and a modification text describing the intended change. Existing CoVR benchmarks emphasize appearance shifts or coarse event changes and therefore do not test the ability to capture subtle, fast-paced temporal differences. We introduce TF-CoVR, the first large-scale benchmark dedicated to temporally fine-grained CoVR. TF-CoVR focuses on gymnastics and diving and provides 1.8 M triplets drawn from FineGym and FineDiving. Previous CoVR benchmarks focusing on temporal aspect, link each query to a single target segment taken from the same video, limiting practical usefulness. In TF-CoVR, we instead construct each pair by prompting an LLM with the label differences between clips drawn from different videos; every pair is thus associated with multiple valid target videos (3.9 on average), reflecting real-world tasks such as sports-highlight generation. To model these temporal dynamics we propose TF-CoVR-Base, a concise two-stage training framework: (i) pre-train a video encoder on fine-grained action classification to obtain temporally discriminative embeddings; (ii) align the composed query with candidate videos using contrastive learning. We conduct the first comprehensive study of image, video, and general multimodal embedding (GME) models on temporally fine-grained composed retrieval in both zero-shot and fine-tuning regimes. On TF-CoVR, TF-CoVR-Base improves zero-shot mAP@50 from 5.92 (LanguageBind) to 7.51, and after fine-tuning raises the state of the art from 19.83 to 25.82.}, keywords = {}, pubstate = {published}, tppubtype = {conference} } CloseComposed Video Retrieval (CoVR) retrieves a target video given a query video and a modification text describing the intended change. Existing CoVR benchmarks emphasize appearance shifts or coarse event changes and therefore do not test the ability to capture subtle, fast-paced temporal differences. We introduce TF-CoVR, the first large-scale benchmark dedicated to temporally fine-grained CoVR. TF-CoVR focuses on gymnastics and diving and provides 1.8 M triplets drawn from FineGym and FineDiving. Previous CoVR benchmarks focusing on temporal aspect, link each query to a single target segment taken from the same video, limiting practical usefulness. In TF-CoVR, we instead construct each pair by prompting an LLM with the label differences between clips drawn from different videos; every pair is thus associated with multiple valid target videos (3.9 on average), reflecting real-world tasks such as sports-highlight generation. To model these temporal dynamics we propose TF-CoVR-Base, a concise two-stage training framework: (i) pre-train a video encoder on fine-grained action classification to obtain temporally discriminative embeddings; (ii) align the composed query with candidate videos using contrastive learning. We conduct the first comprehensive study of image, video, and general multimodal embedding (GME) models on temporally fine-grained composed retrieval in both zero-shot and fine-tuning regimes. On TF-CoVR, TF-CoVR-Base improves zero-shot mAP@50 from 5.92 (LanguageBind) to 7.51, and after fine-tuning raises the state of the art from 19.83 to 25.82.Close Carnemolla, Simone; Pennisi, Matteo; Samarasinghe, Sarinda; Bellitto, Giovanni; Palazzo, Simone; Giordano, Daniela; Shah, Mubarak; Spampinato, ConcettoDEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models Conference The Thirty-Ninth Annual Conference on Neural Information Processing Systems, 2025.Abstract | BibTeX | Links: @conference{nokey, title = {DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models}, author = {Simone Carnemolla and Matteo Pennisi and Sarinda Samarasinghe and Giovanni Bellitto and Simone Palazzo and Daniela Giordano and Mubarak Shah and Concetto Spampinato}, url = {https://neurips.cc/virtual/2025/poster/117167}, year = {2025}, date = {2025-11-30}, publisher = {The Thirty-Ninth Annual Conference on Neural Information Processing Systems}, abstract = {Understanding and explaining the behavior of machine learning models is essential for building transparent and trustworthy AI systems. We introduce DEXTER, a data-free framework that combines diffusion models and large language models to generate global, textual explanations of visual classifiers. DEXTER operates by optimizing text prompts to synthesize class-conditional images that strongly activate a target classifier. These synthetic samples are then used to elicit detailed natural language reports that describe class-specific decision patterns and biases. Unlike prior work, DEXTER enables natural language reasoning about a classifier's decision process without access to training data or ground-truth labels. We demonstrate DEXTER's flexibility across three tasks—activation maximization, slice discovery and debiasing, and bias explanation—each illustrating its ability to uncover the internal mechanisms of visual classifiers. Quantitative and qualitative evaluations, including a user study, show that DEXTER produces accurate, interpretable outputs. Experiments on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms existing approaches in global model explanation and class-level bias reporting. }, keywords = {}, pubstate = {published}, tppubtype = {conference} } CloseUnderstanding and explaining the behavior of machine learning models is essential for building transparent and trustworthy AI systems. We introduce DEXTER, a data-free framework that combines diffusion models and large language models to generate global, textual explanations of visual classifiers. DEXTER operates by optimizing text prompts to synthesize class-conditional images that strongly activate a target classifier. These synthetic samples are then used to elicit detailed natural language reports that describe class-specific decision patterns and biases. Unlike prior work, DEXTER enables natural language reasoning about a classifier's decision process without access to training data or ground-truth labels. We demonstrate DEXTER's flexibility across three tasks—activation maximization, slice discovery and debiasing, and bias explanation—each illustrating its ability to uncover the internal mechanisms of visual classifiers. Quantitative and qualitative evaluations, including a user study, show that DEXTER produces accurate, interpretable outputs. Experiments on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms existing approaches in global model explanation and class-level bias reporting. Close Shafique, Bhuiyan Sanjid; Vayani, Ashmal; Maaz, Muhammad; Rasheed, Hanoona Abdul; Dissanayake, Dinura; Kurpath, Mohammed Irfan; Hmaiti, Yahya; Inoue, Go; Lahoud, Jean; Rashid, Md. Safirur; Quasem, Shadid Intisar; Fatima, Maheen; Vidal, Franco; Maslych, Mykola; More, Ketan Pravin; Baliah, Sanoojan; Watawana, Hasindri; Li, Yuhao; Farestam, Fabian; Schaller, Leon; Tymtsiv, Roman; Weber, Simon; Cholakkal, Hisham; Laptev, Ivan; Satoh, Shin'ichi; Felsberg, Michael; Shah, Mubarak; Khan, Salman; Khan, Fahad ShahbazA Culturally-diverse Multilingual Multimodal Video Benchmark & Model Conference Empirical Methods in Natural Language Processing, 2025.Abstract | BibTeX | Links: @conference{Shafique2025, title = {A Culturally-diverse Multilingual Multimodal Video Benchmark & Model}, author =",
  "content_length": 177451,
  "method": "requests",
  "crawl_time": "2025-12-01 14:02:04"
}