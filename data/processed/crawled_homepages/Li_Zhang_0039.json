{
  "name": "Li Zhang 0039",
  "homepage": "https://zharry29.github.io",
  "status": "success",
  "content": "Li \"Harry\" Zhang Li \"Harry\" Zhang  张力 About Me I am an assistant professor at Drexel University focusing on Natural Language Processing and Artificial Intelligence. I'm interested in planning and reasoning using Large Language Models. I earned my PhD (thesis) from the University of Pennsylvania, having the honor to be mentored by Prof. Chris Callison-Burch, with a thesis committee chaired by Prof. Dan Roth. I earned my BS from the University of Michigan in 2018, mentored by Prof. Rada Mihalcea and Prof. Dragomir Radev. I offer remote or in-person research mentorship and collaboration for selected students looking to apply for a PhD program. Those interested should fill out this form. I cannot respond to emails on this matter. I am currently not hiring PhD students. CV Semantic Scholar Google Scholar Harry.Zhang@drexel.edu Affiliations Drexel University Assistant Professor; Dec 2024 to Present University of Pennsylvania Ph.D.; Aug 2019 to Aug 2024 Allen Institute for Artifical Intelligence Research Intern; April 2023 to Dec 2023 IBM Research Research Intern; May 2021 to Aug 2021, April 2019 to June 2019 University of Michigan B.S.E.; Aug 2015 to Dec 2018 Mentorship and Teaching Haz Lab at Drexel Cassie Huang, PhD student Ceyhun Efe Kayan, PhD student Prabhu Prakash Kagitha, MS student Chimezie Maduno, MS intern Rikhil Amonkar, UG student Stuti Mohan, UG student Renxiang Wang, UG intern Past Students and Alumni Muyu He, Intern→Collinear AI Krystal Gong, Intern→University of Maryland Yuan Yuan, Intern Tianyi Zhang, Mentee Hainiu Xu, Mentee→King's College London Zhaoyi Hou, Mentee→University of Pittsburg Young-Min Cho, Mentee→University of Pennsylvania Only those advised directly are included. Teaching Instructor: CS T780-001: Applied NLP (Spring 2025, Spring 2026 at Drexel) Instructor: CS 614: Applied AI (Spring 2026 at Drexel) TA: CIS 530: Computational Linguistics (Winter, Fall 2020 at Penn) TA: EECS 595: Natural Language Processing (Fall 2018 at Michigan) TA: EECS 280: Programming and Introductory Data Structures (Winter, Fall 2016 at Michigan) Service I have reviewed more than 50 papers of and chaired for many NLP conferences and workshops. Area Chair of ARR Feb 2025 / ACL 2025, ARR Dec 2024, COLING 2025, ARR Aug 2024, ARR Jun 2024 / EMNLP 2024, ARR Feb 2024 / ACL 2024 Session Chair of ACL 2024, AACL-IJCNLP 2020 Program Chair of MASC-SLL 2023, MASC-SLL 2021 Reviewer of LREC-COLING 2024, EMNLP 2023, ACL 2023, ARR Mar 2022, DaSH Workshop @ EMNLP 2022, COLING 2022, LREC 2022, ARR Nov 2021, COLING 2020, Computer Speech and Language 2018 Research LLM-as-Formalizer: Executable and Trustworthy Planning and Problem-Solving Despite recent efforts in using large language models (LLMs) to plan and solve problems as agents, their hallucinations and lack of verifiability undermine executability and trust, preventing real-world deployment. My work advances an alternative paradigm: LLM-as-formalizer. Instead of relying on LLMs to generate plans directly, we use them as a code generator to translate a user’s environment and goal into formal languages that can be deterministically solved by off-the-shelf solvers. My primary efforts lie in using LLMs to generate formal language, such as PDDL that describes the planning environment. [39] \tLanguage Model as Planner and Formalizer under Constraints Cassie Huang, Stuti Mohan, Ziyi Yang, Stefanie Tellex and Li Zhang; preprint.Paper BibTeX Code @misc{huang2025languagemodelplannerformalizer, title={Language Model as Planner and Formalizer under Constraints}, author={Cassie Huang and Stuti Mohan and Ziyi Yang and Stefanie Tellex and Li Zhang}, year={2025}, eprint={2510.05486}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2510.05486}, } [37] Vision Language Models Cannot Plan, but Can They Formalize? Muyu He, Yuxi Zheng, Yuchen Liu, Zijian An, Bill Cai, Jiani Huang, Lifeng Zhou, Feng Liu, Ziyang Li and Li Zhang; preprint.Paper BibTeX Code @misc{he2025visionlanguagemodelsplan, title={Vision Language Models Cannot Plan, but Can They Formalize?}, author={Muyu He and Yuxi Zheng and Yuchen Liu and Zijian An and Bill Cai and Jiani Huang and Lifeng Zhou and Feng Liu and Ziyang Li and Li Zhang}, year={2025}, eprint={2509.21576}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2509.21576}, } [36] Documentation Retrieval Improves Planning Language Generation; Renxiang Wang and Li Zhang; in AACL 2025.Paper BibTeX Code @misc{wang2025documentationretrievalimprovesplanning, title={Documentation Retrieval Improves Planning Language Generation}, author={Renxiang Wang and Li Zhang}, year={2025}, eprint={2509.19931}, archivePrefix={arXiv}, primaryClass={cs.IR}, url={https://arxiv.org/abs/2509.19931}, } [35] Zero-Shot Iterative Formalization and Planning in Partially Observable Environments; Liancheng Gong, Wang Zhu, Jesse Thomason and Li Zhang; preprint.Paper BibTeX Code @misc{gong2025zeroshotiterativeformalizationplanning, title={Zero-Shot Iterative Formalization and Planning in Partially Observable Environments}, author={Liancheng Gong and Wang Zhu and Jesse Thomason and Li Zhang}, year={2025}, eprint={2505.13126}, archivePrefix={arXiv}, primaryClass={cs.AI}, url={https://arxiv.org/abs/2505.13126}, } [34] Unifying Inference-Time Planning Language Generation ; Prabhu Prakash Kagitha, Bo Sun, Ishan Desai, Andrew Zhu, Cassie Huang, Manling Li, Ziyang Li and Li Zhang; preprint.Paper BibTeX Code @misc{kagitha2025unifyinginferencetimeplanninglanguage, title={Unifying Inference-Time Planning Language Generation}, author={Prabhu Prakash Kagitha and Bo Sun and Ishan Desai and Andrew Zhu and Cassie Huang and Manling Li and Ziyang Li and Li Zhang}, year={2025}, eprint={2505.14763}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2505.14763}, } [33] Are LLMs Better Formalizers than Solvers on Complex Problems; Rikhil Amonkar,May Lai, Ronan Le Bras and Li Zhang; preprint.Paper BibTeX Code @misc{amonkar2025naturallanguageplanningcoding, title={Are LLMs Better Formalizers than Solvers on Complex Problems?}, author={Rikhil Amonkar and May Lai and Ronan Le Bras and Li Zhang}, year={2025}, eprint={2505.13252}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2505.13252}, } [30] On the Limit of Language Models as Planning Formalizers; Cassie Huang and Li Zhang; in ACL 2025.Paper BibTeX Code @inproceedings{huang-zhang-2025-limit, title = \"On the Limit of Language Models as Planning Formalizers\", author = \"Huang, Cassie and Zhang, Li\", editor = \"Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher\", booktitle = \"Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\", month = jul, year = \"2025\", address = \"Vienna, Austria\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2025.acl-long.242/\", pages = \"4880--4904\", ISBN = \"979-8-89176-251-0\", abstract = \"Large Language Models have been found to create plans that are neither executable nor verifiable in grounded environments. An emerging line of work demonstrates success in using the LLM as a formalizer to generate a formal representation of the planning domain in some language, such as Planning Domain Definition Language (PDDL). This formal representation can be deterministically solved to find a plan. We systematically evaluate this methodology while bridging some major gaps. While previous work only generates a partial PDDL representation, given templated, and therefore unrealistic environment descriptions, we generate the complete representation given descriptions of various naturalness levels. Among an array of observations critical to improve LLMs' formal planning abilities, we note that most large enough models can effectively formalize descriptions as PDDL, outperforming those directly generating plans, while being robust to lexical perturbation. As the descriptions become more natural-sounding, we observe a decrease in performance and provide detailed error analysis.\" } [29] PDDLEGO: Iterative Planning in Textual Environments; Li Zhang, Peter Jansen, Peter Clark, Chris Callison-Burch and Niket Tandon; in *SEM 2024.Paper BibTeX Code @inproceedings{zhang-etal-2024-pddlego, title = \"{PDDLEGO}: Iterative Planning in Textual Environments\", author = \"Zhang, Li and Jansen, Peter and Zhang, Tianyi and Clark, Peter and Callison-Burch, Chris and Tandon, Niket\", editor = \"Bollegala, Danushka and Shwartz, Vered\", booktitle = \"Proceedings of the 13th Joint Conference on Lexical and Computational Semantics (*SEM 2024)\", month = jun, year = \"2024\", address = \"Mexico City, Mexico\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2024.starsem-1.17\", pages = \"212--221\", abstract = \"Planning in textual environments have been shown to be a long-standing challenge even for current models. A recent, promising line of work uses LLMs to generate a formal representation of the environment that can be solved by a symbolic planner. However, existing methods rely on a fully-observed environment where all entity states are initially known, so a one-off representation can be constructed, leading to a complete plan. In contrast, we tackle partially-observed environments where there is initially no sufficient information to plan for the end-goal. We propose PDDLEGO that iteratively construct a planning representation that can lead to a partial plan for a given sub-goal. By accomplishing the sub-goal, more information is acquired to augment the representation, eventually achieving the end-goal. We show that plans produced by few-shot PDDLEGO are 43{\\%} more efficient than generating plans end-to-end on the Coin Collector simulation, with strong performance (98{\\%}) on the more complex Cooking World simulation where end-to-end LLMs fail to generate coherent plans (4{\\%}).\", } [28] PROC2PDDL: Open-Domain Planning Representations from T",
  "content_length": 51080,
  "method": "requests",
  "crawl_time": "2025-12-01 13:46:56"
}