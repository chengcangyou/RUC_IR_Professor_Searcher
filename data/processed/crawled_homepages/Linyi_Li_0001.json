{
  "name": "Linyi Li 0001",
  "homepage": "https://linyil.com",
  "status": "success",
  "content": "Linyi Li Linyi Li Assistant Professor, CS@SFU. Principal Investigator of TAI Lab @ SFU [firstnamelowercase]_[lastnamelowercase]@sfu.ca Burnaby, BC (Greater Vancouver), Canada [Curriculum Vitae] I am Linyi LI, assistant professor in School of Computing Science, Simon Fraser University. I am directing TAI Lab at SFU. My research is in trustworthy deep learning, with a focus on certifiably trustworthy deep learning and trustworthy foundation models. My research spans machine learning and computer security. More concretely, I like to enable certifiable and verifiable trustworthiness guarantees (such as robustness, fairness, and numerical reliability) for large-scale deep learning systems; understand and analyze mechanisms of deep learning and foundation models especially their root causes of trustworthiness issues; and evaluate foundation models scientifically and comprehensively. I have published over 30 papers in flagship machine learning and computer security conferences such as ICML, NeurIPS, ICLR, IEEE S&P, and ACM CCS. I am awarded Rising Stars in Data Science, AdvML Rising Star Award, and Wing Kai Cheng Fellowship. I co-led the Team \\(\\alpha,\\beta\\)-CROWN in 2023 that won 4th International Verification of Neural Networks Competition (VNN-COMP'23). I am the finalist of 2022 Qualcomm Innovation Fellowship and 2022 Two Sigma PhD Fellowship. I got my PhD in Computer Science, University of Illinois Urbana-Champaign in 2023 advised by gorgeous Bo Li and awesome Tao Xie. I got my bachelor's degree from Department of Computer Science and Technology, Tsinghua University in 2018, where I did research on Web API Automated Testing, advised by Xiaoying Bai. I was a senior research scientist at ByteDance between 2023 and 2024. I interned in Microsoft twice (mentored by Adam Kalai and Neel Sundaresan in 2022 and 2019 respectively), Fujitsu Research of America (mentored by Mukul Prasad) in 2021, and Carnegie Mellon University (mentored by Matt Fredrikson) in 2017. More About My Research Teaching Lab Openings (Multiple PhD openings in 2026 Fall) News [Sept, 2025] Our TAI lab welcomes 2 new PhD students: Peng He and Zhe Wang! [Jun, 2025] I served/will serve as an Area Chair for NeurIPS 2025 and ICLR 2026. [Jun, 2025] Our lab proudly recevied NSERC Discovery Grant with Launch Supplement. [Dec, 2024] I will give a talk at AAAI 2025 New Faculty Highlights program about Certified Trustworthiness in the Era of Large Language Models. [Dec, 2024] I will be serving as an Area Chair for ICML 2025. [Nov, 2024] Our lab website is available now. We have multiple PhD openings in 2025 Fall. [Sept, 2024] Our InfiBench, a novel benchmark for open-world question-answering for code large language models, will appear at NeurIPS 2024! We summarize the empirical scaling laws and trends from over 100 open-source code LLMs. [Aug, 2024] I joined School of Computing Science at Simon Fraser University as a tenure-track assistant professor. Excited to embark on this new journey! [July, 2023] Our α,β-CROWN wins the neural network verification competition again! [Dec, 2022] Our RANUM framework for assuring numerical reliability of deep neural networks is accepted by ICSE 2023. [Oct, 2022] Happy to be selected as Rising Stars in Data Science at DSI, University of Chicago! [Sept, 2022] I am co-organizing the workshop on Trustworthy and Socially Responsible Machine Learning at NeurIPS 2022. We invite submissions on any aspect of trustworthy and socially responsible machine learning. [Sept, 2022] Five papers accepted to NeurIPS 2022. My co-first authored paper proposes a scalable method for certifying model's distributional fairness. [Aug, 2022] Happy to receive 2022 AdvML Rising Star Award! [Jun, 2022] We release a systematization of knowledge (SOK) paper (accepted by IEEE SP 2023) along with a toolkit for evaluating about 20 neural network verification approaches on GitHub. [May, 2022] Three papers accepted by ICML 2022. We provide a tighter certification against L2 perturbations (link), a tighter certification for point cloud models (link), and an out-of-domain generalization certification (link). Look forward to seeing you in Baltimore in July 2022. [May, 2022] Started internship at Microsoft Research New England on deep program synthesis - I am in Boston area this summer. [Apr, 2022] Selected as finalist for 2022 Qualcomm Innovation Fellowship. [Jan, 2022] Selected as finalist for 2022 Two Sigma PhD Fellowship. [Jan, 2022] We propose practical robustness certification approaches for RL against evasion attacks (CROP, accepted by ICLR 2022) and poisoning attacks (COPA, accepted by ICLR 2022). [Jan, 2022] Motivated by theoretical analysis, we propose DRT, a training approach for randomized smoothing that diversifies submodels within an ensemble to achieve state-of-the-art certified robustness. Check out our paper at ICLR 2022! Older News [Sept, 2021] Regularizing gradient similarity and model smoothness is sufficient to diversify sub-models in an ensemble, and thus leading to significant improvements on ensemble NN robustness. Details available in our paper at NeurIPS 2021. [May, 2021] Simple downsampling combined with Progressive GAN can attack neural networks very efficiently. Details available in our paper at ICML 2021. [May, 2021] We provide the first rigorous robustness certification on ImageNet against common image transformations including rotation and scaling! Paper will appear at CCS 2021. [Jan, 2021] We will present a novel analysis of using non-linear projections for neural networks black-box attack at AISTATS 2021. [Aug, 2020] Paper on clustering test steps leveraging NLP for automating software testing got accepted by ESEC/FSE'20 (Industry Track). [Apr, 2020] Passed the Ph.D. Qualifying exam. [Nov, 2019] Our team ranked 2nd in ICPC Mid-Central USA Regional Contest 2019. [May, 2019] Paper on training provable robust NN via reference adversarial space got accepted by IJCAI'19. [July, 2018] Graduated from Tsinghua University with Outstanding Underguaduate Award from the university and Excellence Undergraduate Award from the department. [Feb, 2018] Recevied CS Ph.D admission offers from Carnegie Mellon University, University of Illinois at Urbana-Champaign and University of Wisconsin-Madison. Many thanks to everyone who helped my application! [Sept, 2017] Finished summer internship at Carnegie Mellon University on neural network explaining, advised by Prof. Matt Fredrikson. [Mar, 2017] Paper on Cloud API Testing got accepted by COMSPAC'17. [Nov, 2015] Started to work with Prof. Xiaoying Bai on software testing. Selected Publications Full publication list is available at TAI Lab - Publication and Google Scholar. (* denotes to equal contribution) Linyi Li, Shijie Geng, Zhenwen Li, Yibo He, Hao Yu, Ziyue Hua, Guanghan Ning, Siwei Wang, Tao Xie, Hongxia Yang InfiBench: Evaluating the Question-Answering Capabilities of Code Large Language Models 38th Conference on Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS 2024 D&B) [Full Version] [Conference Version] [Code] [Project Website] [Slides] [BibTex] @inproceedings{ li2024infibench, title={InfiBench: Evaluating the Question-Answering Capabilities of Code Large Language Models}, author={Linyi Li and Shijie Geng and Zhenwen Li and Yibo He and Hao Yu and Ziyue Hua and Guanghan Ning and Siwei Wang and Tao Xie and Hongxia Yang}, booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, year={2024}, } Topic: LLM benchmark code Summary A comprehensive benchmark for code large language models (LLMs) evaluating model ability on answering freeform real-world questions in the code domain. From the evaluation of over 100 models, we summarize the empirical trends and scaling laws for existing open-source code LLMs. Linyi Li Certifiably Trustworthy Deep Learning Systems at Scale Doctoral Thesis [Full Version] [Official Version] [BibTex] @phdthesis{li2023thesis, title = {Certifiably Trustworthy Deep Learning Systems at Scale}, author = {Linyi Li}, year = 2023, month = {Oct}, school = {University of Illinois Urbana-Champaign}, type = {PhD thesis} } Topic: certified ML Summary My PhD thesis. The thesis systematically summarizes the current research horizon of deep learning certified trustworthiness. Compared to the SoK paper, the thesis extends beyond just robustness and covers the technical details of representative methods. Linyi Li, Tao Xie, Bo Li SoK: Certified Robustness for Deep Neural Networks 44th IEEE Symposium on Security and Privacy (SP 2023) [Full Version] [Conference Version] [Slides] [Code] [Leaderboard] [BibTex] @inproceedings{li2023sok, author={Linyi Li and Tao Xie and Bo Li}, title = {SoK: Certified Robustness for Deep Neural Networks}, booktitle = {44th {IEEE} Symposium on Security and Privacy, {SP} 2023, San Francisco, CA, USA, 22-26 May 2023}, publisher = {{IEEE}}, year = {2023}, } Topic: certified ML Summary A comprehensive systemization of knowledge on DNN certified robustness, including discussion on practical and theoretical implications, findings, main challenges, and future directions, accompanied with an open-source unified platform to evaluate 20+ representative approaches. Linyi Li, Yuhao Zhang, Luyao Ren, Yingfei Xiong, Tao Xie Reliability Assurance for Deep Neural Network Architectures Against Numerical Defects 45th IEEE/ACM International Conference on Software Engineering (ICSE 2023) [Full Version] [Conference Version] [Slides] [Code] [BibTex] @inproceedings{li2023reliability, author={Linyi Li and Yuhao Zhang and Luyao Ren and Yingfei Xiong and Tao Xie}, title = {Reliability Assurance for Deep Neural Network Architectures Against Numerical Defects}, booktitle = {45th International Conference on Software Engineering, {ICSE} 2023, Melbourne, Australia, 14-20 May 2023}, publisher = {{IEEE/ACM}}, year = {2023}, } Topic: certified ML numerical reliability Summary An effective and efficient white-box framework f",
  "content_length": 20315,
  "method": "requests",
  "crawl_time": "2025-12-01 13:47:32"
}