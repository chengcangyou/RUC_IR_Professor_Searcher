{
  "name": "Daniele De Sensi",
  "homepage": "http://danieledesensi.github.io",
  "status": "success",
  "content": "Daniele De Sensi I am an Associate Professor at Sapienza University of Rome. I was previously an Assistant Professor at Sapienza University of Rome, and a PostDoc researcher at ETH ZÃ¼rich and University of Pisa. I received the Ph.D. in Computer Science from the University of Pisa. My main reseach interests lie on the fields of Interconnection Networks, High Performance Computing, Green Computing, and Parallel Programming. You can find more about my research by checking the publications page. If you have anything to discuss, feel free to send me an e-mail. News Oct 22, 2025 ðŸŽ‰ Iâ€™m happy to announce that our paper Flowcut Switching: High-Performance Adaptive Routing with In-Order Delivery Guarantees has been accepted in IEEE Transactions on Networking. Oct 15, 2025 ðŸŽ‰ Iâ€™m happy to announce that two papers have been accepted at SC 2025 (Bine Trees: Enhancing Collective Operations by Optimizing Communication Locality and Uno: A One-Stop Solution for Inter- and Intra-Data Center Congestion Control and Reliable Connectivity) and one at EuroSys 2026 (REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and Failure Mitigation). I am going to present the paper on Bine Trees in a few weeks at SC 2025. The paper introduces new algorithms for collective operations, reducing distance between communicating ranks and improving performance up to 80% on LUMI and MareNostrum5, 50% on Leonardo, and up to 5x on Fugaku. Aug 27, 2024 ðŸŽ‰ Iâ€™m happy to announce that the paper Exploring GPU-to-GPU Communication: Insights into Supercomputer Interconnects have been accepted at SC â€˜24. The paper focuses on GPU-GPU interconnect. We analyzed three top-10 supercomputers, on up to 4k NVIDIA and AMD GPUs, offering valuable insights to architects, researchers, practitioners, and developers. The paper is the outcome of a fruitful collaboration with NVIDIA, Cray HPE, ETH Zurich, CINECA, University of Trento, Vrije Universitiet, and University of Antwerp. You can find a preprint here. May 2, 2024 ðŸŽ‰ Iâ€™m happy to announce that the paper OSMOSIS: Enabling Multi-Tenancy in Datacenter SmartNICs have been accepted at USENIX ATC â€˜24. OSMOSIS is a resource manager for SmartNICs to support multi-tenancy with low overhead. Apr 4, 2024 ðŸŽ‰ Iâ€™m happy to announce that a new paper have been accepted at HPDC â€˜24. The paper introduces a performance model and new algorithms for reduce and allreduce on the Cerebras Wafer-Scale Engine (WSE), a novel architecture optimized for machine learning workloads. The new algorithms outperform the current vendor solutions by more than 3x. Selected Publications 2025 SC25 Bine Trees: Enhancing Collective Operations by Optimizing Communication Locality Daniele De Sensi,Â Saverio Pasqualoni,Â Lorenzo Piarulli, and 5 more authors In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SCâ€™25), Nov 2025 Abs arXiv Bib As high-performance computing (HPC) systems grow, optimizing communication locality becomes essential for performance. HPC networks are often oversubscribed, consisting of fully connected groups that are sparsely connected. We introduce Binomial Negabinary (Bine) trees, a novel approach to enhance collective operations by reducing inter-group communication. They minimize the distance between communicating ranks, reducing traffic on global links and alleviating congestion. Unlike traditional hierarchical algorithms, Bine trees are topology-agnostic and do not assume a uniform partition of ranks, making them ideal for production supercomputers with irregular process allocations. We design algorithms for eight collectives, achieving up to 5x speedups and 33% less global traffic on four supercomputers with four different topologies. Our results emphasize their effectiveness in improving performance while reducing the load on global links. @inproceedings{bine, author = {De Sensi, Daniele and Pasqualoni, Saverio and Piarulli, Lorenzo and Bonato, Tommaso and Ba, Seydou and Turisini, Matteo and Domke, Jens and Hoefler, Torsten}, title = {Bine Trees: Enhancing Collective Operations by Optimizing Communication Locality}, year = {2025}, month = nov, booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC'25)}, doi = {To Appear}, dimensions = {true}, } 2024 CACM HammingMesh: A Network Topology for Large-Scale Deep Learning Torsten Hoefler,Â Tommaso Bonato,Â Daniele De Sensi, and 6 more authors Communications of the ACM, Nov 2024 Online First Abs Bib Numerous microarchitectural optimizations unlocked tremendous processing power for deep neural networks that in turn fueled the ongoing AI revolution. With the exhaustion of such optimizations, the growth of modern AI is now gated by the performance of training systems, especially their data movement. Instead of focusing on single accelerators, we investigate data-movement characteristics of large-scale training at full system scale. Based on our workload analysis, we design HammingMesh, a novel network topology that provides high bandwidth at low cost with high job scheduling flexibility. Specifically, HammingMesh can support full bandwidth and isolation to deep learning training jobs with two dimensions of parallelism. Furthermore, it also supports high global bandwidth for generic traffic. Thus, HammingMesh will power future large-scale deep learning systems with extreme bandwidth requirements. @article{10.1145/3623490, author = {Hoefler, Torsten and Bonato, Tommaso and De Sensi, Daniele and Di Girolamo, Salvatore and Li, Shigang and Heddes, Marco and Goel, Deepak and Castro, Miguel and Scott, Steve}, title = {HammingMesh: A Network Topology for Large-Scale Deep Learning}, year = {2024}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, issn = {0001-0782}, url = {https://doi.org/10.1145/3623490}, doi = {10.1145/3623490}, note = {Online First}, journal = {Communications of the ACM}, month = nov, numpages = {15}, dimensions = {true}, } NSDI24 Swing: Short-cutting Rings for Higher Bandwidth Allreduce Daniele De Sensi,Â Tommaso Bonato,Â David Saam, and 1 more author In 21th USENIX Symposium on Networked Systems Design and Implementation (NSDI 24), Apr 2024 Abs arXiv Bib PDF Video The allreduce collective operation accounts for a significant fraction of the runtime of workloads running on distributed systems. One factor determining its performance is the distance between communicating nodes, especially on networks like torus, where a higher distance implies multiple messages being forwarded on the same link, thus reducing the allreduce bandwidth. Torus networks are widely used on systems optimized for machine learning workloads (e.g., Google TPUs and Amazon Trainium devices), as well as on some of the Top500 supercomputers. To improve allreduce performance on torus networks we introduce Swing, a new algorithm that keeps a low distance between communicating nodes by swinging between torus directions. Our analysis and experimental evaluation show that Swing outperforms by up to 3x existing allreduce algorithms for vectors ranging from 32B to 128MiB, on different types of torus and torus-like topologies, regardless of their shape and size. @inproceedings{swing, author = {De Sensi, Daniele and Bonato, Tommaso and Saam, David and Hoefler, Torsten}, title = {Swing: Short-cutting Rings for Higher Bandwidth Allreduce}, booktitle = {21th USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)}, year = {2024}, address = {Santa Clara, CA}, publisher = {USENIX Association}, month = apr, dimensions = {true}, } 2022 SIGMETRICS23 Noise in the Clouds: Influence of Network Performance Variability on Application Scalability Daniele De Sensi,Â Tiziano De Matteis,Â Konstantin Taranov, and 3 more authors Proc. ACM Meas. Anal. Comput. Syst., Dec 2022 Abs arXiv Bib PDF Code Video Cloud computing represents an appealing opportunity for cost-effective deployment of HPC workloads on the best-fitting hardware. However, although cloud and on-premise HPC systems offer similar computational resources, their network architecture and performance may differ significantly. For example, these systems use fundamentally different network transport and routing protocols, which may introduce \\textitnetwork noise that can eventually limit the application scaling. This work analyzes network performance, scalability, and cost of running HPC workloads on cloud systems. First, we consider latency, bandwidth, and collective communication patterns in detailed small-scale measurements, and then we simulate network performance at a larger scale. We validate our approach on four popular cloud providers and three on-premise HPC systems, showing that network (and also OS) noise can significantly impact performance and cost both at small and large scale. @article{cloudnoise, author = {De Sensi, Daniele and De Matteis, Tiziano and Taranov, Konstantin and Di Girolamo, Salvatore and Rahn, Tobias and Hoefler, Torsten}, title = {Noise in the Clouds: Influence of Network Performance Variability on Application Scalability}, year = {2022}, issue_date = {December 2022}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {6}, number = {3}, doi = {10.1145/3570609}, journal = {Proc. ACM Meas. Anal. Comput. Syst.}, month = dec, articleno = {49}, numpages = {27}, keywords = {cloud; HPC; network noise; scalability;}, eprint = {2210.15315}, dimensions = {true}, } 2021 SC21 Flare: Flexible in-Network Allreduce Daniele De Sensi,Â Salvatore Di Girolamo,Â Saleh Ashkboos, and 2 more authors In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, Dec 2021 Abs arXiv Bib PDF Code Slides Video The allreduce operation is one of the most commonly used communication routines in distributed applications. To improve its bandwidth and to reduce network traffic, this operation can be accelerated by offl",
  "content_length": 15523,
  "method": "requests",
  "crawl_time": "2025-12-01 12:57:49"
}