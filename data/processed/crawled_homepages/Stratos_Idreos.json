{
  "name": "Stratos Idreos",
  "homepage": "http://stratos.seas.harvard.edu",
  "status": "success",
  "content": "Stratos Idreos | Stratos Idreos Skip to main content arrow_circle_down Stratos Idreos CrimsonDB arrow_circle_right Queriosity arrow_circle_right Self-designing Data Systems arrow_circle_right Hero Pagination Gordon McKay Professor of Computer ScienceHarvard John A. PaulsonSchool of Engineering & Applied SciencesI lead the Data Systems Laboratory at Harvard Our research at Harvard DASlab aims to help with making it easier to design, tune and use data systems. We are currently building three major systems:(1) a design engine that accelerates research and improves developer productivity,(2) a shape-shifting zero-knobs key-value store, and(3) a data science accelerator. Here is a video on the periodic table of data structures and self-designing systems Selected Recent Publications Download 10 citations download BibTeX EndNote X3 XML EndNote 7 XML Endnote tagged Marc PubMedId RIS 2022 S. Chatterjee, M. Jagadeesan, W. Qin, and S. Idreos,“Cosine: A Cloud-Cost Optimized Self-Designing Key-Value Storage Engine”, in Proceedings of the Very Large Databases Endowment (PVLDB), 2022. S. Chatterjee, M. Jagadeesan, W. Qin, and S. Idreos,“Cosine: A Cloud-Cost Optimized Self-Designing Key-Value Storage Engine”, in Proceedings of the Very Large Databases Endowment (PVLDB), 2022. add_circle_outline do_not_disturb_on Abstract picture_as_pdf cosine.pdf We present a self-designing key-value storage engine, Cosine, which can always take the shape of the close to “perfect” engine architec- ture given an input workload, a cloud budget, a target performance, and required cloud SLAs. By identifying and formalizing the first principles of storage engine layouts and core key-value algorithms, Cosine constructs a massive design space comprising of sextillion (10^36) possible storage engine designs over a diverse space of hardware and cloud pricing policies for three cloud providers – AWS, GCP, and Azure. Cosine spans across diverse designs such as Log-Structured Merge-trees, B-trees, Log-Structured Hash-tables, in-memory accelerators for filters and indexes as well as trillions of hybrid designs that do not appear in the literature or industry but emerge as valid combinations of the above. Cosine includes a unified distribution-aware I/O model and a learned concurrency-aware CPU model that with high accuracy can calculate the performance and cloud cost of any possible design on any workload and virtual machines. Cosine can then search through that space in a matter of seconds to find the best design and materializes the actual code of the resulting storage engine design using a templated Rust imple- mentation. We demonstrate that on average Cosine outperforms state-of-the-art storage engines such as write-optimized RocksDB, read-optimized WiredTiger, and very write-optimized FASTER by 53x, 25x, and 20x, respectively, for diverse workloads, data sizes, and cloud budgets across all YCSB core workloads and many variants. picture_as_pdf cosine.pdf B. Hentschel, U. Sirin, and S. Idreos,“Entropy-Learned Hashing Constant Time Hashing with Controllable Uniformity”, in ACM SIGMOD International Conference on Management of Data, 2022. B. Hentschel, U. Sirin, and S. Idreos,“Entropy-Learned Hashing Constant Time Hashing with Controllable Uniformity”, in ACM SIGMOD International Conference on Management of Data, 2022. add_circle_outline do_not_disturb_on Abstract picture_as_pdf entropylearnedhashing.pdf Hashing is a widely used technique for creating uniformly random numbers from arbitrary input data. It is a core component in relational data systems, key-value stores, compilers, networks and many more areas used for a wide range of operations including indexing, partitioning, filters, and sketches. Due to both the computational and data heavy nature of hashing in such operations, numerous recent studies observe that hashing emerges as a core bottleneck in modern systems. For example, a typical complex database query (TPC-H) could spend 50% of its total cost in hash tables, while Google spends at least 2% of its total computational cost across all systems on C++ hash tables, resulting in a massive yearly footprint coming from just a single operation.\tIn this paper we propose a new method, called Entropy-Learned Hashing, which reduces the computational cost of hashing by up to an order of magnitude. The key question we ask is “how much randomness is needed?”: We look at hashing from a pseudorandom point of view, wherein hashing is viewed as extracting randomness from a data source to create random outputs and we show that state-of-the-art hash functions do too much work. Entropy-Learned Hashing 1) models and estimates the randomness (entropy) of the input data, and then 2) creates data-specific hash functions that use only the parts of the data that are needed to differentiate the outputs. Thus the resulting hash functions can minimize the amount of computation needed while we prove that they act similarly to traditional hash functions in terms of the uniformity of their outputs. We test Entropy-Learned Hashing across diverse and core hashing operations such as hash tables, Bloom filters, and partitioning and we observe an increase in throughput in the order of 3.7X, 4.0X, and 14X respectively compared to the best in-class hash functions and implementations used at scale by Google and Meta. picture_as_pdf entropylearnedhashing.pdf 2019 N. Dayan and S. Idreos,“The Log-Structured Merge-Bush &amp; the Wacky Continuum”, in ACM SIGMOD International Conference on Management of Data, 2019. N. Dayan and S. Idreos,“The Log-Structured Merge-Bush &amp; the Wacky Continuum”, in ACM SIGMOD International Conference on Management of Data, 2019. add_circle_outline do_not_disturb_on Abstract picture_as_pdf wackyandthebush.pdf Data-intensive key-value stores based on the Log-Structured Merge-Tree are used in numerous modern applications ranging from social media and data science to cloud infrastructure. We show that such designs exhibit an intrinsic contention be- tween the costs of point reads, writes and memory, and that this trade-off deteriorates as the data size grows. The root of the problem is that in all existing designs, the capacity ratio between any pair of levels is fixed. This causes write cost to increase with the data size while yielding exponentially diminishing returns for point reads and memory.We introduce the Log-Structured Merge-Bush (LSM-Bush), a new data structure that sets increasing capacity ratios between adjacent pairs of smaller levels. As a result, smaller levels get lazier by gathering more runs before merging them. By using a doubly-exponential ratio growth rate, LSM-bush brings write cost down from O(log N ) to O(log log N ), and it can trade this gain to either improve point reads or memory. Thus, it enables more scalable trade-offs all around.We further introduce Wacky, a design continuum that includes LSM-Bush as well as all state-of-the-art merge policies, from laziest to greediest, and can assume any of them within a single implementation. Wacky encompasses a vast space of performance properties, including ones that favor range reads, and it can be searched analytically to find the design that performs best for a given workload in practice. picture_as_pdf wackyandthebush.pdf S. Idreos et al.,“Design Continuums and the Path Toward Self-Designing Key-Value Stores that Know and Learn”, in Biennial Conference on Innovative Data Systems Research (CIDR), 2019. S. Idreos et al.,“Design Continuums and the Path Toward Self-Designing Key-Value Stores that Know and Learn”, in Biennial Conference on Innovative Data Systems Research (CIDR), 2019. add_circle_outline do_not_disturb_on Abstract picture_as_pdf selfdesign.pdf We introduce the concept of design continuums for the data layout of key-value stores. A design continuum unifies major distinct data structure designs under the same model. The critical insight and potential long-term impact is that such unifying models 1)~render what we consider up to now as fundamentally different data structures to be seen as ``views'' of the very same overall design space, and 2)~allow ``seeing'' new data structure designs with performance properties that are not feasible by existing designs. The core intuition behind the construction of design continuums is that all data structures arise from the very same set of fundamental design principles, i.e., a small set of data layout design concepts out of which we can synthesize any design that exists in the literature as well as new ones. We show how to construct, evaluate, and expand, design continuums and we also present the first continuum that unifies major data structure designs, i.e., B+Tree, BeTree, LSM-tree, and LSH-Table.The practical benefit of a design continuum is that it creates a fast inference engine for the design of data structures. For example, we can near instantly predict how a specific design change in the underlying storage of a data system would affect performance, or reversely what would be the optimal data structure (from a given set of designs) given workload characteristics and a memory budget. In turn, these properties allow us to envision a new class of self-designing key-value stores with a substantially improved ability to adapt to workload and hardware changes by transitioning between drastically different data structure designs to assume a diverse set of performance properties at will. picture_as_pdf selfdesign.pdf 2018 S. Idreos, K. Zoumpatianos, B. Hentschel, M. S. Kester, and D. Guo,“The Data Calculator: Data Structure Design and Cost Synthesis From First Principles, and Learned Cost Models”, in ACM SIGMOD International Conference on Management of Data , 2018. S. Idreos, K. Zoumpatianos, B. Hentschel, M. S. Kester, and D. Guo,“The Data Calculator: Data Structure Design and Cost Synthesis From First Principles, and Learned Cost Models”, in ACM SIGMOD International Conference on Management of Data , 2018. add_circle_outline do_not_",
  "content_length": 22528,
  "method": "requests",
  "crawl_time": "2025-12-01 14:32:50"
}