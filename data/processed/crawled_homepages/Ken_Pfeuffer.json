{
  "name": "Ken Pfeuffer",
  "homepage": "https://kenpfeuffer.com",
  "status": "success",
  "content": "Ken Pfeuffer ‚Äì Professor and Sapere Aude Research Leader in HCI, XR @ Aarhus University Skip to content I‚Äôm an Associate Professor and Sapere Aude Research Leader in Human-Computer Interaction (HCI) at Aarhus University in Denmark. I lead the XI team, where we explore how we can render technology more natural to use ‚Äì especially through multimodal, spatial, and AI-driven techniques in emerging computer devices (mobiles, wearables, AR/VR, mobiles). Our research is funded by AUFF, Danish Pioneer Center of AI, and DFF Sapere Aude. üìö Publications: Google Scholarüè¢ Previous Affiliations: Usable Security and Privacy, Lancaster University,¬†Microsoft, Google.‚úâÔ∏è Contact: ken [at] cs [dot] au [dot] dk Twitter LinkedIn YouTube Bluesky Medium Research Vision Today, we mostly control computers with a mouse, keyboard, or touchscreen in graphical user interfaces (GUI), tools that have not evolved much in decades. But us humans have much more to give than simple finger taps on flat, retangular buttons. What if interfaces could understand what you are thinking, feeling, where you‚Äôre looking‚Äîand exploit this to transform computer interaction for a more intuitive and seamless experience? As example, much of my prior work focused on the integration of our eye movements in the computer interface. Through my vision of an Eye-Hand Symbiosis, I have established the scientific foundations for eyes+hands computer control in various contexts. This approach lets users interact with any object they see‚Äînot just where their finger is. For example: Gaze + Touch: Touch anywhere while looking at a target- no need to aim by hand. Gaze + Pinch: With your bare hands only, look at an object and pinch your fingers to select it. Gaze-Shifting: Use the eye direction to choose direct or indirect gestures types on-demand. Input devices such as a touchscreen or hand-tracking, typically involve a (1:1) mapping between the user‚Äôs hand input position and the manipulated object, to manipulate one object that coincides with your hand‚Äôs position in space. In contrast, eye+hand interfaces enable control¬†any¬†object you see, from¬†any¬†hand position (N:N). These methods are already being adopted in next-generation XR devices like Apple Vision Pro1,2. In principle, Eye-hand Symbiosis could transform how we interact with all kinds of digital devices, and their potential is subject to further research. The Eye-Hand Symbiosis research project covers a range of research efforts since 2013, as illustrated below. See the drop-down list in the homepage menu to dive into specific articles that were published over the years in scientific venues. Since 2023, I have written popular science articles aimed at a broader audience, for a high-level overviews of various scientific articles and reflections on the past/ongoing evolution. Eyes & hands in AR: A sci-fi-inspired mobile UI research (Oct 2023, Medium, LinkedIn) Design Principles & Issues for Gaze and Pinch Interaction (Jan 2024, Medium, LinkedIn) History of Eyes and Hands for Computer Control (Mar 2024, Medium, LinkedIn) The Role of Eyes and Hands in the Evolution of Computer Interfaces (Nov 2024, Future of Text) Blending Direct and Indirect Interaction: A Concept for Seamless Computer Interfaces (Dec 2024, Medium, LinkedIn) Talks (on Video) See full list of talks here. Evolution of XR Input, Future of Text, May 19, 2025 How to design for gaze + hand tracking, XR Design Community, 9th March 2024 Gaze + Pinch Interaction in Virtual Reality, ACM¬†Symposium on Spatial User Interaction 2017 (SUI‚Äô17) Gaze and Touch Interaction on Tablets, ACM Symposium on User Interface Software and Technology 2016 (UIST‚Äô16) Partially-indirect Bimanual Input with Gaze, Pen, and Touch for Pan, Zoom, and Ink Interaction, ACM¬†SIGCHI Conference on Human Factors in Computing Systems (CHI ‚Äô16) Extended Interaction (XI) Team We are a multidisciplinary team at Aarhus University exploring how people interact with digital worlds, through studying the synergy of fundamental human body modalities (like body, arm, hand, finger, gaze) and digital tools (applications, interface types, UI objects, interaction tasks). Please reach out for potential internships, bachelor/master thesis, research stays, collaborations and more! NameRoleResearch FocusKen PfeufferAssociate ProfessorHCI, XR, UI Design, Eye-tracking (Team Lead)Hans GellersenProfessorHCI, XR, Eye-tracking, UbicompJens Emil Gr√∏nb√¶kAssistant ProfessorCollaborative XRQiushi ZhouPostdoctoral ResearcherHCI in Extended RealityMathias N. Lystb√¶kPhD studentVisual Attention in AR for ManufacturingPavel ManakhovPhD studentMobile Spatial Interaction in XRChristoph JohnsPhD studentAdaptive Interaction & Multi-objective OptimizationJuan S√°nchez EsquivelPhD studentSpatial Interaction for XRThorbj√∏rn MikkelsenPhD studentEye-hand Manipulation in 3DUta WagnerPhD student (Alumni)Gaze-based 3D Interaction ‚Äì now Postdoc at University of Konstanz Share this: Click to share on X (Opens in new window) X Click to share on Facebook (Opens in new window) Facebook Subscribe Subscribed Ken Pfeuffer Sign me up Already have a WordPress.com account? Log in now. Ken Pfeuffer Subscribe Subscribed Sign up Log in Copy shortlink Report this content View post in Reader Manage subscriptions Collapse this bar Loading Comments... Write a Comment... Email (Required) Name (Required) Website",
  "content_length": 5356,
  "method": "requests",
  "crawl_time": "2025-12-01 13:41:35"
}