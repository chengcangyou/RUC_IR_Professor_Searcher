{
  "name": "Jaehyung Kim 0001",
  "homepage": "https://sites.google.com/view/jaehyungkim",
  "status": "success",
  "content": "Jaehyung KimSearch this siteEmbedded FilesSkip to main contentSkip to navigationJaehyung KimI am an assistant professor at the Department of AI at Yonsei University. Previously, I was a postdoc withÂ  Yiming Yang at CMU. I earned my Ph.D. at KAIST under the supervision of Prof. Jinwoo Shin. During Ph.D., I also worked closely with Prof. Dongyeop Kang at the University of Minnesota. I am a recipient of Qualcomm Innovation Fellowship Korea (2021, 2022) for three of my papers. I also receive Silver prize from Samsung Humantech Paper Awards. Â Lab homepage: Machine and Language Learning Lab (ML3)Contact: jaehyungk@yonsei.ac.kr (or wogudehowl@gmail.com )Google Scholar, CV (update on 25.01)ðŸ”¥ NewsÂ Nov 2025: LfU is accepted to AAAI 2026 as Oral Presentation!Nov 2025: The homepage of my lab (ML3 @ Yonsei AI) is finally open!Oct 2025: Jaehyung serves as an Area Chair for NeurIPS 2025, ICLR 2026, ARR 2025 Oct!Sep 2025: Two papers (Robot-R1 and [C26]) are accepted to NeurIPS 2025 including Spotlight Presentation (207/21575=3.19%)!Â Aug 2025: Three papers (CoPe, PriME and CLEANMOL) are accepted to EMNLP 2025 including Oral Presentation!May 2025: Two papers (MSR and PFP) are accepted to ACL 2025!Â May 2025: ReVise is accepted to ICML 2025!Â Jan 2025: SPA is accepted to ICLR 2025 as Oral Presentation (207/11672=1.77%)!Â ðŸ“‘ ResearchÂ My research goal is to enhance machine learning (ML) and NLP frameworks to be more accurate and reliable in real-world scenarios, by designing proper algorithms. Recently, Iâ€™ve been mostly interested in large language models (LLMs), especially in their alignment, adaptation (e.g., personalization), and development. While my recent focus has mainly been on NLP and LLMs, Iâ€™m also interested in improving ML frameworks in other domains.(C: Conference, J: Journal, W: Workshop, P: Preprint, *: Equal contribution)2026[C27] Learning from the Undesirable: Robust Adaptation of Language Models without ForgettingÂ Yunhun Nam, Jaehyung Kim, and Jongheon JeongAAAI Conference on Artificial Intelligence (AAAI) 2026 (Oral Presentation)2025[C26] Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning [pdf][code]Youngbin Seo, Dongha Lee, Jaehyung Kim, and Jinyoung YeoNeural Information Processing Systems (NeurIPS) 2025 (Spotlight Presentation, 207/21575=3.19%)[C25] Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics [pdf][code]Dongyoung Kim, Sumin Park, Huiwon Jang, Jinwoo Shin, Jaehyung Kim*, and Younggyo Seo*Neural Information Processing Systems (NeurIPS) 2025[C24] Personalized LLM Decoding via Contrasting Personal Preference [pdf][code]Hyungjune Bu*, Chanjoo Jung*, Minjae Kang, and Jaehyung KimConference on Empirical Methods in Natural Language Processing (EMNLP) 2025 (Main)[C23] Improving Chemical Understanding of LLMs via SMILES Parsing [pdf][code]Yunhui Jang, Jaehyung Kim,Â  and Sungsoo AhnConference on Empirical Methods in Natural Language Processing (EMNLP) 2025 (Main)[C22] Personalized Language Models via Privacy-Preserving Evolutionary Model Merging [pdf][code]Kyuyoung Kim, Jinwoo Shin, and Jaehyung KimConference on Empirical Methods in Natural Language Processing (EMNLP) 2025 (Oral Presentation)[C21] Debiasing Online Preference Learning via Preference Feature Preservation [pdf][code]Dongyoung Kim, Jinsung Yoon, Jinwoo Shin, and Jaehyung KimAnnual Meeting of the Association for Computational Linguistics (ACL) 2025 (Findings)[C20] Structural Reasoning Improves Molecular Understanding of LLM [pdf][code]Yunhui Jang, Jaehyung Kim,Â  and Sungsoo AhnAnnual Meeting of the Association for Computational Linguistics (ACL) 2025 (Main)Â Neural Information Processing Systems (NeurIPS) 2024, AIDrugX Workshop[C19] ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification [pdf][code]Hyunseok Lee*, Seunghyuk Oh*, Jaehyung Kim, Jinwoo Shin, and Jihoon TackInternational Conference on Machine Learning (ICML) 2025Â International Conference on Learning Representations (ICLR) 2025, Reasoning and Planning for LLMs WorkshopÂ [C18] Few-shot Personalization of LLMs with Mis-aligned Responses [pdf][code]Jaehyung Kim and Yiming YangAnnual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL) 2025 (Main)[C17] Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment [pdf][code]Dongyoung Kim, Kimin Lee, Jinwoo Shin, and Jaehyung KimInternational Conference on Learning Representations (ICLR) 2025 (Oral Presentation, 207/11672=1.77%)[J1] Alternative Mixed Integer Linear Programming Optimization for Joint Job Scheduling and Data Allocation in Grid Computing [pdf][code]Shengyu Feng*, Jaehyung Kim*, Yiming Yang, Joseph Boudreau, Tasnuva Chowdhury, Adolfy Hoisie, Raees Khan, Ozgur O Kilic, Scott Klasky, Tatiana Korchuganova, Paul Nilsson, Verena Ingrid Martinez Outschoorn, David K Park, Norbert Podhorszki, Yihui Ren, Frederic Suter, Sairam Sri Vatsavai, Wei Yang, Shinjae Yoo, Tadashi Maeno, andÂ  Alexei KlimentovFuture Generation Computer Systems[P11] Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges [pdf][code]Hamin Koo, Minseon Kim, and Jaehyung KimArxiv Preprint 2025Â [P10] Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces [pdf][code]Minju Gwak, Guijin Son, and Jaehyung KimArxiv Preprint 2025Â [P9] TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA [pdf][code]Chanjoo Jung and Jaehyung KimArxiv Preprint 2025Â [W3] Training-free LLM Verification via Recycling Few-shot Examples [pdf][code]Dongseok Lee, Jimyung Hong, Dongyoung Kim, and Jaehyung KimInternational Conference on Machine Learning (ICML) 2025 Workshop ES-FoMo-III (Spotlight Presentation, 14/146=9.59%)[W2] Â Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs [pdf][code]Beomsik Cho and Jaehyung KimInternational Conference on Machine Learning (ICML) 2025 Workshop ES-FoMo-IIIÂ [P8] Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity [pdf][code]Youngbin Seo, Gayoung Kim, Jaehyung Kim, and Jinyoung YeoArxiv Preprint 2025Â [P7] Collaborative LLM Inference via Planning for Efficient Reasoning [pdf][code]Byeongchan Lee, Jonghoon Lee, Dongyoung Kim, Jaehyung Kim, and Jinwoo ShinArxiv Preprint 2025Â [P6] LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models [pdf][code]Jieyong Kim*, Tongyoung Kim*, Soojin Yoon, Jaehyung Kim, and Dongha LeeArxiv Preprint 2025Â [P5] EMCee: Improving Multilingual Capability of LLMs via Bridging Knowledge and Reasoning with Extracted Synthetic Multilingual Context [pdf][code]Hamin Koo and Jaehyung KimArxiv Preprint 2025Â 2024[C16] Optimized Feature Generation for Tabular Data via LLMs with Decision Tree Reasoning [pdf][code]Jaehyun Nam*, Kyuyoung Kim*, Seunghyuk Oh, Jihoon Tack, Jaehyung Kim,Â  and Jinwoo ShinNeural Information Processing Systems (NeurIPS) 2024[C15] Online Adaptation of Language Models with a Memory of Amortized Contexts [pdf][code]Jihoon Tack, Jaehyung Kim, Â Eric Mitchell, Jinwoo Shin, Yee Whye Teh, and Jonathan Richard SchwarzNeural Information Processing Systems (NeurIPS) 2024[C14] Learning to Correct for QA Reasoning with Black-box LLMs [pdf][code]Jaehyung Kim, Dongyoung Kim, and Yiming YangConference on Empirical Methods in Natural Language Processing (EMNLP) 2024 (Main)[C13] Tabular Transfer Learning via Prompting LLMs [pdf][code]Jaehyun Nam, Woomin Song, Seong Hyeon Park, Jihoon Tack, Sukmin Yun, Jaehyung Kim, Kyu Hwan Oh, and Jinwoo ShinConference on Language Modeling (COLM) 2024ICML Workshop on Efficient Systems for Foundation Models (ES-FoMo) 2023[C12] Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs [pdf][code]Woomin Song*, Seunghyuk Oh*, Sangwoo Mo, Jaehyung Kim, Sukmin Yun, Jung-Woo Ha, and Jinwoo ShinInternational Conference on Learning Representations (ICLR) 2024[C11] SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs [pdf][code][slide][poster]Jaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and Jinwoo ShinInternational Conference on Learning Representations (ICLR) 2024[P4] Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity [pdf][code]Hyosoon Jang, Yunhui Jang, Jaehyung Kim,Â  and Sungsoo AhnArxiv Preprint 2024[P3] SelectLLM: Can LLMs Select Important Instructions to Annotate? [pdf][code]Ritik Sachin Parkar*, Jaehyung Kim*, Jong Inn Park, and Dongyeop KangArxiv Preprint 2024[P2] Under the Surface: Tracking the Artifactuality of LLM-Generated Data [pdf][code][project]Debarati Das*, Karin De Langis*, Anna Martin*, Jaehyung Kim*, Minhwa Lee*, Zae Myung Kim*, Shirley Hayati, Risako Owan, Bin Hu, Ritik Parkar, Ryan Koo, Jonginn Park, Aahan Tyagi, Libby Ferland, Sanjali Roy, Vincent Liu, and Dongyeop KangArxiv Preprint 2024[W1] Meta-Crafting: Improved Detection of Out-of-distributed Texts via Crafting Metadata SpaceRyan Koo, Yekyung Kim, Dongyeop Kang, and Jaehyung KimAAAI Conference on Artificial Intelligence (AAAI) Student Abstract and Poster Program 20242023[C10] RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training [pdf][code][slide][poster]Jaehyung Kim, Yuning Mao, Rui Hou, Hanchao Yu, Davis Liang, Pascale Fung, Qifan Wang, Fuli Feng, Lifu Huang, and Madian KhabsaConference on Empirical Methods in Natural Language Processing (EMNLP) 2023 (Findings)[C9] A Universal Framework for Dataset Characterization with Multidimensional Meta-information [pdf][code][slide][poster]Jaehyung Kim, Yekyung Kim, Karin Johanna Denton de Langis, Jinwoo Shin, and Dongyeop KangAnnual Meeting of the Association for Computational Linguistics (ACL) 2023 (Main)[C8] Prefer to Classify: Improving Text Classifiers via Auxiliary Preference Learning [pdf][code][slide][poster]Jaehyung Kim,Â  Jinwoo Shin, and Dongyeop KangInternational Conference on Machine Lear",
  "content_length": 14900,
  "method": "requests",
  "crawl_time": "2025-12-01 13:25:21"
}