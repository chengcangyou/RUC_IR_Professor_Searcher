{
  "name": "Amartya Sanyal",
  "homepage": "https://amartya18x.github.io",
  "status": "success",
  "content": "Amartya Sanyal Amartya Sanyal I am an Assistant Professor in Machine Learning in the Department of Computer Science in the University of Copenhagen and an Adjunct Assistant Professor at the Department of Computer Science, IIT Kanpur. I lead the Copenhagen Foundation of Responsible Machine Learning group in UCPH. Email  / GitHub  / Google Scholar  / CV Prior to this, I was a postdoctoral fellow at the Empirical Inference group in Max Planck Institute for Intelligent Systems, Tubingen where I worked closely with Prof. Bernhard Schölkopf . Before that, I was a postdoctoral fellow at ETH Zurich AI Center where I worked closely with Prof. Fanny Yang . I completed my DPhil (PhD) at the Department of Computer Science, University of Oxford, funded by the Turing Doctoral Studentship. I was also a member of the Torr Vision Group. My DPhil advisors were Varun Kanade and Philip H.S. Torr. I am also a member of the ELLIS Society. Prior to that, I completed my undergraduate (B.Tech in Computer Science) at the Indian Institute of Technology, Kanpur. On various occassions, I have spent time at Facebook AI Research (FAIR), Twitter Cortex , Laboratory for Computational and Statistical Learning, Montreal Institute of Learning Algorithms , and Amazon ML. Openings with me If you are interested in a PhD or Postdoc position with me, see here for more details If you are a student at UCPH and interested in doing a undergraduate or masters project with me, see here for more details Research Interests I am interested in understanding both theoretical and empirical aspects of Trustworthy Machine Learning including privacy, robustness, fairness. In particular, I am focusing on the impacts that inadequate data and computation may have on the trustworthiness of ML algorithms, especially under adversarial settings, and how one can solve these issues with reasonable approximations and relaxations including semi-supervised and self-supervised learning. Current Students (Copenhagen Lab for Foundation of Responsible Machine Learning) I am very lucky to be currently working with the following students. Giorgio Racca (Co-advised with Michal Valko ) Luka Radic Carolin Heinzler (Co-advised with Prof. Amir Yehudayoff) Johanna Düngler (DDSA PhD fellow; Co-advised with Prof. Rasmus Pagh) Max Cairney-Leeming (ELLIS PhD student; Co-advised with Prof. Christoph H. Lampert ) Anmol Goel (ELLIS PhD Student; Co-advised with Prof. Iryna Gurevych) Omri Ben-Dov (PhD Student; Co-advised with Dr. Samira Samadi) Yaxi Hu (PhD Student; Advised by Prof. Bernhard Schölkopf and Prof. Fanny Yang) Upcoming/Recent Talks Feb 19 ELLIS talk: Privacy with Correlated data @ Institute of Science and Technology, Austria (ISTA) Feb 25 Tutorial - Building trustworthy ML: The role of label quality and availability @ Association for Advancement of Artificial Intelligence (AAAI) Feb 28 Lower Bounds for Differentially Private Online Learning @ Columbia University Mar 11 Keynote: Machine Unlearning: Promises, Failures, and New Directions @ Huawei Future Device Technology Summit - Aurora Summit Recent News Sep, 2025 Our paper titled An Iterative Algorithm for Differentially Private k-PCA with Adaptive Noise is accepted to NeurIPS 2025. See you in San Diego and Copenhagen! Sep, 2025 I will be co-hosting two events at EurIPS 2025: the Learning Theory Alliance affinity event and the EurIPS Privacy-Preserving Machine Learning Workshop . Jan, 2025 I received the Villum Young Invesigator Grant and will be working on topics related to Privacy, unlearning, and online learning. I will be hiring motivated PhD students and Postdocs. Email me if you think you would be good match. Jan, 2025 Three papers on differentially private alignment of LLMs, Machine unlearning, and training data attribution were accepted at ICLR 2025 and one paper on OOD robustness and label noise at AISTATS 2025 . Publications SODA, 2025 Learning in an Echo Chamber: Online Learning with Replay Adversary Daniil Dmitriev, Harald Eskelund Franck, Carolin Heinzler, Amartya Sanyal Symposium on Discrete Algorithms, 2025 Proceedings / NeurIPS, 2025 An iterative algorithm for Differentially Private k-PCA with adaptive noise Johanna Duengler, Amartya Sanyal Advances in Neural Information Processing Systems, 2025 Proceedings / TPDP, 2025 Online Learning and Unlearning Yaxi Hu, Bernhard Schölkopf, Amartya Sanyal Theory and Practice of Differential Privacy, 2025 Arxiv / AISTATS, 2025 Accuracy on the wrong line: On the pitfalls of noisy data for out-of-distribution generalisation Amartya Sanyal , Yaxi Hu, Yaodong Yu, Yian Ma Yixin Wang ,Bernhard Schölkopf International Conference on Artificial Intelligence and Statistics, 2025 Arxiv / ICLR, 2025 PSA: Differentially Private Steering for Large Language Model Alignment Anmol Goel, Yaxi Hu, Iryna Gurevych, Amartya Sanyal International Conference on Learning Representations, 2025 Arxiv / Proceedings / code / ICLR, 2025 Provable unlearning in topic modeling and downstream tasks Stanley Wei, Sadhika Malladi, Sanjeev Arora, Amartya Sanyal International Conference on Learning Representations, 2025 Arxiv / Proceedings / ICLR, 2025 Protecting against simultaneous data poisoning attacks Neel Alex, Shoaib Ahmed Siddiqui, Amartya Sanyal, David Krueger International Conference on Learning Representations, 2025 Arxiv / Proceedings / TMLR, 2024 Corrective Machine Unlearning Shashwat Goel, Ameya Prabhu, Philip Torr, Ponnurangam Kumaraguru, Amartya Sanyal Transactions on Machine Learning Research, 2024 Arxiv / Proceedings / code / NeurIPS, 2024 Robust Mixture Learning when Outliers Overwhelm Small Groups Daniil Dmitriev, Rares-Darius Buhai, Stefan Tiegel, Alexander Wolters, Gleb Novikov, Amartya Sanyal , David Steurer, Fanny Yang Advances in Neural Information Processing Systems, 2024 Arxiv / NeurIPS, 2024 What Makes and Breaks Safety Fine-tuning? A Mechanistic Study Samyak Jain, Ekdeep Singh Lubana, Kemal Oksuz, Tom Joy, Philip H.S. Torr, Amartya Sanyal , Puneet K. Dokania Advances in Neural Information Processing Systems, 2024 Arxiv / TPDP, COLT, 2024 On the Growth of Mistakes in Differentially Private Online Learning: A Lower Bound Perspective Daniil Dmitriev, Kristóf Szabó, Amartya Sanyal Conference on Learning Theory, 2024 Arxiv / Proceedings / ICML, 2024 The Role of Learning Algorithms in Collective Action Omri Ben Dov* , Jake Fakes* , Samira Samadi, Amartya Sanyal International Conference on Machine Learning, 2024 Arxiv / Proceedings / TPDP, ICML, 2024 Provable Privacy with Non-Private Pre-Processing Yaxi Hu, Amartya Sanyal , Bernhard Schölkopf International Conference on Machine Learning, 2024 Arxiv / Proceedings / TPDP, AISTATS, 2024 Certified private data release for sparse Lipschitz functions Konstantin Donhauser, Johan Lokna, Amartya Sanyal , March Boedihardjo, Robert Hönig, Fanny Yang, Artificial Intelligence and Statistics Theory and Practice of Differential Privacy , 2024 Arxiv / Proceedings / NeurIPS, 2023 Spotlight Paper Can semi-supervised learning use all the data effectively? A lower bound perspective Gizem Yüce* , Alexandru Țifrea , Amartya Sanyal , Fanny Yang Advances in Neural Information Processing Systems, 2023 Spotlight Paper Arxiv / Proceedings / TPDP, SaTML, 2023 PILLAR: How to make semi-private learning more effective Francesco Pinto, Yaxi Hu, Fanny Yang, Amartya Sanyal IEEE Conference on Secure and Trustworthy Machine Learning Theory and Practice of Differential Privacy Workshop on Pitfalls of limited data and computation for Trustworthy ML , 2023 Arxiv / Proceedings / code / poster / TMLR, 2023 Catastrophic overfitting can be induced with discriminative non-robust features Guillermo Ortiz-Jiménez, Pau de Jorge, Amartya Sanyal , Adel Bibi, Puneet Dokania, Pascal Frossard, Gregory Rogéz, Philip H.S. Torr TMLR , 2023 Arxiv / code / ICML, 2023 Certifying Ensembles: A General Certification Theory with S-Lipschitzness Aleksandar Petrov, Francisco Eiras, Amartya Sanyal , Philip H.S. Torr, Adel Bibi International Conference on Machine Learning , 2023 Arxiv / NeurIPS Workshop, 2023 Oral Paper How robust accuracy suffers from certified training with convex relaxations Piersilvio De Bartolomeis, Jacob Clarysse, Fanny Yang, Amartya Sanyal Workshop on Understanding Deep Learning Through Empirical Falsification , 2023 Oral Paper Arxiv / Proceedings / Preprint, 2023 Towards Adversarial Evaluations for Inexact Machine Unlearning Shashwat Goel, Ameya Prabhu, Amartya Sanyal , Ser-Nam Lim, Philip Torr, Ponnurangam Kumaraguru Arxiv, 2023 Arxiv / ICLR, 2023 A law of adversarial risk, interpolation, and label noise Daniel Paleka*, Amartya Sanyal* International Conference on Learning Representations, 2023 Arxiv / Proceedings / poster / ICLR, 2023 How robust is unsupervised representation learning to distribution shift? Yuge Shi, Imant Daunhawer, Julia E. Vogt, Philip H.S. Torr , Amartya Sanyal . International Conference on Learning Representations, 2023 Arxiv / Proceedings / NeurIPS, 2022 Make Some Noise: Reliable and Efficient Single-Step Adversarial Training Pau De Jorge Aranda, Adel Bibi , Ricardo Volpi, Amartya Sanyal , Philip H.S. Torr, Grégory Rogez , Puneet Dokania Neural Information Processing Systems (NeurIPS) 2022., 2022 Arxiv / Proceedings / UAI, 2022 Oral Paper How unfair is private learning ? Amartya Sanyal , Yaxi Hu, Fanny Yang Conference on Uncertainty in Artificial Intelligence (UAI) , 2022 Oral Paper Arxiv / Proceedings / poster / slides / COLT, 2022 Open Problem: Do you pay for Privacy in Online Learning ? Amartya Sanyal , Giorgia Ramponi Conference on Learning Theory (COLT) Open Problems, 2022 Arxiv / Proceedings / slides / ICLR, 2021 Spotlight Paper How benign is benign overfitting? Amartya Sanyal , Varun Kanade, Puneet Dokania, Philip H.S. Torr International Conference of Learning Representations , 2021 Spotlight Paper Arxiv / Proceedings / poster / ICLR, 2021 Progressive Skeletonization: Trimming more fat from a network at initialization Pau De Jorge Aranda, Amartya Sanyal , Ha",
  "content_length": 11300,
  "method": "requests",
  "crawl_time": "2025-12-01 12:55:17"
}