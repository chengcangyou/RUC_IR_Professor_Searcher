{
  "name": "Mikhail Khodak",
  "homepage": "https://pages.cs.wisc.edu/~khodak",
  "status": "success",
  "content": "Misha's Homepage Misha Khodak Vita Publications FAQ I am an assistant professor of computer sciences at the University of Wisconsin-Madison, where I study foundations and applications of machine learning. Most recently, I have been working on specialized foundation models and on incorporating AI tools into algorithm design and scientific computing. My body of work includes fundamental theory for modern meta-learning (scalable methods that \"learn-to-learn\" using multiple learning tasks as data) and end-to-end guarantees for learning-augmented algorithms (algorithms that incorporate learned predictions about their instances to improve performance). These results are based on a set of theoretical tools that port the idea of surrogate upper bounds from supervised learning to learning algorithmic cost functions. In addition to providing natural measures of task-similarity, this approach often yields effective and practical methods, such as for personalized federated learning and scientific computing. I have also led the push to develop automated ML methods for diverse tasks and have worked on efficient deep learning, neural architecture search, and natural language processing. Previously, I spent a year as a fellow at Princeton Language & Intelligence after completing a PhD in CS at CMU. To get in touch, email me at khodak@wisc.edu. Please read my FAQ first if you are a prospective advisee or student. Recent Work: Specialized Foundation Models Struggle to Beat Supervised Baselines. ICLR 2025. Zongzhe Xu*, Ritvik Gupta*, Wenduo Cheng, Alexander Shen, Junhong Shen, Ameet Talwalkar, Mikhail Khodak. [paper] [arXiv] [DASHA code] [Auto-AR code] SureMap: Simultaneous Mean Estimation for Single-Task and Multi-Task Disaggregated Evaluation. NeurIPS 2024. Mikhail Khodak, Lester Mackey, Alexandra Chouldechova, Miroslav DudÃ­k. [paper] [arXiv] [code] [poster] Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances. ICLR 2024. Mikhail Khodak, Edmond Chow, Maria-Florina Balcan, Ameet Talwalkar. [paper] [arXiv] [code] [poster] [OpenFOAM package] Selected Papers: Hovering over an image reveals a paper summary and retrospective. One of the many remarkable properties of so-called \"foundation models\" is that they can obtain non-trivially good performance when fine-tuned on out-of-modality downstream tasks, e.g. transferring from text to protein sequences. As a result, one could consider using them to quickly and automatically obtain good models on diverse tasks across many different modalities, a key objective of modern AutoML. However, direct fine-tuning is not competitive with more traditional approaches, e.g. architecture search, for this purpose. In this paper we solve this problem by adding an alignment step to the fine-tuning workflow, in which we first align the target inputs to look more like the pretraining data. This is done by passing the target inputs through an embedder trained to minimize the optimal transport dataset distance between its outputs and a dataset similar to the pretraining data (e.g. CoNLL for RoBERTa or CIFAR for Swin Transformer). After alignment, we refine the model using regular fine-tuning on the aligned inputs. This workflow, which we call ORCA, outperforms hand-designed architectures on 9 out of 10 tasks in NAS-Bench-360 and beats the previous SOTA automated method DASH on all but two. It is further competitive on PDEBench with the popular U-Net, PINN, and FNO architectures for PDE-solving and beats XGBoost on a suite of tabular tasks, almost matching Amazon's AutoGluon-Tabular. ORCA's empirical success opens up vast possibilities for cross-modal transfer of foundation models, suggesting that scaling them up can help downstream tasks far beyond the vision, text, and audio modalities on which they are usually pretrained. Cross-Modal Fine-Tuning: Align then Refine. ICML 2023. Junhong Shen, Liam Li, Lucio M. Dery, Corey Staten, Mikhail Khodak, Graham Neubig, Ameet Talwalkar. [paper] [arXiv] [code] [slides] The field of algorithms with predictions (a.k.a. learning-augmented algorithms) designs algorithms whose performance (e.g. runtime) improves with a good prediction or hint about the instance they are run on. Our work develops a systematic way to answer a crucial question in this area: where do the predictions come from? As the alternative name suggests, predictions often come from learning, but prior to our work the question of learnability—i.e. whether predictions can be learned with polynomially many instances or sublinear regret—had been under-addressed. We show that for many algorithms with predictions, existing cost functions that are challenging to analyze can be relaxed into surrogate bounds that are easy to online-learn, all while incurring only a small penalty. As a consequence, we show dramatically improved learning-theoretic results for several graph algorithms, e.g. for minimum-weight bipartite matching with predictions on graphs with at most n nodes, our sample complexity guarantee is O(n2) times better than the previous best bounds. We also study several online algorithms and show the first learning guarantees for learning-augmented caching and online page migration. In most cases, our results also extend to learning linear models from instance features to predictions, or linear auto-regressive models from past states to actions in online algorithms. Overall, our relaxation-based approach suggests that learning-augmented guarantees might best be viewed as surrogate algorithmic losses, in that just like we never optimize the actual 0-1 error when training a binary classifier, preferring instead a convex objective like the log-loss, for data-driven algorithms we can also get away with optimizing a nice surrogate function rather than the actual cost of the algorithm, which is rarely convex or even continuous. Since its publication, the ideas in our work have directly inspired learning-theoretic guarantees for several problems in learning-augmented discrete convex optimization. In my own work, we have used a similar approach to design algorithms for learning to release differentially private statistics—e.g. quantiles or covariance estimates—across related datasets. Learning Predictions for Algorithms with Predictions. NeurIPS 2022. Mikhail Khodak, Maria-Florina Balcan, Ameet Talwalkar, Sergei Vassilvitskii. [paper] [arXiv] [poster] [talk] In federated learning (FL), the goal is to train an model on the data of a heterogeneous network of devices, without sending all their data to a central server. Among other complications, this causes many challenges for a crucial aspect of ML: hyperparameter tuning. In particular, we often can't even compute a good estimate of validation performance because devices are often unavailable, and the high costs of training on weak devices makes the multiple runs required by standard approaches such as random search exorbitantly expensive. Can we tune hyperparameters in only a few training runs while making good use of noisy validation data? We propose FedEx, a method that tackles this problem for local hyperparameters, a subset of all hyperparameters that arises because of the local training approach used by most federated optimizers such as FedAvg. At each communication round, such optimizers run identically initialized local SGD on each device in a batch, and then move the initialization closer to some aggregation (e.g. the average) of the last iterates. FedEx works by running these local SGD algorithms with different randomly sampled hyperparameters on each device and using the last iterates' performances on device validation data as signal to update the hyperparameter distribution. Drawing upon the connection between (personalized) FL and meta-learning, we use ARUBA to show that—when devices have sufficiently similar data—FedEx provably finds a good local step-size in the convex setting when the devices are sufficiently similar. Empirically, our method is applicable to any local hyperparameter and we find that it consistently improves the performance of vanilla tuning across several stanard FL tasks. More recently, the utility of FedEx has been independently confirmed by the authors of FedHPO-Bench, a benchmark dedicated to hyperparameter tuning in FL; they show that applying our method is beneficial in 11 out of 12 evaluation settings. Federated Hyperparameter Tuning: Challenges, Baselines, and Connections to Weight-Sharing. NeurIPS 2021. Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina Balcan, Virginia Smith, Ameet Talwalkar. [paper] [arXiv] [code] [poster] [slides] [talk] While neural architecture search (NAS) has attained significant success in designing architectures for well-studied tasks, especially in computer vision, a major original motivation was to automatically design neural networks for diverse, understudied tasks beyond vision, text, and audio. Our paper studies whether it is possible to design and efficiently search a space of architectures that can be competitive with hand-designed approaches on such tasks. We identify parameterized operations such as convolutions as the key ingredient, and ask an ambitious question: can we find the right operation for a given task, in the same way that convolution is the \"right\" operation for vision? To do so, we design a search space that generalizes convolutions to any operation that can be expressed in an efficient form that we call Expressive Diagonalization (XD); it includes regular convolutions, graph convolutions, and Fourier neural operators (FNOs), among others, while preserving those operations' asymptotic efficiency. Empirically, we find XD operations that are competitive with hand-designed architectures on many tasks, including permuted image classification, solving partial differential equations, predicting protein folding, and music modeling. Beyond its empirical results, our work initiated the new direction of automated ML for diverse tasks, which ",
  "content_length": 17825,
  "method": "requests",
  "crawl_time": "2025-12-01 13:59:52"
}