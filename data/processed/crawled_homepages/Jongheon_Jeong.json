{
  "name": "Jongheon Jeong",
  "homepage": "https://tail.korea.ac.kr",
  "status": "success",
  "content": "TAIL @ KUSearch this siteEmbedded FilesSkip to main contentSkip to navigationTrustworthy AI Lab at Korea UniversityWe conduct foundational research to bring AI into societyNewsNov 2025: Jongheon will serve as an Area Chair for ICML 2026.Nov 2025: Two papers have been accepted to AAAI 2026, including one oral presentation.Sep 2025: \"BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing\" has been accepted to NeurIPS 2025.Aug 2025: Jongheon will serve as an Area Chair for ICLR 2026.June 2025: \"FaceShield: Defending Facial Image against Deepfake Threats\" has been accepted to ICCV 2025.May 2025: \"StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment\" has been accepted to IJCAI 2025.Jan 2025: Two papers have been accepted to ICLR 2025, including one oral presentation.Nov 2024: \"Confidence-aware Denoised Fine-tuning of Off-the-shelf Models for Certified Robustness\" has been accepted to TMLR.Jul 2024: \"Adversarial Robustification via Text-to-Image Diffusion Models\" has been accepted to ECCV 2024 as an oral presentation.Feb 2024: Our lab's website has just been launched! Who we areThe Trustworthy AI Lab at Korea University (or TAIL for short) is a research group led by Prof. Jongheon Jeong in the Department of Artificial Intelligence. Our mission is to make recent developments in AI not only powerful but also trustworthy and reliable, so that they can be more beneficial when integrated with society. Open positions: I am looking for self-motivated, curiosity-driven graduate students and undergraduate interns to join our lab. Please reach out to me via email with your CV and transcript if you are interested.What we doWe conduct research on learning representations that are both useful and societally acceptable. We focus on developing ideas and algorithms that are (a) generalizable in out-of-distribution scenarios and (b) scalable within modern AI-based systems, so that they can be additive in building safer AI as a complex system. Several important research directions we address include, but are not limited to, the following:AI Safety - \"Are we truly prepared to expose AI to the public, even to its potential hazards?\"Robustness: adversarial machine learning, out-of-distribution generalization, test-time adaptation, etc.Monitoring: novelty/anomaly detection, uncertainty estimation, interpretable AI, etc.Alignment: preference optimization, believable agents, reward modeling, etc.Responsibility: copyright protection, deepfake prevention, fairness, privacy, etc. Foundation Models - \"What properties emerge at scale? Is scale either sufficient or necessary for trustworthiness?\"Generative AI: diffusion and flow-based models, language models, high-dimensional vision, etc.Scalable Representations: multimodal learning, self-supervised learning, robust fine-tuning, etc.Not only for these topics, we are broadly interested in advancing foundational areas of machine learning and deep learning at large. Google SitesReport abusePage detailsPage updated Google SitesReport abuse",
  "content_length": 3058,
  "method": "requests",
  "crawl_time": "2025-12-01 13:34:11"
}