{
  "name": "Samson Zhou",
  "homepage": "https://samsonzhou.github.io",
  "status": "success",
  "content": "Samson Zhou Samson Zhou â˜€ï¸ ðŸŒ™ Hi, I'm an Assistant Professor in the Department of Computer Science & Engineering at Texas A&M University. My research lies at the intersections of theoretical computer science, data science, and machine learning. In particular, I am currently interested in numerical linear algebra, streaming algorithms, and differential privacy. Here are some of my favorite links. I'm looking for PhD students and postdocs interested in broadly working in sublinear algorithms, differential privacy, or the theoretical foundations of machine learning, for the Fall 2026 cycle. Please contact me directly if interested! For Texas A&M University undergraduate students: If you are interested in research and have a solid background in computer science and/or mathematics, feel free to reach out anytime to explore opportunities. Profiles: [dblp] [Scholar] E-mail: samsonzhou AT gmail DOT com Academic Positions and Education Texas A&M University, Assistant Professor (August 2023-Present) Simons Institute for the Theory of Computing, Sublinear Algorithms, Visting Scientist (Summer 2024) UC Berkeley and Rice University, Postdoctoral Researcher, hosted by --jelaninelson-- and --vladimirbraverman-- (September 2022-July 2023) Carnegie Mellon University, Postdoctoral Researcher, hosted by --davidwoodruff-- (September 2019-August 2022) Indiana University, Postdoctoral Researcher, hosted by --grigoryyaroslavtsev-- (August 2018-2019) Purdue University, Postdoctoral Researcher, hosted by --jeremiahblocki-- (Summer 2018) Purdue University, Doctor of Philosophy, Computer Science, advised by --gregfrederickson-- and --elenagrigorescu-- (2018) Massachusetts Institute of Technology, Master of Engineering, Computer Science (2011) Massachusetts Institute of Technology, Bachelor of Science, Computer Science (2011) Massachusetts Institute of Technology, Bachelor of Science, Mathematics (2010) Teaching Spring 2026: CSCE 411: Design and Analysis of Algorithms Fall 2025: CSCE 411: Design and Analysis of Algorithms Spring 2025: CSCE 411: Design and Analysis of Algorithms Fall 2024: CSCE 411: Design and Analysis of Algorithms Spring 2024: CSCE 658: Randomized Algorithms Fall 2023: CSCE 689: Special Topics in Modern Algorithms for Data Science Spring 2018: CS 584: Theory of Computation/Complexity Theory (Teaching Assistant) Spring 2016: CS 381: Introduction to the Analysis of Algorithms (Teaching Assistant) Fall 2015: CS 580: Algorithm Design and Analysis (Teaching Assistant, ACM Graduate TA Award) Service September 15-17, 2025: Co-organizer of the EnCORE Workshop: New Horizons for Adaptive Robustness September 13, 2025: Co-organizer of the Texas Area Theory and Algorithms Day April 14-16, 2025: Co-organizer of the Workshop on Algorithms for Large Data (Online), i.e., WALDO 2025 April 7, 2025: Texas A&M University CSCE Student Poster Expo Fall 2023-Present: Problem-Solving Session Instructor at the TAMU Math Circle Fall 2023-Present: Co-organizer of the Algorithms and Data Science Reading Group at Texas A&M University August 19-21, 2024: Co-organizer of the TTIC Workshop: Learning-Augmented Algorithms November 6-9, 2023: Co-organizer of the FOCS 2023 Workshop: Exploring the Frontiers of Adaptive Robustness August 23-25, 2021: Co-organizer of the Workshop on Algorithms for Large Data (Online) Fall 2018: Organizer of Convex Optimization Reading Group at Indiana University Fall 2016-Fall 2017: Organizer of Theoretical CS Reading Group at Purdue University Program committee/area chair: ICLR 2026, SOSA 2026, NeurIPS 2025, APPROX 2025, RANDOM 2025, ICML 2025, NeurIPS 2024, ESA 2024, ISAAC 2024, COLT 2024, COLT 2023, COLT 2022, RANDOM 2022 Reviewer: 2026: AISTATS, ITCS, SODA, SOSA 2025: AISTATS, APPROX, ESA, FOCS, ICALP, ICLR, IPCO, ISIT, ITCS, PODS, RANDOM, SODA, STOC 2024: AISTATS, COLT, ESA, FOCS, ICALP, ICDT, ICLR, ICML, INFOCOM, ISAAC, ISIT, ITCS, NeurIPS, PODS, RANDOM, SoCG, SODA, STACS, STOC 2023: COLT, ESA, FOCS, ICALP, ICML, ITCS, NeurIPS, PODS, SIGMETRICS, SODA, STACS, STOC 2022: AAAI, AISTATS, COLT, ICALP, ICLR, ICML, ITCS, NeurIPS, PODS, RANDOM, SODA, SPAA, STOC 2021: AAAI, CPM, ESA, ICDT, ISAAC, NeurIPS, PODS, SODA, STOC 2020: ESA, FOCS, IPCO, ISIT, ITCS, NeurIPS, PODS, RANDOM, SODA, STOC 2019: AISTATS, CSR, ESA, FC, FOCS, ICANN, ICML, ISAAC, ISIT, ITCS, NeurIPS, RANDOM, SODA, SPAA, STOC 2018: APPROX, ESA, ICALP, ISIT, LATIN, SODA, STOC 2017: CSR, FOCS, SPAA 2016: RANDOM Journals: Algorithmica, ACM Transactions on Algorithms, IEEE Transactions on Information Theory, SIAM Journal on Computing, SIAM Journal on Matrix Analysis and Applications, Information Processing Letters, Journal of Computer and System Sciences Publications Lp Sampling in Distributed Data Streams with Applications to Adversarial Robustness --honghaolin--, --zhaosong--, --davidwoodruff--, --shenghaoxie--, Samson Zhou SODA 2026 [abstract] [pdf] In the distributed monitoring model, a data stream over a universe of size $n$ is distributed over $k$ servers, who must continuously provide certain statistics of the overall dataset, while minimizing communication with a central coordinator. In such settings, the ability to efficiently collect a random sample from the global stream is a powerful primitive, enabling a wide array of downstream tasks such as estimating frequency moments, detecting heavy hitters, or performing sparse recovery. Of particular interest is the task of producing a perfect $L_p$ sample, which given a frequency vector $f \\in \\mathbb{R}^n$, outputs an index $i$ with probability $\\frac{f_i^p}{\\|f\\|_p^p}+\\frac{1}{\\mathrm{poly}(n)}$. In this paper, we resolve the problem of perfect $L_p$ sampling for all $p\\ge 2$ in the distributed monitoring model. Specifically, our algorithm runs in $k^{p-1} \\cdot \\mathrm{polylog}(n)$ bits of communication, which is optimal up to polylogarithmic factors. Utilizing our perfect $L_p$ sampler, we achieve adversarially-robust distributed monitoring protocols for the $F_p$ moment estimation problem, where the goal is to provide a $(1+\\varepsilon)$-approximation to $f_1^p+\\ldots+f_n^p$. Our algorithm uses $\\frac{k^{p-1}}{\\varepsilon^2}\\cdot\\mathrm{polylog}(n)$ bits of communication and achieves optimal bounds up to polylogarithmic factors, matching long-standing lower bounds by Woodruff and Zhang (STOC 2012). Crucially, we use our perfect $L_p$ sampler in conjunction with a novel bivariate Taylor series expansion to construct a difference estimator, which approximates the growth of a function between two different times and may be of independent interest. Finally, we apply our framework to achieve near-optimal adversarially robust distributed protocols for central problems such as counting, frequency estimation, heavy-hitters, and distinct element estimation. Online Learning with Limited Information in the Sliding Window Model --vladimirbraverman--, --sumeghagarg--, --chenwang--, --davidwoodruff--, Samson Zhou SODA 2026 [abstract] Motivated by recent work on the experts problem in the streaming model, we consider the experts problem in the sliding window model. The sliding window model is a well-studied model that captures applications such as traffic monitoring, epidemic tracking, and automated trading, where recent information is more valuable than older data. Formally, we have $n$ experts, $T$ days, the ability to query the predictions of $q$ experts on each day, a limited amount of memory, and should achieve the optimal regret $\\sqrt{W}\\text{polylog}(nT)$ regret over a window of the last $W$ days at any time. While it is impossible to achieve such regret with $1$ query, we show that with $2$ queries we can achieve such regret and with only $\\text{polylog}(nT)$ bits of memory. Not only are our algorithms optimal for sliding windows, but we also show for every interval $\\mathcal{I}$ of days that we achieve $\\sqrt{|\\mathcal{I}|}\\text{polylog}(nT)$ regret with $2$ queries and only $\\text{polylog}(nT)$ bits of memory, providing an exponential improvement on the memory of previous interval regret algorithms. Building upon these techniques, we address the bandit problem in data streams, where $q=1$, achieving $n T^{2/3}\\text{polylog}(T)$ regret with $\\text{polylog}(nT)$ memory, which is the first sublinear regret in the streaming model in the bandit setting with polylogarithmic memory; this can be further improved to the optimal $\\mathcal{O}(\\sqrt{T n})$ regret if the best expert's losses are in a random order. Perfect Lp Sampling with Polylogarithmic Update Time --williamswartworth--, --davidwoodruff--, Samson Zhou FOCS 2025 [abstract] Perfect $L_p$ sampling in a stream was introduced by Jayaram and Woodruff (FOCS, 2018) as a streaming primitive which, given turnstile updates to a vector $x \\in \\{-\\poly(n), \\ldots, \\poly(n)\\}^n$, outputs an index $i \\in \\{1, 2, \\ldots, n\\}$ such that the probability of returning index $i$ is exactly \\[\\frac{|x_i|^p}{\\|x\\|_p^p} \\pm \\frac{1}{n^C},\\] where $C > 0$ is an arbitrarily large constant. They achieve the optimal $\\tilde{O}(\\log^2 n)$ bits of memory for $0 < p < 2$. An important question left open by that work is to improve the time needed to process each new stream update, which is at least $n^C$ on every single update. For $0 < p < 2$, we give the first perfect $L_p$-sampler with the same optimal amount of memory but with only $\\poly(\\log n)$ update time. Crucial to our result is an efficient simulation of a sum of reciprocals of powers of truncated exponential random variables by approximating its characteristic function, using the Gil-Paelez inversion formula, and applying variants of the trapezoid formula to quickly approximate it. On Fine-Grained Distinct Element Estimation --iliasdiakonikolas--, --danielkane--, --jasperlee--, --thansispittas--, --davidwoodruff--, Samson Zhou ICML 2025 [abstract] [poster] [pdf] We study the problem of distributed distinct element estimation, where $\\alpha$ servers each receive a subset of a universe $[n]$ and aim to compute a $(1+\\varepsilon)$-",
  "content_length": 147442,
  "method": "requests",
  "crawl_time": "2025-12-01 14:23:17"
}