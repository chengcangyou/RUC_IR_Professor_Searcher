{
  "name": "He Wang 0010",
  "homepage": "https://hughw19.github.io",
  "status": "success",
  "content": "He Wang He Wang News Publications Awards Teaching Professional Service Opportunities Home / Lab He Wang Prof. He Wang Tenure-track Assistant Professor at Peking University Director of Embodied Perception and InteraCtion (EPIC) Lab Director of PKU-Galbot Joint Lab of Embodied AI I am a tenure-track assistant professor in the Center on Frontiers of Computing Studies (CFCS) at Peking University. I founded and lead the Embodied Perception and InteraCtion (EPIC) Lab, with the mission of developing generalizable skills and embodied multimodal embodied multimodal large models for robots to facilitate embodied AGI. I am also the founder and CTO of Beijing Galbot Co., Ltd., and the director of the BAAI Center of Embodied AI. TrackVLA is a product-level navigation large model introduced by Galbot, capable of pure visual environmental perception and driven by natural language instructions. GraspVLA is the world's first end-to-end embodied grasping foundation model. Its pre-training is entirely based on billion-scale \"vision-language-action\" synthetic data. GroceryVLA is the world's first end-to-end embodied VLA large model designed for the retail industry. Material-agnostic generalized grasping technology, capable of handling scenes with completely transparent objects with high success. 《DexGraspNet》ICRA 2023 Best Manipulation Paper Nominee，Million-level dexterous hands Dataset. The world's first open command large model system for object pick-and-place, capable of controlling object orientation: Open6DOR. The world's first generalized embodied navigation large model: NaVid. TrackVLA is a product-level navigation large model introduced by Galbot, capable of pure visual environmental perception and driven by natural language instructions. GraspVLA is the world's first end-to-end embodied grasping foundation model. Its pre-training is entirely based on billion-scale \"vision-language-action\" synthetic data. GroceryVLA is the world's first end-to-end embodied VLA large model designed for the retail industry. Material-agnostic generalized grasping technology, capable of handling scenes with completely transparent objects with high success. 《DexGraspNet》ICRA 2023 Best Manipulation Paper Nominee，Million-level dexterous hands Dataset. The world's first open command large model system for object pick-and-place, capable of controlling object orientation: Open6DOR. The world's first generalized embodied navigation large model: NaVid. NEWS Two papers get accepted to NeurIPS,with one selected as a spotlight. I am invited to be a speaker in Area Chair Workshop at ICCV 2025. I am invited to be a speaker in the 3rd edition of the TRICKY workshop(Transparent & Reflective Objects in the Wild Challenges) at ICCV 2025. I am invited to be a speaker in the 1st Workshop and Challenge on Category-Level Object Pose Estimation in the Wild at ICCV 2025. I am invited to be a speaker in the Workshop on Human-aware Embodied AI at IROS 2025. I am invited to be a speaker in the 2nd AI Meets Autonomy: Vision, Language, and Autonomous Systems Workshop at IROS 2025. Three papers get accepted to CoRL 2025. One paper gets accepted to SCIENCE CHINA Technological Sciences. Two papers get accepted to ICCV 2025, with one selected as a Highlight. One paper gets accepted to RA-L. Two papers get accepted to RSS 2025. Two papers get accepted to CVPR 2025. Two papers get accepted to RA-L. Five papers get accepted to ICRA 2025. One paper gets accepted to TPAMI. Four papers get accepted to CoRL 2024. SAGE,won the Best Paper Award at RSS 2024 SemRob Workshop. Two papers get accepted to IROS 2024. Two papers get accepted to RSS 2024. I accepted an interview from Xinhua News Agency's Economic Information Daily titled \"Humanoid Robots Open the Blueprint of Embodied Intelligence.\" I accepted an interview from 36Kr titled \"After the Impact of Large Models on the Humanoid Robot Track, a Trillion-Dollar Market's New Narrative.\" I was invited to participate in CCTV-2's \"Dialogue\" program on the theme of \"Humanoid Robots: Coexisting with Humans.\" Two papers get accepted to CVPR 2024. Three papers get accepted to ICRA 2024 and one paper gets accepted by RAL. Our 3D dexterous grasping policy learning paper, UniDexGrasp++, receives ICCV 2023 best paper finalist. One paper gets accepted to SIGGRAPH Asia 2023. Three papers get accepted to ICCV 2023 with UniDexGrasp++ receiving final reviews of all strong accepts (the highest ratings). I am invited to be a speaker in HANDS workshop at ICCV 2023. I am invited to be a speaker in Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition Workshop at CoRL 2023. I am invited to be a speaker in Workshop on Symmetries in Robot Learning at RSS 2023. Our dexterous grasping synthesis and dataset paper, DexGraspNet, is selected as a finalist of ICRA 2023 outstanding paper in manipulation (top 1% of submissions). Seven papers get accepted to CVPR 2023 with GAPartNet receiving highlight (top 2.5% of submissions) with final reviews of all accepts (the highest ratings). ... SELECTED PUBLICATIONS DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge Wenyao Zhang*, Hongsi Liu*, Zekun Qi*, Yunnan Wang*, XinQiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, He Wang, Zhizheng Zhang, Li Yi, Wenjun Zeng, Xin Jin† NeurIPS 2025 arXiv Project SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation Zekun Qi*, Wenyao Zhang*, Yufei Ding*, Runpei Dong, XinQiang Yu, Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, Jiazhao Zhang, Jiawei He, Jiayuan Gu, Xin Jin, Kaisheng Ma, Zhizheng Zhang†, He Wang†, Li Yi† NeurIPS 2025(spotlight) arXiv Project bibtex bibtex: @article{sofar25, title={Sofar: Language-grounded orientation bridges spatial reasoning and object manipulation}, author={Qi, Zekun and Zhang, Wenyao and Ding, Yufei and Dong, Runpei and Yu, Xinqiang and Li, Jingwen and Xu, Lingyun and Li, Baoyu and He, Xialin and Fan, Guofan and others}, journal={arXiv preprint arXiv:2502.13143}, year={2025} } Advancing general robotic manipulation with multimodal foundation models: Anembodied Al paradigm Shifeng HUANG , He WANG , Xing ZHOU , Wenkai CHEN , Haibin YANG , Jianwei ZHANG SCIENCE CHINA Technological Sciences arXiv TrackVLA: Embodied Visual Tracking in the Wild Shaoan Wang∗，Jiazhao Zhang∗, Minghan Li，Jiahang Liu，Anqi Li，Kui Wu， Fangwei Zhong，Junzhi Yu，Zhizheng Zhang†, He Wang† CoRL 2025 arXiv Project FetchBot: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real Weiheng Liu*, Yuxuan Wan*, Jilong Wang, Yuxuan Kuang, Wenbo Cui, Xuesong Shi, Haoran Li, Dongbin Zhao, Zhizheng Zhang†, He Wang† CoRL 2025（Oral） arXiv Project bibtex @inproceedings{liufetchbot, title={FetchBot: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real}, author={Liu, Weiheng and Wan, Yuxuan and Wang, Jilong and Kuang, Yuxuan and Shi, Xuesong and Li, Haoran and Zhao, Dongbin and Zhang, Zhizheng and Wang, He}, booktitle={9th Annual Conference on Robot Learning} } GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Heming Cui, Zhizheng Zhang, He Wang† CoRL 2025 arXiv Project bibtex @article{deng2025graspvla, title={GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data}, author={Shengliang Deng and Mi Yan and Songlin Wei and Haixin Ma and Yuxin Yang and Jiayi Chen and Zhiqi Zhang and Taoyu Yang and Xuheng Zhang and Wenhao Zhang and Heming Cui and Zhizheng Zhang and He Wang}, year={2025}, eprint={2505.03233}, archivePrefix={arXiv}, primaryClass={cs.RO}, url={https://arxiv.org/abs/2505.03233} } DexVLG: Dexterous Vision-Language-Grasp Model at Scale Jiawei He*, Danshi Li*, Xinqiang Yu*, Zekun Qi, Wenyao Zhang, Jiayi Chen, Zhaoxiang Zhang†, Zhizheng Zhang†, Li Yi†, He Wang† ICCV 2025（highlight） arXiv Project bibtex @misc{he2025dexvlgdexterousvisionlanguagegraspmodel, title={DexVLG: Dexterous Vision-Language-Grasp Model at Scale}, author={Jiawei He and Danshi Li and Xinqiang Yu and Zekun Qi and Wenyao Zhang and Jiayi Chen and Zhaoxiang Zhang and Zhizheng Zhang and Li Yi and He Wang}, year={2025}, eprint={2507.02747}, archivePrefix={arXiv}, primaryClass={cs.CV}, url={https://arxiv.org/abs/2507.02747}, } DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation Jiangran Lyu, Ziming Li, Xuesong Shi , Chaoyi Xu, Yizhou Wang†, He Wang† ICCV 2025 arXiv Project bibtex @article{lyu2025dywa, title={DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation}, author={Lyu, Jiangran and Li, Ziming and Shi, Xuesong and Xu, Chaoyi and Wang, Yizhou and Wang, He}, journal={arXiv preprint arXiv:2503.16806}, year={2025} } RoboHanger: Learning Generalizable Robotic Hanger Insertion for Diverse Garments Yuxing Chen, Songlin Wei,Bowen Xiao, Jiangran Lyu, Jiayi Chen, Feng Zhu and He Wang† RA-L arXiv Project bibtex @misc{chen2025robohangerlearninggeneralizablerobotic, title={RoboHanger: Learning Generalizable Robotic Hanger Insertion for Diverse Garments}, author={Yuxing Chen and Songlin Wei and Bowen Xiao and Jiangran Lyu and Jiayi Chen and Feng Zhu and He Wang}, year={2025}, eprint={2412.01083}, archivePrefix={arXiv}, primaryClass={cs.RO}, url={https://arxiv.org/abs/2412.01083}, } Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy Jiayi Chen , Yubin Ke , Lin Peng, He Wang† RSS 2025 arXiv Project Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks Jiazhao Zhang, Kunyu Wang, Shaoan Wang, Minghan Li, Haoran Liu, Songlin Wei, Zhongyuan Wang, Zhizheng Zhang†, He Wang† RSS 2025 arXiv Project Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable Manipulation with Tools Yang You, Bokui Shen, Congyue Deng, Haoran Geng, Songlin Wei, He Wang, Leonidas Guibas RA-L 2025 arXiv Proje",
  "content_length": 36503,
  "method": "requests",
  "crawl_time": "2025-12-01 13:19:14"
}