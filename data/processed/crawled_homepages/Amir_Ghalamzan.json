{
  "name": "Amir Ghalamzan",
  "homepage": "https://www.surrey.ac.uk/people/amir-ghalamzan",
  "status": "success",
  "content": "Dr Amir Ghalamzan | University of Surrey This website uses cookiesSome of these cookies are necessary and are used to help make our site work. With your consent, we will also use cookies to improve your experience, analyse site usage and assist in our marketing efforts. By clicking 'Accept all', you consent to our use of cookies.You can learn more about these cookies and manage your preferences at any time on our cookies page .Necessary Necessary These cookies enable essential website functions and ensure the site operates properly. They do not store personally identifiable information. While you can disable them in your browser settings, some features may not function correctly.Analytics Analytics These cookies help us understand how visitors interact with our website by collecting anonymous usage data. We use this information to improve website performance and enhance your experience.Personalisation Personalisation These cookies allow us to customise the content you see based on your activity, such as the courses you view, applications you make, or your location. This helps us provide a more relevant and tailored experience when using our site.Marketing Marketing These cookies deliver personalised ads based on your browsing activity and help us measure the effectiveness of our advertising campaigns. Accept all Manage preferences Save preferences Reject all Google Scholar LinkedIn ORCID ResearchGate YouTube Dr Amir Ghalamzan Associate Professor in Robotics PhD a.ghalamzan@surrey.ac.uk Intelligent Manipulation LabAcademic and research departments Computer Science Research Centre, School of Computer Science and Electronic Engineering. AboutBiographyAs of September 2025, Amir is with the University of Sheffield and the Robotics and Autonomous Systems theme lead at the Centre for Machine Intelligence. He is also an honorary Associate Professor at the University of Surrey, Computer Science.  He leads the research in Intelligent Manipulation. His primary areas of focus encompass robot learning, robotic grasping and manipulation, teleoperation, Agri-food robotics, haptics, and tactile sensors.Amir served as an Associate Professor at the University, the Lincoln Institute for Agri-Food Technology and as a Senior Lecturer at the School of Computer Science at the University of Lincoln. Preceding this, he held the position of Senior Research Fellow at the University of Birmingham from 2015 to 2018.Amir earned his Ph.D. in Robot Learning from Demonstration at Politecnico di Milano in 2015. He also holds a Second Level Specialisation in Automatic Control Engineering from Politecnico di Torino (2011) and an M.Sc. in Mechanical Engineering-Systems, Vibration, and Control from Iran University of Science and Technology (2009).His research interests lie in fundamental aspects of robot learning, and data-driven control and planning, with a strong commitment to addressing real-world challenges. Amir has particularly focused on mitigating labour shortages in the agri-food sector through the development of robots capable of functioning under extreme conditions, such as strawberry picking in high temperatures and humidity. Expand biography News 24 JUL 2025Breakthrough robotic slip-prevention method could bring human-like dexterity to industrial automation In the media 09 October 2021 2021 Meet ‘Robofruit’: The net zero robot farmer developed at the University of Lincoln Project Lead thelincolniteResearchResearch projects Past Research Projects 2020 - 2021 Cambridge Enterprise:CERES Agtech funded Robofruit; £310K (PI) As a sole investigator, I spearheaded the development of a ground-breaking, fully autonomous strawberry-picking robot (SPR) that meets all food safety and standards requirements. The SPR incorporates state-of-the-art perception, AI, motion planning, and control components that were specifically designed and integrated to achieve optimal field performance. Our tireless efforts have resulted in a patent-protected invention that has the potential to revolutionise the agricultural industry. To validate the SPR’s capabilities, we conducted extensive testing in UoL’s strawberry polytunnel and Dyson Glasshouses, ensuring its readiness for real-world applications.2021 - 2022 funded by IUK Fastpick; £150K (PI) As head of the UoL team, I played a vital role in developing a revolutionary perception and computing system for strawberry picking. The system was the first of its kind, utilising a private 5G network and edge server for computationally demanding processes. Collaborating with Saga Robotics and Robotfruit, we conducted real-time perception testing to validate its capabilities.2021 -  2022 (PI) funded by IUK Bi-SENSS: £300K (PI) As the lead at UoL, I spearheaded the development of a dynamic and kinematic robot model, a grasping method using an RGB-D sensor, and motion planning and control for transforming DEXTER from a teleoperated system to an autonomous system. Our team worked closely with Veolia, the project coordinator, as well as Faculty.ai and other partners to ensure seamless integration of the high-level decision-making AI into the autonomous robotic system.2021 - 2020 (PI) funded by Cancer Research UK ARTEMIS: £100K (PI) Under my leadership at UoL, we developed a pioneering learning-from-demonstration technique that allows a robot to learn the complex task of palpating a breast phantom directly from a human demonstration, without relying on any hand-designed features or computer vision techniques. Our approach directly maps visual information to robot action plans, resulting in highly efficient and accurate performance. Collaborating with colleagues at Imperial College London and the University of Bristol, we demonstrated the potential for using robotic systems for early breast cancer detection and screening.2021 - 2021 (PI) funded by EPSRC via NCNR CELLO: £100K (PI) Haptic-guided mobile manipulation (CELLO): my team developed a novel haptic-guided method for mobile manipulation. This allows the teleoperation of autonomous cars as well as mobile manipulators much easier as the control of the system is shared between AI and human operators.2023 - 2027 (Co-I) funded by IUK Agri Open-Core: £1,500K In this project, we aim to develop open-source software for robotic selective harvesting. I will contribute to this project by integrating motion planning into a few harvesting robots.2017 - 2022 (Co-I) funded by EPSRC NCNR: £2,000K National Centre for Nuclear Robotics (NCNR): As one of the project supervisors, I oversaw a team of Ph.D. and postdoctoral researchers and developed novel haptic-guided teleoperation systems and data-driven control methods for physical robot interaction. Our goal was to enable robust grasping and manipulation of objects in high-consequence environments.2019 - 2022 (Co-I) funded by IUK Grasp-berry: £300K As the supervisor, I led the team in the development of an innovative interactive motion planning method designed for handling complex tasks, including strawberry cluster manipulation during picking. We verified the efficacy of our approach using simulation techniques.2020 - 2020 (Co-I) funded by IUK SBRI Competition Sort and Seg: £30K As the supervisor, I led the team in the development of an innovative interactive motion planning method designed for handling complex tasks, including strawberry cluster manipulation during picking. We verified the efficacy of our approach using simulation techniques.2022 - 2022 (Co-I) funded by UKSA Space Debris removal: £30K Expand research projects PublicationsVishnu Rajendran S, Willow Mandil, Kiyanoush Nazari, Simon Parsons, Amir Ghalamzan (2024)Acoustic Soft Tactile Skin (AST Skin), In: 2024 IEEE International Conference on Robotics and Automation (ICRA)pp. 4105-4111 IEEEDOI: 10.1109/ICRA57147.2024.10610768This paper presents a novel acoustic soft tactile (AST) skin technology operating with sound waves. In this innovative approach, the sound waves generated by a speaker travel in channels embedded in a soft membrane and get modulated due to a deformation of the channel when pressed by an external force and received by a microphone at the end of the channel. The sensor leverages regression and classification methods for estimating the normal force and its contact location. Our sensor can be affixed to any robot part, e.g., end effectors or arm. We tested several regression and classifier methods to learn the relation between sound wave modulation, the applied force, and its location, respectively and picked the best-performing models for force and location predictions. The best skin configurations yield more than 93% of the force estimation within ±1.5 N tolerances for a range of 0-30 +1 N and contact locations with over 96% accuracy. We also demonstrated the performance of AST Skin technology for a real-time gripping force control application.Mohammad Sheikh Sofla, Hanita Golshanian, Vishnu Rajendran S, Amir Ghalamzan E (2024)Soft Acoustic Curvature Sensor: Design and Development, In: IEEE Robotics and Automation Letters9(11)pp. 9518-9525 Institute of Electrical and Electronics Engineers (IEEE)DOI: 10.1109/LRA.2024.3460429This paper introduces a novel Soft Acoustic Curvature (SAC) sensor. SAC incorporates integrated audio components and features an acoustic channel within a flexible structure. A reference acoustic wave, generated by a speaker at one end of the channel, propagates and is received by a microphone at the other channel's end. Our previous study revealed that acoustic wave energy dissipation varies with acoustic channel deformation, leading us to design a novel channel capable of large deformation due to bending. We then use Machine Learning (ML) models to establish a complex mapping between channel deformations and sound modulation. Various sound frequencies and ML models were evaluated to enhance curvature detection accuracy. The sensor, constructed using soft material and 3D printing, was validated experimentally, with curvature measurement e",
  "content_length": 16130,
  "method": "requests",
  "crawl_time": "2025-12-01 12:55:32"
}