{
  "name": "Mehrdad Mahdavi",
  "homepage": "http://www.cse.psu.edu/~mzm616",
  "status": "success",
  "content": "Mehrdad Mahdavi, Machine Learning and Optimization Mehrdad Mahdavi Assistant Professor Department of Computer Science & Engineering Pennsylvania State University I am a Dorothy Quiggle Career Development Assistant Professor of Computer Science & Engineering at Pennsylvania State University and an Associate Director of the Center for Artificial Intelligence Foundations and Engineered Systems (CAFÉ). I run the Machine Learning and Optimization Lab, where we work on fundamental problems in computational and theoretical machine learning. Before joining PSU in 2018, I was a Research Assistant Professor at Toyota Technological Institute, at University of Chicago. I have also worked at Voleon as a Member of Research Staff, and Microsoft Reaserch and NEC Laboratories America as a research intern. I obtained my Ph.D. under the supervision of Prof. Rong Jin from Michigan State University. I received my M.Sc in Computer Engineering department at Sharif University of Technology where my advisor was Prof. Mohammad Ghodsi, and my BS from Tehran Polytechnic. Research Interests I am broadly interested in computational and statistical machine learning, and design and analysis of randomized algorithms with a focus on (see the research page for more details): Large-scale machine learning Statistical learning theory Adversarial learning theory Convex and non-convex optimization and computational learning theory Distributed optimization Applications of machine learning Awards: NSF CAREER Award (IIS: Robust Intelligence, 2023), Mark Fulk Best Student Paper Award (Conference on Learning Theory, 2012), Top Cited Paper Award (Journal of Applied Mathematics and Computation, Elsevier, 2010) Ph.D. Thesis: Exploiting smoothness in statistical learning, sequential prediction, and stochastic optimization Selected Publications Distributed personalized empirical risk minimization with Yuyang Deng, Mohammad Mahdi Kamani, Pouria Mahdavinia Advances in Neural Information Processing Systems (NeurIPS), 2023 Mixture weight estimation and model prediction in multi-source multi-target domain adaptation with Yuyang Deng, Ilja Kuzborskij Advances in Neural Information Processing Systems (NeurIPS), 2023 Do we really need complicated model architectures for temporal networks? with Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, and Hanghang Tong International Conference on Learning Representations (ICLR), 2023 Tight analysis of extra-gradient and optimistic gradient methods for nonconvex minimax problems with Pouria Mahdavinia, Yuyang Deng, and Haochuan Li Advances in Neural Information Processing Systems (NeurIPS), 2022 Learning distributionally robust models at scale via composite optimization with Farzin Haddadpour, Mohammad Mahdi Kamani, and Amin Karbasi International Conference on Learning (ICLR), 2022 Federated learning with compression: unified analysis and sharp guarantees with Farzin Haddadpour, Mohammad Mahdi Kamani, and Aryan Mokhtari Artificial Intelligence and Statistics (AISTAT), 2021. Distributionally robust federated averaging with Yuyang Deng and Mohammad Mahdi Kamani Advances in Neural Information Processing Systems (NeurIPS), 2020. GCN meets GPU: decoupling “When to Sample” from “How to Sample” with Morteza Ramezani, Weilin Cong, Anand Sivasubramaniam, and Mahmut Kandemir Advances in Neural Information Processing Systems (NeurIPS), 2020. Local SGD with periodic averaging: tighter analysis and adaptive synchronization with Farzin Haddadpour, Mohammad Mahdi Kamani and Viveck R. Cadambe Advances in Neural Information Processing Systems (NeurIPS), 2019. Sketching meets random projection in the dual: A provable recovery algorithm for big and high-dimensional data with Jialei Wang, Jason D. Lee, Mladen Kolar, and Nathan Srebro Electronic Journal of Statistics, 2017 Train and test tightness of LP relaxations in structured prediction with Ofer Meshi, Adrian Weller, and David Sontag International Conference on Machine Learning (ICML), 2016. Lower and upper bounds on the generalization of stochastic exponentially concave optimization [Errata] with Lijun Zhang and Rong Jin Conference on Learning Theory (COLT), 2015. Random projections for classification: A recovery approach with Lijun Zhang, Rong Jin, Tianbao Yang, and Shenghuo Zhu IEEE Transactions on Information Theory, 2014. Regret bounded by gradual variation for online convex optimization with Tianbao Yang, Rong Jin, and Shenghuo Zhu Machine Learning Journal, 2014. Mixed optimization for smooth functions with Lijun Zhang and Rong Jin Advances in Neural Information Processing Systems (NeurIPS), 2013. Stochastic convex optimization with multiple objectives with Tianbao Yang and Rong Jin Advances in Neural Information Processing Systems (NeurIPS), 2013. Passive learning with target risk with Rong Jin Conference on Learning Theory (COLT), 2013. Trading regret for efficiency: Online convex optimization with long term constraints with Rong Jin and Tianbao Yang Journal of Machine Learning Research (JMLR), 2012. Teaching CMPSC 448: Machine Learning and Algorithmic AI The goal of this undergraduate course is to introduce data analysis from the machine learning perspective. Students will gain familiarity with the workings of common machine learning models and will learn how noise and bias in the data affect their results. The goal of the course is to equip students with required skills to understand and design new learning algorithms! CSE 588: Large-Scale Machine Learning: Mathematical Foundations and Applications This graduate-level course will aim to cover various mathematical aspects of big and high-dimensional learning arising in data science and machine learning applications. The focus will be on building a principled understanding of randomized methods via a mixture of empirical evaluations and mathematical modeling. Specifically, we will explore large-scale optimization algorithms for both convex and non-convex optimization, dimension reduction and random projection methods, large-scale numerical linear algebra, sparse recovery and compressed sensing, low-rank matrix recovery, convex geometry and linear inverse problems, empirical processes and generalization bounds, as well as theory and optimization landscape of neural networks, etc. Contact mzm616 (at) psu (dot) edu (814) 863-0076 mehrdad_mahdavi W365 Westgate Building, University Park, PA 16802, USA Tue Thur 11:00 to 12:00 or email for appointment × Cite Copy Download",
  "content_length": 6478,
  "method": "requests",
  "crawl_time": "2025-12-01 13:57:14"
}