{
  "name": "Saurabh Gupta 0001",
  "homepage": "http://saurabhg.web.illinois.edu",
  "status": "success",
  "content": "Saurabh Gupta Saurabh Gupta I am an Associate Professor in the ECE Department at UIUC. Before this, I was a Research Scientist at Facebook AI Research in Pittsburgh working with Prof. Abhinav Gupta. Earlier, I was a Computer Science graduate student at UC Berkeley, where I was advised by Prof. Jitendra Malik. Even earlier, I was an under graduate at IIT Delhi, in India, where I majored in Computer Science and Engineering. Prospective Students: I am looking for strong and motivated students to work with. If you are interested in working with me, please directly apply through the CS or ECE departments and mention my name. No GRE Needed! You do not need to directly contact me (and if you do, I am sorry that I may not be able to promptly respond back to you). If you are already at UIUC, please fill up this form and I will be in touch if and when I have an opening that will be a good fit for you. EMail / CSL 319 / CV / Scholar / Github / PhD Thesis Research I work on computer vision, robotics and machine learning. I am interested in building agents that can intelligently interact with the physical world around them. I am specifically interested in improving their generalization capabilities. Some of my recent work has focussed on 3D image and video understanding, understanding hand-object interaction, robot learning from videos, mobile manipulation, imitation learning, and control of humanoid robots. Research Group Current PhD Students Arjun Gupta (PhD, 2021 - ) Shaowei Liu (PhD, Joint with Prof. Shenlong Wang, 2021 - ) Aditya Prakash (PhD, Joint with Prof. David Forsyth, 2021 - ) Runpei Dong (PhD, 2024 -) Xialin He (PhD, 2024 -) Rahul Ramachandran (PhD, 2025 -) Former PhD Students Dr. Matthew Chang, Robot Learning from Videos, 2024. Next: Research Scientist at Meta AI Research Full Group Recent Talks Robot Learning by Understanding Egocentric Videos at RI Seminar at CMU. Robot Learning by Understanding Egocentric Videos at GRASP Seminar at U Penn. Understanding and Articulating Articulated Objects at Workshop on 3D Vision and Robotics at CVPR 2023. Representation for Visual Navigation and How to Train Them at 3DGV Seminar on 3D Geometry & Vision. Learning to Move and Moving to Learn at 3D Scene Understanding for Vision, Graphics, and Robotics at CVPR 2021 and Visual Learning and Reasoning for Robotics at RSS 2021. Learning to Speculate about People and Places for Visual Navigation at Perception and Control for Autonomous Navigation in Crowded, Dynamic Environments Workshop at RSS 2021. Teaching CS 444: Deep Learning for Computer Vision: Fall 2023, Fall 2024, Fall 2025. ECE 598 SG1: Robot Learning: Spring 2025. ECE 549 / CS 543: Computer Vision: Spring 2020, Spring 2021, Spring 2023, Spring 2024. ECE 598 SG: Special Topics in Learning-based Robotics: Fall 2019, Fall 2020, Fall 2021, Fall 2022. Publications 2025 Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images Aditya Prakash, David Forsyth, Saurabh Gupta arXiv, 2025 abstract / bibtex / website / code / video We tackle the problem of forecasting bimanual 3D hand motion & articulation from a single image in everyday settings. To address the lack of 3D hand annotations in diverse settings, we design an annotation pipeline consisting of a diffusion model to lift 2D hand keypoint sequences to 4D hand motion. For the forecasting model, we adopt a diffusion loss to account for the multimodality in hand motion distribution. Extensive experiments across 6 datasets show the benefits of training on diverse data with imputed labels (14% improvement) and effectiveness of our lifting (42% better) & forecasting (16.4% gain) models, over the best baselines, especially in zero-shot generalization to everyday images. @article{prakash2025bimanual, author = \"Prakash, Aditya and Forsyth, David and Gupta, Saurabh\", title = \"Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images\", journal = \"arXiv:2510.06145\", year = \"2025\" } Precise Mobile Manipulation of Small Everyday Objects Arjun Gupta, Rishik Sathua, Saurabh Gupta arXiv, 2025 abstract / bibtex / website / video Many everyday mobile manipulation tasks require precise interaction with small objects, such as grasping a knob to open a cabinet or pressing a light switch. In this paper, we develop Servoing with Vision Models (SVM), a closed-loop framework that enables a mobile manipulator to tackle such precise tasks involving the manipulation of small objects. SVM uses state-of-the-art vision foundation models to generate 3D targets for visual servoing to enable diverse tasks in novel environments. Naively doing so fails because of occlusion by the end-effector. SVM mitigates this using vision models that out-paint the end-effector, thereby significantly enhancing target localization. We demonstrate that aided by out-painting methods, open-vocabulary object detectors can serve as a drop-in module for SVM to seek semantic targets (e.g. knobs) and point tracking methods can help SVM reliably pursue interaction sites indicated by user clicks. We conduct a large-scale evaluation spanning experiments in 10 novel environments across 6 buildings including 72 different object instances. SVM obtains a 71% zero-shot success rate on manipulating unseen objects in novel environments in the real world, outperforming an open-loop control method by an absolute 42% and an imitation learning baseline trained on 1000+ demonstrations also by an absolute success rate of 50%. @article{gupta2025training, author = \"Gupta, Arjun and Sathua, Rishik and Gupta, Saurabh\", title = \"Precise Mobile Manipulation of Small Everyday Objects\", journal = \"arXiv preprint 2502.13964\", year = \"2025\" } Learning Getting-Up Policies for Real-World Humanoid Robots Xialin He*, Runpei Dong*, Zixuan Chen, Saurabh Gupta Robotics: Science and Systems (RSS), 2025 abstract / bibtex / website / video Automatic fall recovery is a crucial prerequisite before humanoid robots can be reliably deployed. Hand-designing controllers for getting up is difficult because of the varied configurations a humanoid can end up in after a fall and the challenging terrains humanoid robots are expected to operate on. This paper develops a learning framework to produce controllers that enable humanoid robots to get up from varying configurations on varying terrains. Unlike previous successful applications of humanoid locomotion learning, the getting-up task involves complex contact patterns, which necessitates accurately modeling the collision geometry and sparser rewards. We address these challenges through a two-phase approach that follows a curriculum. The first stage focuses on discovering a good getting-up trajectory under minimal constraints on smoothness or speed / torque limits. The second stage then refines the discovered motions into deployable (i.e. smooth and slow) motions that are robust to variations in initial configuration and terrains. We find these innovations enable a real-world G1 humanoid robot to get up from two main situations that we considered: a) lying face up and b) lying face down, both tested on flat, deformable, slippery surfaces and slopes (e.g., sloppy grass and snowfield). To the best of our knowledge, this is the first successful demonstration of learned getting-up policies for human-sized humanoid robots in the real world. @inproceedings{he2025learning, author = \"He, Xialin and Dong, Runpei and Chen, Zixuan and Gupta, Saurabh\", title = \"Learning Getting-Up Policies for Real-World Humanoid Robots\", booktitle = \"Robotics: Science and Systems\", year = \"2025\" } Demonstrating MOSART: Opening Articulated Structures in the Real World Arjun Gupta, Michelle Zhang*, Rishik Sathua*, Saurabh Gupta Robotics: Science and Systems (RSS), 2025 abstract / bibtex / website / video What does it take to build mobile manipulation systems that can competently operate on previously unseen objects in previously unseen environments? This work answers this question using opening of articulated objects as a mobile manipulation testbed. Specifically, our focus is on the end-to-end performance on this task without any privileged information, i.e. the robot starts at a location with the novel target articulated object in view, and has to approach the object and successfully open it. We first develop a system for this task, and then conduct 100+ end-to-end system tests across 13 real world test sites. Our large-scale study reveals a number of surprising findings: a) modular systems outperform end-to-end learned systems for this task, even when the end-to-end learned systems are trained on 1000+ demonstrations, b) perception, and not precise end-effector control, is the primary bottleneck to task success, and c) state-of-the-art articulation parameter estimation models developed in isolation struggle when faced with robot-centric viewpoints. Overall, our findings highlight the limitations of developing components of the pipeline in isolation and underscore the need for system-level research, providing a pragmatic roadmap for building generalizable mobile manipulation systems. @inproceedings{gupta2025opening, author = \"Gupta, Arjun and Zhang, Michelle and Sathua, Rishik and Gupta, Saurabh\", title = \"Demonstrating MOSART: Opening Articulated Structures in the Real World\", booktitle = \"Robotics: Science and Systems\", year = \"2025\" } How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions Aditya Prakash, Benjamin Lundell, Dmitry Andreychuk, David Forsyth, Saurabh Gupta*, Harpreet Sawhney* Computer Vision and Pattern Recognition (CVPR), 2025 abstract / bibtex / website We tackle the novel problem of predicting 3D hand motion and contact maps (or Interaction Trajectories) given a single RGB view, action text, and a 3D contact point on the object as input. Our approach consists of (1) Interaction Codebook: a VQVAE model to learn a latent codebook of hand poses and contact points, effectively tokenizing interaction trajectories, (2) Interaction Pr",
  "content_length": 117844,
  "method": "requests",
  "crawl_time": "2025-12-01 14:24:56"
}