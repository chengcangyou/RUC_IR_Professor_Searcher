{
  "name": "Qiang Liu 0001",
  "homepage": "https://www.cs.utexas.edu/~lqiang",
  "status": "success",
  "content": "Qiang Liu Qiang Liu Qiang Liu Associate Professor Computer Science University of Texas at Austin lqiang@cs.utexas.edu (for UT business) qiang.liu.research@gmail.com Office: GDC 4.806 Phone: 512-232-7866 (\"Qiang\" sounds like \"Chee-ah-ng\", and \"Liu\" as \"l-yo\") Research I aim to develop useful algorithms, by simplifying complexity and uncovering mathematical essence. I am broadly interested in the algorithmic core of machine learning, wherever mathematical insights can unlock new capabilities. <My Google scholar> Selected Works [New] We have been working on a suit of tutorials, code and notes on rectified flow. Flow and Diffusion models for Generative Modeling, Domain Transfer, Optimal Transport [ rectified flow (intro, for generative and transfer modeling, for optimal transport), diffusion bridges and Doob's h-transform (let us build bridges, first hitting diffusion, diffusion w/ informative bridges) ] Lion-K is a principled generalization of the machine-discovered Lion optimization program, with convergence verified via a Lyapunov function [Lion-K, slides]. [New] Lion-K also captures the more recent Muon optimizer, which solves optimization problems under spectral norm constraints. See blog post. Dynamic Barrier Descent for Optimization/Sampling in the Wild: Constrained, Lexicographic, Multi-objective, Multi-level [ slides, constrained, lexicographic optimization, optimization within Pareto set, finding diverse local modes, bi-level optimization, multi-task learning, sampling on Pareto set, sampling with constrained moments, orthonal-space variational gradient ] Steepest Descent Methods for Neural Architecture Optimization: Going Beyond Black Boxes [ splitting steepest descent (intro, paper, slides), firefly neural architecture descent (paper, slides ) ] Off-Policy Evaluation [ slides, non-asymptotic confidence intervals, breaking the curse of horizon ] Certifiable Robustness and Other Properties [ a functional optimization framework, certified robustness in language models, certified monotonicity ] Stein Variational Inference: Approximate Learning and Inference With Stein's Method [ intro, kernel Stein discrepancy (KSD) (intro, paper), Stein variational gradient descent (SVGD) (intro, basic algorithm, SVGD as gradient flow, SVGD as moment matching, gradient free SVGD, matrix kernel), slides, slides, slides, slides, code ] Distributed Learning, Information Loss and Curved Exponential Families [paper, slides] Reasoning and Decisions in Probabilistic Graphical Models [ thesis, slides, marginal MAP ] Variational Inference for Crowdsourcing [paper, slides] Working with me If you are a UT student interested in working with me, please consider enrolling in one of my classes and interacting with me there. If you are applying for graduate studies at UT and wish to work with me, please indicate your interest in your application and research statement. Please do NOT email me directly. Teaching Undergraduate intro to machine learning [here]. Undergraduate intro to optimization [here] Lecture notes on probabilistic learning and inference [here] Learning theory (graduate level, scribed notes, not proofreaded!) [here] Advanced ML for undergraduates (scribed notes, not proofreaded!) [here]",
  "content_length": 3232,
  "method": "requests",
  "crawl_time": "2025-12-01 14:13:36"
}