{
  "name": "Sebastian U. Stich",
  "homepage": "https://www.sstich.ch",
  "status": "success",
  "content": "Sebastian U. Stich - Faculty at the CISPA Helmholtz Center for Information Security Sebastian U. Stich mail: stich@cispa.de address: CISPA Helmholtz Center for Information Security, Stuhlsatzenhaus 5, D-66123 Saarbrücken, Germany. Faculty at CISPA, Member of the European Lab for Learning and Intelligent Systems (ELLIS). News & Open Positions [fully-funded postdoc position] Applications are invited for a postdoc position in my group. Find more details (and how to apply) here. Contact form: [Postdoc application]. I am reviewing applications on a rolling basis. [fully-funded PhD position, ERC CollectiveMinds] Applications are invited for a fully funded 4-year PhD position. Contact form: [PhD application]. I am reviewing applications on a rolling basis. I am also accepting ELLIS PHD candidates (application deadline: 15 November 2025). If you apply to ELLIS and want to work with me, drop me an email. Please note that I may not be able to respond to every email immediately, but I will do my best to reply as soon as possible. Thank you for your patience! I am always looking for strong postdocs, PhD students, interns, HiWis, master theses, or other research collaborations possible (see here). If you are interested in applying for a position, please read this page and send me an email detailing your research interests and motivation, and attach your CV (and possibly your transcript). [Teaching Summer 25]: I am teaching an advanced course on Modern Optimization Methods together with Anton Rodomanov. [Teaching Summer 24]: I am teaching an advanced lecture on Optimization for Machine Learning at Saarland University. [Teaching Winter 23]: I am teaching an advanced lecture on Games in Machine Learning together with Tatjana Chavdarova [Tatjana's Course Slides]. [June 2025] I am happy to announce that I was appointed as tenured faculty at CISPA. [December 2024] I am honored to be awarded an ERC Consolidator Grant 2024 for our project CollectiveMinds. Join me on this exciting journey! If you are interested in a PostDoc or PhD position specifically related to this project, please include [CollectiveMinds] in the subject line when reaching out via email. [December 2024] Co-organizing the optimization workshop at NeurIPS 2024 [August 2024] First time at KDD, for a keynote at the FedKDD workshop. [June 2024] Plenary speaker at the EUROPT Conference on Advances in Continuous Optimization (EUROPT 2024) on A Universal Framework for Federated Optimization [Slides]. [September 2023] I am honored to receive a Google Reserach Scholar Award. [August 2022] I am honored to receive a Meta Privacy-Enhancing Technologies Research Award. [June 24, 21] Invited keynote talk on Efficient Federated Learning Algorithms [Slides] at the FL-ICML 2021 Workshop. Team (PhD students & Postdocs) Dr. Anton Rodomanov (since 09/2023) Dr. Rotem Mulayoff (since 09/2024) Xiaowen Jiang (since 01/2023) Yuan Gao (since 06/2023) Polina Dolgova (since 07/2025) Alumni Anastasia Koloskova (defended 11/2024) Research Interests optimization for machine learning methods for collaborative learning (distributed, federated and decentralized methods) efficient optimization methods adaptive stochastic methods and generalization performance theory of deep learning privacy and security in machine learning Workshops (Organizer) 17th OPT Workshop on Optimization for Machine Learning at NeurIPS, co-organizer December 6, 2025, San Diego, USA. Reviewing for Conferences: International Conference on Learning Representations (ICLR)24 (area chair)2322International Conference on Machine Learning (ICML)24 (area chair)23 (area chair)2220191817Conference on Computer Vision and Pattern Recognition (CVPR)2224Genetic and Evolutionary Computation Conference (GECCO)2423222120191817161514International Conference on Artificial Intelligence and Statistics (AISTATS)2423222019Neural Information Processing Systems (NeurIPS)23 (area chair)22 (area chair)22 (area chair)21 (area chair)201918Parallel Problem Solving from Nature (PPSN)222018International Symposium on Computational Geometry (SoCG)21Symposium on Discrete Algorithms (SODA)19Symposium on the Theory of Computing (STOC)18 Journals: Annals of Statistics (AOS)Artificial Intelligence (ARTINT)Computational Optimization and Application (COAP)European Journal of Operational Research (EJOR)IEEE Journal on Selected Areas in Information Theory (JSAIT)IEEE Transactions on Evolutionary Computation (TEVC)IEEE Transactions on Pattern Analysis and Machine Learning (TPAMI)IEEE Transactions on Signal Processing (TSP)Journal of Machine Learning Research (JMLR)Machine Learning (MACH)Markov Processes And Related Fields (MPRF)Mathematical Programming (MAPR)Numerical Algebra, Control and Optimization (NACO)Optimization Methods and Software (OMS)SIAM Journal on Mathematics of Data Science (SIMODS)SIAM Journal on Optimization (SIOPT) Workshops: NeurIPS Workshop on Optimization for Machine Learning (OPT)2322212019International Workshop on Trustable, Verifiable and Auditable Federated Learning in Conjunction with AAAI 2022 (FL-AAAI-22)22International Workshop on Federated Learning for User Privacy and Data Confidentiality (FL-ICML)2120 Editorial Board Journal of Optimization Theory and Applications Transactions on Machine Learning Research (Action Editor) Education and Work since Dec 1 2021, TT Faculty at CISPA. since June 2020, member of the European Lab for Learning and Intelligent Systems. From Dec 1 2016 to Nov 30 2021, research scientist at EPFL, hosted by Prof. Martin Jaggi, Machine Learning and Optimization Laboratory (MLO). From Nov 1 2014 to Oct 31 2016, I worked with Prof. Yurii Nesterov and Prof. François Glineur at the Center for Operations Research and Econometrics (CORE) and the ICTEAM. From Sep 15 2010 to Sep 30 2014, I was a PHD student in Prof. Emo Welzl's research group, supervised by Prof. Bernd Gärtner and Christian Lorenz Müller. From Sep 2005 to Mar 2010 I did my Bachelor and Master in Mathematics at ETH Zurich. Publications I became too busy to update this list regularly. Please check my Google Scholar Profile to see what I have been working on recently. Refereed Publications Simultaneous Training of Partially Masked Neural NetworksAmirkeivan MohtashamiMartin JaggiSebastian U. StichAbstractIn:AISTATS2022For deploying deep learning models to lower end devices, it is necessary to train less resource-demanding variants of state-of-the-art architectures. This does not eliminate the need for more expensive models as they have a higher performance. In order to avoid training two separate models, we show that it is possible to train neural networks in such a way that a predefined 'core' subnetwork can be split-off from the trained full network with remarkable good performance. We extend on prior methods that focused only on core networks of smaller width, while we focus on supporting arbitrary core network architectures. Our proposed training scheme switches consecutively between optimizing only the core part of the network and the full one. The accuracy of the full model remains comparable, while the core network achieves better performance than when it is trained in isolation. In particular, we show that training a Transformer with a low-rank core gives a low-rank model with superior performance than when training the low-rank model alone. We analyze our training scheme theoretically, and show its convergence under assumptions that are either standard or practically justified. Moreover, we show that the developed theoretical framework allows analyzing many other partial training schemes for neural networks.The Peril of Popular Deep Learning Uncertainty Estimation MethodsYehao LiuMatteo PagliardiniTatjana ChavdarovaSebastian U. StichAbstractCodeIn:NeurIPS 2021 Workshop: Bayesian Deep Learning2021Uncertainty estimation (UE) techniques -- such as the Gaussian process (GP), Bayesian neural networks (BNN), Monte Carlo dropout (MCDropout) -- aim to improve the interpretability of machine learning models by assigning an estimated uncertainty value to each of their prediction outputs. However, since too high uncertainty estimates can have fatal consequences in practice, this paper analyzes the above techniques. Firstly, we show that GP methods always yield high uncertainty estimates on out of distribution (OOD) data. Secondly, we show on a 2D toy example that both BNNs and MCDropout do not give high uncertainty estimates on OOD samples. Finally, we show empirically that this pitfall of BNNs and MCDropout holds on real world datasets as well. Our insights (i) raise awareness for the more cautious use of currently popular UE methods in Deep Learning, (ii) encourage the development of UE methods that approximate GP-based methods -- instead of BNNs and MCDropout, and (iii) our empirical setups can be used for verifying the OOD performances of any other UE method.An Improved Analysis of Gradient Tracking for Decentralized Machine LearningAnastasia KoloskovaTao LinSebastian U. StichAbstractPosterIn:NeurIPS342021We consider decentralized machine learning over a network where the training data is distributed across agents, each of which can compute stochastic model updates on their local data. The agent's common goal is to find a model that minimizes the average of all local loss functions. While gradient tracking (GT) algorithms can overcome a key challenge, namely accounting for differences between workers' local data distributions, the known convergence rates for GT algorithms are not optimal with respect to their dependence on the mixing parameter (related to the spectral gap of the connectivity matrix). We provide a tighter analysis of the GT method in the stochastic strongly convex, convex and non-convex settings. We improve the dependency on p from p to pc in the noiseless case and from p to pc in the general stochastic case, where c>p is related to the negative eigenvalues of the connectivity matrix (and is a constant in most practical applications). This impro",
  "content_length": 79902,
  "method": "requests",
  "crawl_time": "2025-12-01 14:25:20"
}