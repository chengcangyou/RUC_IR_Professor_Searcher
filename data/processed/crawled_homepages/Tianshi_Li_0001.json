{
  "name": "Tianshi Li 0001",
  "homepage": "https://tianshili.me",
  "status": "success",
  "content": "Tianshi Li Iâm an Assistant Professor at Northeastern University in the Khoury College of Computer Sciences, directing the PEACH (Privacy-Enabling AI and Computer-Human interaction) Lab. Iâm also a core faculty member at the Cybersecurity and Privacy Institute at Northeastern University. My work is driven by the belief that privacy sustains human agency, safe exploration, and authentic expression in a connected world. My interest focuses on studying and addressing the emerging LLM privacy issues from a human-centered perspective. Read my recent papers (1, 2, 3, 4, 5, 6) to learn more. Check out my recent talks (1, 2) for a quicker overview of our vision and research agenda. My broad research interests lie at the intersection of Human-Computer Interaction (HCI), Privacy, and AI. I strive to address the increasing privacy issues in todayâs digital world using a blend of human-centered problem understanding and technical problem solving. I conduct mixed-methods research to understand the privacy challenges situated in different stakeholdersâ lived experiences, and build systems and conduct computational experiments to measure, model, and tackle these human-centered problems. News Apr 30, 2025 Dropping a new position paper âPrivacy is Not Just Memorizationâ w/ Niloofar Mireshghallah to spotlight the wildly understudied problem of inference-time privacy in LLMs (e.g., agent-based context leakage and abuse agentic capabilities for democratized surveillance). Apr 30, 2025 HAIPS 2025 CfP is out! Very excited to co-chair the 1st Workshop on Human-Centered AI Privacy and Security at CCS 2025 in Taiwan w/ Toby Li, Yaxing Yao, and Sauvik Das! Join us by submitting your new or published work to explore the current âhypesâ at the intersection of HCI, AI, and S&P. Jan 17, 2025 Two papers accepted at CHI 2025! See you in Yokohama! Oct 18, 2024 Our HCOMP 2024 paper âInvestigating What Factors Influence Usersâ Rating of Harmful Algorithmic Bias and Discriminationâ won the best paper award! Sep 26, 2024 Our PrivacyLens paper has been accepted to the NeurIPS 2024 Track on Datasets and Benchmarks. We introduce a novel framework to benchmark emerging unintended privacy leakage issues in LM agents, which also presents a method for operationalizing the contextual integrity framework with the help of LLMs. Check out our preprint and website to learn more. Sep 20, 2024 Two papers accepted at CSCW 2025! One on secret use of LLMs, and another on ethics of LLM use in HCI research. Sep 19, 2024 Iâm grateful to have received a gift grant of $50K from Google for designing human-centered privacy protection in text input methods! I also visited the Gboard team today and gave a talk titled âNavigating Privacy in the Age of LLMs: A Human-Centered Perspective.â Looking forward to more collaborations! Jul 17, 2024 Excited to share our NSF SaTC award on âEmpathy-Based Privacy Education and Design through Synthetic Persona Data Generationâ ($600K in total, $200K personal share). This grant is in collaboration with Prof. Toby Li (Notre Dame) and Prof. Yaxing Yao (Virginia Tech). Feb 1, 2024 Our Special Integret Group proposal titled âHuman-Centered Privacy Research in the Age of Large Language Modelsâ is accepted at CHIâ24! Excited to meet people from diverse backgrounds at CHI! Jan 19, 2024 Two papers accepted at CHIâ24! One on how LLMs may invade usersâ privacy, and another on how LLMs may empower people to build better multimodal apps! Selected Publications HAIPS Privi: Assisting Users in Authoring Contextual Privacy Rules with an LLM Sandbox Bingcan Guo,Â Zhiping Zhang,Â andÂ Tianshi Li In HAIPS â25: Proceedings of the 2025 Workshop on Human-Centered AI Privacy and Security (co-located with CCSâ25) 2025 Abs PDF Aligning language models (LM) with individual usersâ latent preferences and internal values, such as privacy considerations, is crucial for enhancing output quality and preventing unwanted privacy leakage. Yet, existing methods struggle to capture individualsâ contextualized privacy preferences and formalize them in an extensible and generalizable way to guide model outputs. As an initial exploration, we present Privi, an interactive elicitation mechanism that generates synthetic communication scenarios, leverages usersâ edits of candidate responses to infer privacy preferences, and formalizes them in an extensible privacy rule set. We conducted a within-subjects pilot study (N = 15) to evaluate Privi and the quality of the elicited privacy rules. Results show that responses generated under three conditions: pre-specified rules, elicited rules, no rules (model judgment), were comparable across three key evaluation dimensions: amount of privacy disclosure, perceived utility, and willingness to use. We further analyzed the synthetic scenarios and usersâ editing behavior and identified future directions for improving Privi. HAIPS Speculating Unintended Creepiness: Exploring LLM-Powered Empathy Building for Privacy-Aware UX Design Zeya Chen,Â Jianing Wen,Â Toby Jia-Jun Li,Â Yaxing Yao,Â andÂ Tianshi Li In HAIPS â25: Proceedings of the 2025 Workshop on Human-Centered AI Privacy and Security (co-located with CCSâ25) 2025 Abs PDF Despite increasing awareness of dark patterns and anti-patterns in UX design, privacy invasive design choices remain prevalent in real world systems. These choices often stem not from malicious intent, but from a lack of structured guidance and contextual understanding among designers. Designers face challenges not only in detecting deceptive interactions, but also in anticipating how certain features may cause harm. Contributing factors such as limited ability to recognize harm, lack of relatable design references, and challenges in connecting abstract privacy principles to concrete design scenarios, particularly when designing for non-dominant user groups. To address this, while currently implemented as a pipeline, PrivacyMotiv is envisioned as a future system that integrates user personas, journey maps, and design audits into a unified tool to help designers identify privacy harms and dark patterns. Grounded in motivation theory and contextual design thinking, our approach supports reasoning across multiple user-feature interactions situated in real world scenarios, with the goal of revealing hidden risks and inspiring designer empathy to promote privacy-aware design for everyone. CHI Rescriber: Smaller-LLM-Powered User-Led Data Minimization for LLM-Based Chatbots Jijie Zhou,Â Eryue Xu,Â Yaoyao Wu,Â andÂ Tianshi Li In CHI 2025 Apr 2025 Abs arXiv The proliferation of LLM-based conversational agents has resulted in excessive disclosure of identifiable or sensitive information. However, existing technologies fail to offer perceptible control or account for usersâ personal preferences about privacy-utility tradeoffs due to the lack of user involvement. To bridge this gap, we designed, built, and evaluated Rescriber, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts. Our studies (N=12) showed that Rescriber helped users reduce unnecessary disclosure and addressed their privacy concerns. Usersâ subjective perceptions of the system powered by Llama3-8B were on par with that by GPT-4o. The comprehensiveness and consistency of the detection and sanitization emerge as essential factors that affect usersâ trust and perceived protection. Our findings confirm the viability of smaller-LLM-powered, user-facing, on-device privacy controls, presenting a promising approach to address the privacy and trust challenges of AI. CSCW Secret Use of Large Language Models Zhiping Zhang,Â Chenxinran Shen,Â Bingsheng Yao,Â Dakuo Wang,Â andÂ Tianshi Li In CSCW 2025 Oct 2025 Abs arXiv The advancements of Large Language Models (LLMs) have decentralized the responsibility for the transparency of AI usage. Specifically, LLM users are now encouraged or required to disclose the use of LLM-generated content for varied types of real-world tasks. However, an emerging phenomenon, usersâ secret use of LLMs, raises challenges in ensuring end users adhere to the transparency requirement. Our study used mixed-methods with an exploratory survey (125 real-world secret use cases reported) and a controlled experiment among 300 users to investigate the contexts and causes behind the secret use of LLMs. We found that such secretive behavior is often triggered by certain tasks, transcending demographic and personality differences among users. Task types were found to affect usersâ intentions to use secretive behavior, primarily through influencing of perceived external judgment regarding LLM usage. Our results yield important insights for future work on designing interventions to encourage more transparent disclosure of LLM/AI use NeurIPS PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action Yijia Shao,Â Tianshi Li,Â Weiyan Shi,Â Yanchen Liu,Â andÂ Diyi Yang In NeurIPS Datasets and Benchmarks Sep 2024 arXiv Code Website CHI SIG Human-Centered Privacy Research in the Age of Large Language Models Tianshi Li,Â Sauvik Das,Â Hao-Ping (Hank) Lee,Â Dakuo Wang,Â Bingsheng Yao,Â andÂ Zhiping Zhang In CHI Conference on Human Factors in Computing Systems (CHIâ24 Companion) Apr 2024 Abs arXiv The emergence of large-language models (LLMs), and their increased use in user-facing systems, has led to substantial privacy concerns. To date, research on these privacy concerns has been model-centered: exploring how LLMs lead to privacy risks like memorization, or can be used to infer personal characteristics about people from their content. We argue that there is a need for more research focusing on the human aspect of these privacy issues: e.g., research on how design paradigms for LLMs affect usersâ disclosure behaviors, usersâ mental models and preferences for privacy co",
  "content_length": 21107,
  "method": "requests",
  "crawl_time": "2025-12-01 14:38:24"
}