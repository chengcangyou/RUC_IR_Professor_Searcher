{
  "name": "Edoardo Maria Ponti",
  "homepage": "https://ducdauge.github.io",
  "status": "success",
  "content": "Edoardo M. Ponti Search Edoardo M. Ponti eponti AT ed.ac.uk University of Edinburgh NVIDIA I am an assistant professor in Natural Language Processing at the University of Edinburgh and a visiting professor at NVIDIA. If you are a prospective student please visit the FAQ page for more information. My research focuses on: efficient architectures (see my NeurIPS 2024 tutorial on dynamic sparsity): I aim to redefine the units of computation of foundation models by adaptively compressing their intermediate representations and memory. This breaks models free from tokenizers and learn hierarchical abstractions over raw data. modular deep learning: I am interested in designing neural architectures that route information to specialised modules (e.g., sparse subnetworks). This facilitates systematic generalisation and conditional computation. computational typology: I wish to understand how languages vary, across the world and its cultures, within a computational and mathematical framework. Multimodal models in particular give us an powerful tool to study how form depends on grounded, embodied representations of meaning and function. Previously, I was a visiting postdoctoral scholar at Stanford University and a postdoctoral fellow in computer science at Mila - Quebec AI Institute in Montreal. In 2021, I obtained a PhD from the University of Cambridge, St John’s College. Once upon a time I studied typological and historical linguistics at the University of Pavia (deep in my heart, I am still a humanist). My research has been featured on the Economist and Scientific American, among others. I received a Google Research Faculty Award and 2 Best Paper Awards at EMNLP 2021 and RepL4NLP 2019. I am a board member of SIGTYP, the ACL special interest group for computational typology, a Scholar of the European Lab for Learning and Intelligent Systems (ELLIS), and part of the TACL journal editorial team. My Lab PhD Students Benjamin Minixhofer with Ivan Vulić Model Merging, Tokenizer Transfer Chentian Jiang with Chris Lucas Causal Discovery Coleman Haley with Sharon Goldwater Computational Typology Giwon Hong with Pasquale Minervini Retrieval-augmented Language Models Nina Gregorio with Sharon Goldwater Computational Typology Osman Batur İnce Multimodal NLP Piotr Nawrot with Ivan Titov Efficient Transformers Yifu Qiu with Shay Cohen and Anna Korhonen Hallucinations, Temporal Grounding Zeyu Huang with Ivan Titov Self-Improving LLMs Postdocs Andreas Grivas Tokenization Emile van Krieken with Antonio Vergari and Pasquale Minervini Neuro-symbolic AI Selected Publications Zero-Shot Tokenizer Transfer Language models (LMs) are bound to their tokenizer, which maps raw text to a sequence of vocabulary items (tokens). This restricts … Benjamin Minixhofer, Edoardo M. Ponti, Ivan Vulić PDF Code Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to … Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David Tarjan, Edoardo M. Ponti PDF Scaling Sparse Fine-Tuning to Large Language Models Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of … Alan Ansell, Ivan Vulić, Hannah Sterz, Anna Korhonen, Edoardo M. Ponti PDF Code Combining Modular Skills in Multitask Learning A modular design encourages neural models to disentangle and recombine different facets of knowledge to generalise more systematically … Edoardo M. Ponti, Alessandro Sordoni, Yoshua Bengio, Siva Reddy PDF Code Visually Grounded Reasoning across Languages and Cultures The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts … Fangyu Liu, Emanuele Bugliarello, Edoardo M. Ponti, Siva Reddy, Nigel Collier, Desmond Elliott PDF Code Dataset Project Cite × Copy Download",
  "content_length": 3968,
  "method": "requests",
  "crawl_time": "2025-12-01 13:04:38"
}