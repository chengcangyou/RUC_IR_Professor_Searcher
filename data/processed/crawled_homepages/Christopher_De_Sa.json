{
  "name": "Christopher De Sa",
  "homepage": "http://www.cs.cornell.edu/~cdesa",
  "status": "success",
  "content": "Chris De Sa Chris De Sa Gates Hall, Room 426 I am an Associate Professor in the Computer Science department at Cornell University. I am a member of the Cornell Machine Learning Group and I lead the Relax ML Lab. My research interests include algorithmic, software, and hardware techniques for high-performance machine learning, with a focus on relaxed-consistency variants of stochastic algorithms such as asynchronous and low-precision stochastic gradient descent (SGD) and Markov chain Monte Carlo. My work builds towards using these techniques to construct data analytics and machine learning frameworks, including for deep learning, that are efficient, parallel, and distributed. I graduated from Stanford University in 2017, where I was advised by Kunle Olukotun and by Chris Râé. Recent News and Awards[show all news][show only recent news] ▷ Our paper \"Is My Prediction Arbitrary? The Confounding Effects of Variance in Fair Classification Benchmarks\" won a Best Student Paper Award Honorable Mention for the AI for Social Impact track at AAAI-2024. ▷ I was awarded a DARPA YFA Grant for \"Decentralized Online Parameter-Efficient Fine-Tuning of Compressed Models,\" 2024. ▷ I gave a keynote speech at the International Conference on AI-ML Systems. ▷ I am program chair at MLSys 2024. ▷ I was awarded the Google Research Scholar Award. ▷ I was a workshop/tutorial chair at MLSys 2022. ▷ Our paper \"Optimal Complexity in Decentralized Training\" (Yucheng Lu, Chris De Sa) won an Outstanding Paper Award Honorable Mention at ICML 2021 (awarded to 4 papers out of 1184 publications). ▷ I co-organized the Cornell Institute for Digital Agriculture Hackathon. ▷ I have joined the executive committee of the Cornell Institute for Digital Agriculture (CIDA). ▷ I won the National Science Foundation CAREER award. ▷ Our paper on bias in model selection was featured in the popular press. ▷ I was awarded the Mr. & Mrs. Richard F. Tucker Excellence in Teaching Award from the College of Engineering. ▷ I was awarded an NSF Robust Intelligence Small Grant for \"Reliable Machine Learning in Hyperbolic Spaces.\" ▷ Three papers from our lab were accepted into NeurIPS 2020, of which two won spotlight awards! ▷ Together with faculty from other departments, I have been teaching PLSCI 7202, a short course on applications of machine learning to plant science. PhD Students ▷ Ruqi Zhang. PhD 2021, Statistics (now an Assistant Professor at Purdue CS). Scalable Bayesian inference for ML. ▷ Yucheng Lu. PhD 2023, Computer Science (now at Together). Distributed optimization, ML systems. ▷ A. Feder Cooper. PhD 2024, Computer Science (postdoc at MSR, affiliate at Stanford HAI; future Assistant Professor at Yale CS). Reliable measurement and evaluation of ML systems. ▷ Tao Yu. PhD 2024, Computer Science. Private and secure ML, accurate learning in hyperbolic space. ▷ Jerry Chee. PhD Student, Computer Science. Scalable and resource-efficient ML. ▷ Yaohui Cai. PhD Student, Electrical and Computing Engineering. Efficient deep learning inference and training (co-advised with Zhiru Zhang). ▷ Si Yi (Cathy) Meng. PhD Student, Computer Science. Optimization algorithms for large-scale machine learning. ▷ Albert Tseng. PhD Student, Computer Science. Deep learning quantization and compression. Teaching ▷ CS 4780/5780 Machine Learning (Spring 2022, Spring 2018) ▷ CS 4787/5777 Principles of Large-Scale Machine Learning (Fall 2023, Fall 2022, Spring 2021, Spring 2020, Spring 2019) ▷ CS 6787 Advanced Machine Learning Systems (Spring 2024, Fall 2021, Fall 2020, Fall 2019, Fall 2018, Fall 2017) ▷ CS 7792 Special Topics in Machine Learning (Spring 2023)Office Hours Wednesdays 2:00-3:00 PM in Gates 426. Publications Lab Website — CV — Google Scholar — Show All Abstracts — Hide All Abstracts NeurIPS 2024 QTIP: Quantization with trellises and incoherence processing Spotlight Albert Tseng, Qingyao Sun, David Hou, Christopher De Sa In Proceedings of the 37th Neural Information Processing Systems Conference, December 2024. [Abstract] [Paper] [Blog] Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing weights to low-precision datatypes. Since LLM inference is usually memory-bound, PTQ methods can improve inference throughput. Recent state-of-the-art PTQ approaches use vector quantization (VQ) to quantize multiple weights at once, which improves information utilization through better shaping. However, VQ requires a codebook with size exponential in the dimension. This limits current VQ-based PTQ works to low VQ dimensions ($\\le 8$) that in turn limit quantization quality. Here, we introduce QTIP, which instead uses trellis coded quantization (TCQ) to achieve ultra-high-dimensional quantization. TCQ uses a stateful decoder that separates the codebook size from the bitrate and effective dimension. QTIP introduces a spectrum of lookup-only to computed lookup-free trellis codes designed for a hardware-efficient \"bitshift\" trellis structure; these codes achieve state-of-the-art results in both quantization quality and inference speed. Diffusion models with learned adaptive noise Subham Sahoo, Aaron Gokaslan, Christopher M De Sa, Volodymyr Kuleshov In Proceedings of the 37th Neural Information Processing Systems Conference, December 2024. [Abstract] [Paper] Diffusion models have gained traction as powerful algorithms for synthesizing high-quality images. Central to these algorithms is the diffusion process, a set of equations which maps data to noise in a way that can significantly affect performance. In this paper, we explore whether the diffusionprocess can be learned from data. Our work is grounded in Bayesian inference and seeks to improve log-likelihood estimation by casting the learned diffusion process as an approximate variational posterior that yields a tighter lower bound (ELBO) on the likelihood. A widely held assumption is that the ELBO is invariant to the noise process: our work dispels this assumption and proposes multivariate learned adaptive noise (MuLAN), a learned diffusion process that applies noise at different rates across an image. Our method consists of three components: a multivariate noise schedule, adaptive input-conditional diffusion, and auxiliary variables; these components ensure that the ELBO is no longer invariant to the choice of the noise schedule as in previous works. Empirically, MuLAN sets a new state-of-the-art in density estimation on CIFAR-10 and ImageNet while matching the performance of previous state-of-the-art models with 50% fewer steps. We provide the code, along with a blog post and video tutorial on the project page: https://s-sahoo.com/MuLAN. Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices Andres Potapczynski, Shikai Qiu, Marc Finzi, Christopher Ferri, Charlie Chen, Micah Goldblum, C Bayan Bruss, Christopher M De Sa, Andrew G Wilson In Proceedings of the 37th Neural Information Processing Systems Conference, December 2024. [Abstract] [Paper] Dense linear layers are the dominant computational bottleneck in large neural networks, presenting a critical need for more efficient alternatives. Previous efforts to develop alternatives have focused on a small number of hand-crafted structured matrices, and have neglected to investigate whether these structures can surpass dense layers in terms of compute-optimal scaling laws when both the model size and training examples are optimally allocated. In this work, we present a unifying framework that enables searching among all linear operators expressible via an Einstein summation. This framework encompasses many previously proposed structures, such as low-rank, Kronecker, Tensor-Train, and Monarch, along with many novel structures. We develop a taxonomy of all such operators based on their computational and algebraic properties, which provides insights into their scaling laws. Combining these insights with empirical evaluation, we identify a subset of structures that achieve equal or better performance than dense layers as a function of training compute. To further improve their compute efficiency, we develop a natural extension of these performant structures that convert them into a sparse Mixture-of-Experts layer. The resulting layer significantly outperforms dense layers in compute-optimal training efficiency for GPT-2 language models. ICML 2024 QuIP\\(\\sharp\\): Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, Christopher De Sa In ICML: the Fortieth International Conference on Machine Learning, July 2024. [Abstract] [Paper] Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision. In this work, we introduce QuIP\\(\\sharp\\), a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes (4 bits per weight) using three novel techniques. First, QuIP\\(\\sharp\\) improves QuIP's (Chee et al., 2023) incoherence processing by using the randomized Hadamard transform, which is faster and has better theoretical properties. Second, QuIP\\(\\sharp\\) uses vector quantization to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP\\(\\sharp\\) uses fine-tuning to improve fidelity to the original model. Our experiments show that QuIP\\(\\sharp\\) outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference. Our code can be found at https://github.com/Cornell-RelaxML/quip-sharp. ICLR 2024 Shadow Cones: A Generalized Framework for Partial Order Embeddings Tao Yu, Toni J.B. Liu, Albert Tseng, Christopher De Sa In ICLR: The Twelfth International Conference on Learning Representations, May 2024. [Abstract] [Paper] Hyperbolic space has proven to be well-",
  "content_length": 119758,
  "method": "requests",
  "crawl_time": "2025-12-01 12:53:11"
}