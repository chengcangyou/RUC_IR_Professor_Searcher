{
  "name": "Jian-Xin Wu",
  "homepage": "https://cs.nju.edu.cn/wujx",
  "status": "success",
  "content": "Jianxin Wu's homepage Jianxin Wu (吴建鑫) Professor (中文简介) School of Artificial Intelligence Nanjing University Office: Room 1018, Computer Science Building I am in the LAMDA group. Email: For research related matters (paper, code, review, etc.): wujx2001 {AT} gmail.com Email: For teaching related matters (teaching, admission, hiring, etc.): wujx2001 {AT} nju.edu.cn My email box will treat 126, 163, 189, QQ, Sina, Yeah email addresses as spam. Emails from these addresses will not be read by me. 请申请进组的本校本科生或申请读研（直博）的同学首先阅读这个文档。 怎样从别人那里要求获得代码/论文/模型/数据? Essentials of Pattern Recognition: An Accessible Approach is now available! 《模式识别》教材中文版现已出版！ Education Ph. D. in College of Computing, Georgia Institute of Technology, 2009; Advisor Prof. Jim Rehg. B.S. & M.S., 1999 & 2002, in Nanjing University, China Career 2021.11 -- present Professor, School of Artificial Intelligence, Nanjing University, China 2013.7 -- 2021.10 Professor, Department of Computer Science and Technology (now renamed to the School of Computer Science), Nanjing University, China 2009.8 -- 2013.7 Assistant professor, School of Computer Engineering, Nanyang Technological University, Singapore Services Associate Editor IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI), 2020.09-- Pattern Recognition, 2017.1-- Program chair CVPR 2024 Tutorial chair CVPR 2023 Area chair/senior area chair ICCV 2015, CVPR 2017, AAAI 2019, CVPR 2020, ECCV 2020, CVPR 2021, IJCAI 2021 (SAC), CVPR 2023, ECCV 2024, NeurIPS 2024, CVPR 2025, ICCV 2025, NeurIPS 2025, AAAI 2026, CVPR 2026 SPC/AC AAAI 2016, AAAI 2017, (both years do not have the area chair rank), AAAI 2018, AAAI 2020, IJCAI 2013, IJCAI 2018, IJCAI 2019 Area chair ACCV 2012, PSIVT 2010, 2011, 2013, ICPR 2020 Publication chair ACCV 2014, PCM 2012 Finance chair ACML 2012 Publications Most of my papers are available for download in this Publications page, and here is my Google Scholar Citations profile. A list of the LAMDA group publications also include my papers. Teaching 2025年课程已经结束，下一次课程预计将于2026年秋季学期开设。 Students Institute Starting Graduated Program First job/Currently Guoqing Liu (刘国清) NTU, Singapore 2010.8 2013 Ph.D. Founder of Minieye (深圳佑驾) Yu Zhang (张宇) NTU, Singapore 2010.8 2014.8 Ph.D. (after 2013.7 with Jianfei Cai; degree 2015.7) Southeast university (东南大学) Hao Yang (杨昊) NTU, Singapore 2011.8 2015.8 Ph.D. (after 2013.7 with Jianfei Cai; degree 2016.7) Amazon Yang Xiao (肖阳) NTU, Singapore 2012.3 2013.2 Postdoc Huazhong University of Science and Technology (华中科技大学) Xiu-Shen Wei (魏秀参) NJU 2014.9 2018.6 Ph.D. Megvii/Face++ (旷视) / Currently at SEU （东南大学） Bin-Bin Gao (高斌斌) NJU 2014.9 2018.6 Ph.D. Tencent (腾讯优图) Jian-Hao Luo (罗建豪) NJU 2015.9 2020.6 Ph.D. Huawei (华为南研所) Chen-Lin Zhang (张晨麟) NJU 2016.9 2021.12 Ph.D. 4paradigm (第四范式) Guo-Hua Wang (王国华) NJU 2018.9 2023.6 Ph.D. Huawei (华为杭研所) Yun-Hao Cao (曹云浩) NJU 2020.9/2018.9 2024.6 Ph.D./M.S. Huawei (华为南研所) Hao Yu (余浩) NJU 2019.9 2025.3 Ph.D. Alibaba (阿里巴巴) Ke Zhu (朱可) NJU 2022.9/2020.9 in progress Ph.D./M.S. - Jie Shao (邵杰) NJU 2022.9 in progress Ph.D. - Ming-Hao Fu (付明浩) NJU 2023.9/2021.9 in progress Ph.D./M.S. - Guang Liang (梁广) NJU 2024.9 in progress Ph.D. - Ning-Yuan Tang (唐宁远) NJU 2025.9/2022.9 in progress Ph.D./M.S. - Guo-Bing Zhou (周国兵) NJU 2013.9 2016.7 M.S. Huatai Securities (华泰证券) Wang Zhou (周旺) NJU 2014.9 2017.7 M.S. Baidu (百度) Chen-Wei Xie (谢晨伟) NJU 2015.9 2018.6 M.S. Alibaba (阿里巴巴) Hong-Yu Zhou (周洪宇) NJU 2015.9 2018.6 M.S. Tencent (腾讯优图), currently Asst. Prof. @ Tsinghua Hao Zhang (张皓) NJU 2016.9 2019.9 M.S. Tencent (腾讯优图) Xin-Xin Liu (刘鑫鑫) NJU 2017.9 2020.6 M.S. Huawei (华为海思) Kun Yi (易坤) NJU 2017.9 2020.6 M.S. Tencent (腾讯) Yong-Shun Zhang (张永顺) NJU 2019.9 2022.6 M.S. SEA/Shopee Yi-Fan Ge (葛一帆) NJU 2019.9 2022.6 M.S. Jump Trading Huuan-Yu Wang (王环宇) NJU 2019.9 2022.6 M.S. SEA/Shopee Ying-Xiao Du (杜映潇) NJU 2020.9 2023.6 M.S. Horizon Robotics (地平线机器人) Yin-Yin He (何银银) NJU 2020.9 2023.6 M.S. ByteDance (字节跳动) Lin Sui (隋霖) NJU 2020.9 2023.6 M.S. 4paradigm (第四范式) Yi-Fan Zhou (周逸帆) NJU 2020.9 2023.6 M.S. Jump Trading Tian-Yu Liu (刘天宇) NJU 2021.9 2024.6 M.S. (杉树资产) Han-Xiao Zhang (张含笑) NJU 2022.9 2025.6 M.S. (九坤投资) Jun-Jie Zhou (周俊杰) NJU 2023.9 in progress M.S. - Pei-Lin Sun (孙培林) NJU 2025.9 in progress M.S. - Jin Tong (童锦) NJU 2025.9 in progress M.S. - Dong-Wei Gan (甘东伟) NJU 2025.9 in progress M.S. - Research I am mainly interested in computer vision (CV) and machine learning (ML), especially when the computing resources (CPU, GPU, running time, memory, model size, etc.) or data resources (size and distribution of training set, quality of labels and annotations, etc.) are limited. Deep learning (DL) with resource constraints are my current focus. LLMs and so on Understanding LLMs--who reasons in a reasoning LLM?: Paper [C83] CV & ML with limited computing resources Finetuning large deep models DTL: How to save 2/3 GPU memory in parameter-efficient finetuning?: Paper [C70] Memory- and parameter- efficient finetuning (extension of DTL): Paper[J57] How to save communication (transferring data between cloud and edge) and fine-tune on edge?: Paper [C79] Model Quantization GPLQ: QAT training with only 1 epoch (instead of 1200!) and not overfitting: Paper and Code [C82] PTQ quantizing all sorts of networks: fast & accurate, simple & general, on GPUs & simulators---QwT (Quantization without Tears): Paper [C78] Quantizing large language models (LLM): Paper [C77] Quantization for self-supervised learning: Paper [C62] To make quantization work for ViT detection etc. by KD from quantized penultimate layer: Paper [C65] Deep network compression, acceleration, and generation Compressing Diffusion models to 1 bit storage: Paper [C80] Teacher-free KD (SSL models like DINO as teacher--no teacher training on downstream task): Paper [C76] Zero-/Few-shot dense compression of Vision Transforms: Paper [C71] Few-shot network compression---the Practise algorithm: Paper [C67] and an interesting theory to support it: Paper [J54] A unified compression scheme for Vision Transformers and variants: Paper [J49] Few-shot, unlabeled compression of (all) linear layers (in particular in Transformers) with compression and acceleration but small accuracy drop: Paper [C66] Compressing recommender systems (embeddings+model): faster, smaller, and better: Paper [C73] Versatile, Full-spectrum and Swift network generation: Paper [J48] Feature mimicking with few-samples: Paper [C60] Feature mimicking: a new knowledge distillation paradigm: Paper [J46] CURL: Network compression with only small dataset and/or residual connections: Paper [C51] AutoPruner: Variable ratio channel pruning: Paper [J42] Channel pruning based on activation approximation: Papers [C41], [J37] A simple acceleration trick for detection using deep networks: Paper [C42] Deep learning beyond CNN/RNN/Attention NRS: Neural random subspace: Paper [J45] RSS: NRS for learning with missing data: Paper [J51] CV & ML with limited data resources Few-shot deep learning A new & practical few-shot learning problem (pFSL): Paper: [C72] Worst-case accuracy more important and difficult than average accuracy in few-shot learning: Paper [C63] Deep learning with only very few samples (training from scratch) Training vision Transformers with only 2040 images (IDMM self-supervised learning): Paper [C61] Weakly supervised localization & detection Weakly supervised object localization (WSOL): Paper [C52] (proposing a paradigm shift for WSOL) Object co-localization using deep models: Papers [C39], [J38] Weakly supervised object detection (WSOD): Papers [C59] (Salvage of Supervision, or SoS) T-PAMI extension of SoS (see the above item) for WSOL, WSOD, WSIS (weakly supervised instance segmentation) and WSSS (weakly supervised semantic segmentation): Papers [J52] Weakly supervised foreground learning for both WSOL and WSOD: Paper [J50] Learning with zero, partial, incomplete, noisy, and weak labels Multi-Label SSL: coarse cropping & turn it into object-centric SSL. Papers [J56] Multi-Label SSL: self-supervised learning on scene images. Papers [C69] ActionFormer: one-shot, anchor-free action localization using Transformers. Papers [C64] Tobias: A random network (without any training) can localize objects! Papers [C58], the theoretical underpinning of Tobias [J53] Theoretical results and practical algorithm for semi-supervised deep learning: Paper [C50] End-to-end deep learning in the presence of noisy labels: Paper [C48] Using weak labels for recognition: Papers [C38], [J32], [C46], CSRA [C57] (simple but powerful multi-label recognition) Fine-grained classification and retrieval without using bounding box annotations: Papers [J22], [J31], New datasets [C55] Dealing with imbalanced & long-tailed data distribution Reviving undersampling for imbalanced learning. PR paper Long-tailed recognition with the help of synthetic (diffusion-generated) data: NeurIPS'24 paper Improving long-tailed detection by rectifying the regression bias: ECCV'24 paper Improving the worst instead of average accuracy in long-tailed recognition: CVPR'23 paper Long-tailed recognition: AAAI'21 paper (bag of tricks for long-tailed recognition), ICCV'21 paper (DiVE: balanced virtual example distribution) Learning with imbalanced datasets: Papers [C5], [C12], [J6] Imbalance in face detection: Papers [C4], [J4], [J5] Multi-instance learning Scalable MIL: Papers [C33], [J21] MIL with multi-view: Paper [C34] Earlier work & other work CV & ML with limited computing resources Kernel approximation (in SVM and beyond): Papers [C10], [C17], [J10], [J19], [C25] Cascade structured classifier and detector: Papers [C3], [C4], [J4], [J5] Creating visual codebooks using additive kernels: Papers [C8], [J8], [J17] High-dimensional visual features and their compact representations: Papers [C28], [C37], [J24] Real time object detection based on HIK: Papers [C10], [C13], [J12] Detection and recognition using sensors beyond camera (RFID, mobile sensor, etc.), and beyond the computer (robot, mobile phone e",
  "content_length": 10551,
  "method": "requests",
  "crawl_time": "2025-12-01 13:30:06"
}