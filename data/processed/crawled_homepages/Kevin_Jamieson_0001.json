{
  "name": "Kevin Jamieson 0001",
  "homepage": "https://homes.cs.washington.edu/~jamieson/about.html",
  "status": "success",
  "content": "Kevin Jamieson Kevin Jamieson Associate Professor, Allen School of Computer Science & Engineering Adjunct Professor, Department of Statistics University of Washington jamieson@cs.washington.edu Office: CSE2 340 Gates Center for Computer Science & Engineering University of Washington Seattle, WA 98195 Now recruiting graduate students and post-docs! Please read more here. About Kevin Jamieson is an Associate Professor in the Paul G. Allen School of Computer Science & Engineering at the University of Washington. He received his B.S. in 2009 from the University of Washington under the advisement of Maya Gupta, his M.S. in 2010 from Columbia University under the advisement of Rui Castro, and his Ph.D. in 2015 from the University of Wisconsin - Madison under the advisement of Robert Nowak, all in electrical engineering. He returned to the University of Washington as faculty in 2017 after a postdoc in the AMP lab at the University of California, Berkeley working with Benjamin Recht. Jamieson's work has been recognized by an NSF CAREER award and Amazon Faculty Research award. Research focus Jamiesonâ€™s research explores how to leverage already-collected data to inform what future measurements to make next, in a closed loop. Such active learning can extract considerably richer insights than any measurement plan fixed in advance, using the same statistical budget. His work ranges from theory to practical algorithms with guarantees to open-source machine learning systems and has been adopted in a range of applications, including measuring human perception in psychology studies, adaptive A/B/n testing in dynamic web-environments, numerical optimization, and choosing hyperparameters for deep neural networks. Specifically, Jamieson has made foundational contributions to the characterization of the instance-dependent sample complexity of a variety of closed-loop active learning scenarios. Algorithms that are instance-dependent optimal adapt to the true difficulty of the problem taking fewer samples when it is easy, and more when it is hard. Sometimes known as gap-dependent bounds, these sample complexity bounds are in contrast to minimax bounds which reflect worst-case performance. A primary motivation for studying this regime is the observation that Nature is not adversarial, and we would like our algorithms to take advantage of easy settings. Jamieson's work has shown that popular strategies like UCB/Thompson Sampling for bandits/reinforcement-learning, and disagreement-based methods for classification, while minimax, can behave arbitrarily worse than the instance-optimal algorithms that his group has developed. His group's papers are a mix of information theoretic lower bounds, computational and sample efficient algorithm design, and the statistical analysis of adaptively collected data. Selected Publications by area See Publications for a more comprehensive list. + Multi-armed Bandits, Subset Selection Minimax Optimal Submodular Optimization with Bandit Feedback, Artin Tajdini, Lalit Jain, Kevin Jamieson, NeurIPS 2024. PDF The True Sample Complexity of Identifying Good Arms, Julian Katz-Samuels, Kevin Jamieson, AISTATS 2020. PDF A Bandit Approach to Multiple Testing with False Discovery Control, Kevin Jamieson, Lalit Jain, NeurIPS, 2018. PDF Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization, Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, Ameet Talwalkar, JMLR, 2018*. PDF The Simulator: Understanding Adaptive Sampling in the Moderate-Confidence Regime, Max Simchowitz, Kevin Jamieson, Benjamin Recht, COLT, 2017. PDF Best-of-K Bandits, Max Simchowitz, Kevin Jamieson, Benjamin Recht, COLT, 2016. PDF Non-stochastic Best Arm Identification and Hyperparameter Optimization, Kevin Jamieson, Ameet Talwalkar, AISTATS, 2016. PDF lil' UCB : An Optimal Exploration Algorithm for Multi-Armed Bandits, Kevin Jamieson, Matt Malloy, Robert Nowak, and Sebastien Bubeck, COLT, 2014. PDF + Linear, Combinatorial, and Contextual Bandits, Experimental Design Optimal Exploration is no harder than Thompson Sampling, Zhaoqi Li, Kevin Jamieson, Lalit Jain, AISTATS 2024. PDF Instance-optimal PAC Algorithms for Contextual Bandits, Zhaoqi Li, Lillian Ratliff, Houssam Nassif, Kevin Jamieson, Lalit Jain, NeurIPS 2022. PDF High-Dimensional Experimental Design and Kernel Bandits, Romain Camilleri, Julian Katz-Samuels, Kevin Jamieson, ICML 2021. PDF Experimental Design for Regret Minimization in Linear Bandits, Andrew Wagenmaker, Julian Katz-Samuels, Kevin Jamieson, AISTATS 2021. PDF An Empirical Process Approach to the Union Bound: Practical Algorithms for Combinatorial and Linear Bandits, Julian Katz-Samuels, Lalit Jain, Zohar Karnin, Kevin Jamieson, NeurIPS 2020. PDF Sequential Experimental Design for Transductive Linear Bandits, Tanner Fiez, Lalit Jain, Kevin Jamieson, Lillian Ratliff, NeurIPS 2019. PDF Active Ranking using Pairwise Comparisons, Kevin Jamieson and Robert Nowak, Neural Information Processing Systems (NeurIPS), 2011. PDF (Extended version) + Classification Fair Active Learning in Low-Data Regimes, Romain Camilleri, Andrew Wagenmaker, Jamie Morgenstern, Lalit Jain, Kevin Jamieson, UAI 2024. PDF Corruption Robust Active Learning, Yifang Chen, Simon Shaolei Du, Kevin Jamieson, NeurIPS 2021. PDF Improved Algorithms for Agnostic Pool-based Active Classification, Julian Katz-Samuels, Jifan Zhang, Lalit Jain, Kevin Jamieson, ICML 2021. PDF A New Perspective on Pool-Based Active Classification and False-Discovery Control, Lalit Jain, Kevin Jamieson, NeurIPS 2019. PDF + Continuous Control and Learning Optimal Exploration for Model-Based RL in Nonlinear Systems, Andrew Wagenmaker, Guanya Shi, Kevin Jamieson, NeurIPS 2023. PDF Task-Optimal Exploration in Linear Dynamical Systems, Andrew Wagenmaker, Max Simchowitz, Kevin Jamieson, ICML 2021. PDF Active Learning for Identification of Linear Dynamical Systems, Andrew Wagenmaker, Kevin Jamieson, COLT 2020. PDF + Reinforcement learning, Tabular Sample Complexity Reduction via Policy Difference Estimation in Tabular Reinforcement Learning, Adhyyan Narang, Andrew Wagenmaker, Lillian Ratliff, Kevin Jamieson, NeurIPS 2024. PDF Beyond No Regret: Instance-Dependent PAC Reinforcement Learning, Andrew Wagenmaker, Max Simchowitz, Kevin Jamieson, COLT 2022. PDF Improved Corruption Robust Algorithms for Episodic Reinforcement Learning, Yifang Chen, Simon S. Du, Kevin Jamieson, ICML 2021. PDF Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs, Max Simchowitz, Kevin Jamieson, NeurIPS 2019. PDF + Reinforcement learning, Function Approximation Overcoming the Sim-to-Real Gap: Leveraging Simulation to Learn to Explore for Real-World RL, Andrew Wagenmaker, Kevin Huang, Liyiming Ke, Byron Boots, Kevin Jamieson, Abhishek Gupta, NeurIPS 2024. PDF Instance-Dependent Near-Optimal Policy Identification in Linear MDPs via Online Experiment Design, Andrew Wagenmaker, Kevin Jamieson, NeurIPS 2022. PDF Reward-Free RL is No Harder Than Reward-Aware RL in Linear Markov Decision Processes, Andrew Wagenmaker, Yifang Chen, Max Simchowitz, Simon S. Du, Kevin Jamieson, ICML 2022. PDF First-Order Regret in Reinforcement Learning with Linear Function Approximation: A Robust Estimation Approach, Andrew Wagenmaker, Yifang Chen, Max Simchowitz, Simon S. Du, Kevin Jamieson, ICML 2022. PDF + Learning and Games with Strategic Actors Near-Optimal Pure Exploration in Matrix Games: A Generalization of Stochastic Bandits & Dueling Bandits, Arnab Maiti, Ross Boczar, Kevin Jamieson, Lillian J. Ratliff, AISTATS 2024. PDF Logarithmic Regret for Matrix Games against an Adversary with Noisy Bandit Feedback, Arnab Maiti, Kevin Jamieson, Lillian J. Ratliff, Preprint. PDF Instance-dependent Sample Complexity Bounds for Zero-sum Matrix Games, Arnab Maiti, Kevin Jamieson, Lillian J. Ratliff, AISTATS 2023. PDF + Representation Learning and Data Curation for Large Models CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning, Yiping Wang*, Yifang Chen*, Wendan Yan, Alex Fang, Wenjin Zhou, Simon Du, Kevin Jamieson, NeurIPS 2024. PDF Active representation learning for general task space with applications in robotics, Yifang Chen, Yingbing Huang, Simon Shaolei Du, Kevin Jamieson, Guanya Shi, NeurIPS 2023. PDF Improved Active Multi-Task Representation Learning via Lasso, Yiping Wang, Yifang Chen, Simon Du, Kevin Jamieson, ICML 2023. PDF Active Multi-Task Representation Learning, Yifang Chen, Simon S. Du, Kevin Jamieson, ICML 2022. PDF + Robotics Overcoming the Sim-to-Real Gap: Leveraging Simulation to Learn to Explore for Real-World RL, Andrew Wagenmaker, Kevin Huang, Liyiming Ke, Byron Boots, Kevin Jamieson, Abhishek Gupta, NeurIPS 2024. PDF Active representation learning for general task space with applications in robotics, Yifang Chen, Yingbing Huang, Simon Shaolei Du, Kevin Jamieson, Guanya Shi, NeurIPS 2023. PDF Large-Scale Package Manipulation via Learned Metrics of Pick Success, Shuai Li, Azarakhsh Keipour, Kevin Jamieson, Nicolas Hudson, Charles Swan, Kostas Bekris, RSS 2023. PDF Leveraging Post Hoc Context for Faster Learning in Bandit Settings with Applications in Robot-Assisted Feeding, Ethan K. Gordon, Sumegh Roychowdhury, Tapomayukh Bhattacharjee, Kevin Jamieson, Siddhartha S. Srinivasa, ICRA 2021. PDF Comparing Human-Centric and Robot-Centric Sampling for Robot Deep Learning from Demonstrations, Michael Laskey, Caleb Chuck, Jonathan Lee, Jeffrey Mahler, Sanjay Krishnan, Kevin Jamieson, Anca Dragan, Ken Goldberg, International Conference on Robotics and Automation (ICRA), 2017. PDF",
  "content_length": 9591,
  "method": "requests",
  "crawl_time": "2025-12-01 13:42:12"
}