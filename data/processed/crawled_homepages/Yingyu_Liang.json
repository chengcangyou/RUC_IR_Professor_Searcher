{
  "name": "Yingyu Liang",
  "homepage": "https://yingyuliang.github.io",
  "status": "success",
  "content": "Yingyu Liang Home Teaching/Advising Publications About Me I am an associate professor of IDS and CS at the University of Hong Kong, and an adjunct professor of Computer Sciences at the University of Wisconsin-Madison by courtesy. Before I was an assistant then associate professor of Computer Sciences at the University of Wisconsin-Madison. I was a postdoc at Princeton and had the pleasure to work with Sanjeev Arora. I received my Ph.D. in 2014 from Georgia Tech, where I was advised by Nina Balcan and also worked closely with Le Song. I received my M.S. (2010) and B.S. (2008) from Tsinghua University advised by Jianmin Li and Bo Zhang. I'm a recipient of the NSF CAREER award. Research Machine learning. In particular, providing theoretical foundations for modern machine learning models and designing efficient algorithms for real world applications. Recent focuses include optimization and generalization in deep learning, language modeling, robust learning, and their applications. Teaching HKU DATA8003: Theoretical Foundation of Deep Learning HKU COMP3354: Statistical Learning UW-Madison CS 839: Theoretical Foundations of Deep Learning [Spring 2023][Spring 2022] UW-Madison CS 540: Introduction to Artificial Intelligence [Fall 2023] [Fall 2022] [Fall 2021] [Fall 2020] [Spring 2019] [Spring 2018] UW-Madison CS 760: Machine Learning [Spring 2021] [Spring 2020] [Fall 2018] [Fall 2017] Princeton COS 495: Introduction to Deep Learning [Spring 2016] Advising Students Zidong Liu, Zhuoyan Xu (co-advised with Yin Li and Yiqiao Zhong) Alumni Zhenmei Shi, Yang Guo (Co-advised with Rob Nowak), Junyi Wei, Nils Palumbo, Mehmet Furkan Demirel, Jiefeng Chen (Co-advised with Somesh Jha), Prathusha Sarma (Co-advised with William Sethares), Siddhant Garg, Zhongkai Sun (Co-advised with William Sethares), Shengchao Liu Publications (authors are listed in alphabetic order, except for those papers with *) (Full/up-to-date list on Google Scholar page) Selected Publications Provable Guarantees for Neural Networks via Gradient Feature Learning* Zhenmei Shi, Jenny Wei, Yingyu Liang. Neural Information Processing Systems (NeurIPS), 2023. [NeurIPS] [arXiv] The Trade-off between Universality and Label Efficiency of Representations from Contrastive Learning* Zhenmei Shi, Jiefeng Chen, Kunyang Li, Jayaram Raghuram, Xi Wu, Yingyu Liang, Somesh Jha. International Conference on Learning Representations (ICLR), 2023. [ICLR] A Theoretical Analysis on Feature Learning in Neural Networks: Emergence from Inputs and Advantage over Fixed Features* Zhenmei Shi, Jenny Wei, Yingyu Liang. International Conference on Learning Representations (ICLR), 2022. [ICLR] Functional Regularization for Representation Learning: A Unified Theoretical Perspective Siddhant Garg, Yingyu Liang. Neural Information Processing Systems (NeurIPS), 2020. [arXiv] Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers Zeyuan Allen-Zhu, Yuanzhi Li, Yingyu Liang. Neural Information Processing Systems (NeurIPS), 2019. [arXiv] Conference Publications Dissecting Submission Limit in Desk-Rejections: A Mathematical Analysis of Fairness in AI Conference Policies Yuefan Cao, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, Jiahao Zhang. International Conference on Machine Learning (ICML), 2025. [OpenReview] [arXiv] Fundamental Limits of Visual Autoregressive Transformers: Universal Approximation Abilities Yifang Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song. International Conference on Machine Learning (ICML), 2025. [OpenReview] Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, Yufa Zhou. International Conference on Learning Representations (ICLR), 2025. [OpenReview] [arXiv] Learning to Inference Adaptively for Multimodal Large Language Models* Zhuoyan Xu, Khoi Duc Nguyen, Preeti Mukherjee, Saurabh Bagchi, Somali Chaterji, Yingyu Liang, Yin Li. International Conference on Computer Vision (ICCV), 2025. [OpenReview] Unraveling the Smoothness Properties of Diffusion Models: A Gaussian Mixture Perspective Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, Mingda Wan, Yufa Zhou. International Conference on Computer Vision (ICCV), 2025. [OpenReview] Towards Infinite-Long Prefix in Transformers Yingyu Liang, Zhenmei Shi, Zhao Song, Chiwun Yang. Empirical Methods in Natural Language Processing (EMNLP), 2025. Circuit Complexity Bounds for RoPE-based Transformer Architecture Bo Chen, Xiaoyu Li, Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, Jiahao Zhang. Empirical Methods in Natural Language Processing (EMNLP), 2025. Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers Yingyu Liang, Heshan Liu, Zhenmei Shi, Zhao Song, Zhuoyan Xu, Jiale Zhao, Zhen Zhuang. Empirical Methods in Natural Language Processing Findings (EMNLP Findings), 2025. NRFlow: Towards Noise-Robust Generative Modeling via High-Order Mechanism Bo Chen, Chengyue Gong, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, Mingda Wan, Xugang Ye. Conference on Uncertainty in Artificial Intelligence (UAI), 2025. [OpenReview] When Can We Solve the Weighted Low Rank Approximation Problem in Truly Subquadratic Time? Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song. International Conference on Artificial Intelligence and Statistics (AISTAT), 2025. [OpenReview] [arXiv] Fourier Circuits in Neural Networks and Transformers: A Case Study of Modular Arithmetic with Multiple Inputs Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Tianyi Zhou. International Conference on Artificial Intelligence and Statistics (AISTAT), 2025. [OpenReview] [arXiv] Looped ReLU MLPs May Be All You Need as Practical Programmable Computers Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, Yufa Zhou. International Conference on Artificial Intelligence and Statistics (AISTAT), 2025. [OpenReview] [arXiv] Bypassing the Exponential Dependency: Looped Transformers Efficiently Learn In-context by Multi-step Gradient Descent Bo Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song. International Conference on Artificial Intelligence and Statistics (AISTAT), 2025. [OpenReview] [arXiv] Differential Privacy Mechanisms in Neural Tangent Kernel Regression Jiuxiang Gu, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2025. [arXiv] The Computational Limits of State-Space Models and Mamba via the Lens of Circuit Complexity Yifang Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song. Conference on Parsimony and Learning (CPAL), 2025. Oral. [OpenReview] [arXiv] Fast John Ellipsoid Computation with Differential Privacy Optimization Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Junwei Yu. Conference on Parsimony and Learning (CPAL), 2025. Oral. [OpenReview] [arXiv] Curse of Attention: A Kernel-Based Perspective for Why Transformers Fail to Generalize on Time Series Forecasting and Beyond Yekun Ke, Yingyu Liang, Zhenmei Shi, Zhao Song, Chiwun Yang. Conference on Parsimony and Learning (CPAL), 2025. [OpenReview] [arXiv] HSR-Enhanced Sparse Attention Acceleration Bo Chen, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song. Conference on Parsimony and Learning (CPAL), 2025. [OpenReview] [arXiv] Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability* Zhuoyan Xu, Zhenmei Shi, Yingyu Liang. Conference on Language Modeling (COLM), Oct 2024. [COLM] [arXiv] Why Larger Language Models Do In-context Learning Differently?* Zhenmei Shi, Junyi Wei, Zhuoyan Xu, Yingyu Liang. International Conference on Machine Learning (ICML), July 2024. [ICML] [arXiv] Two Heads are Actually Better than One: Towards Better Adversarial Robustness via Transduction and Rejection* Nils Palumbo, Yang Guo, Xi Wu, Jiefeng Chen, Yingyu Liang, Somesh Jha. International Conference on Machine Learning (ICML), July 2024. [ICML] Towards Few-shot Adaptation of Foundation Models via Multitask Finetuning* Zhuoyan Xu, Zhenmei Shi, Junyi Wei, Fangzhou Mu, Yin Li, Yingyu Liang. International Conference on Learning Representations (ICLR), May 2024. [ICLR] [arXiv] Domain Generalization via Nuclear Norm Regularization* Zhenmei Shi, Yifei Ming, Ying Fan, Frederic Sala, Yingyu Liang. Conference on Parsimony and Learning (CPAL), Jan 2024. [arXiv] Provable Guarantees for Neural Networks via Gradient Feature Learning* Zhenmei Shi, Jenny Wei, Yingyu Liang. Neural Information Processing Systems (NeurIPS), 2023. [NeurIPS] [arXiv] Dissecting Knowledge Distillation: An Exploration of its Inner Workings and Applications* Utkarsh Ojha, Yuheng Li, Anirudh Sundara Rajan, Yingyu Liang, Yong Jae Lee. Neural Information Processing Systems (NeurIPS), 2023. [NeurIPS] Stratified Adversarial Robustness with Rejection* Jiefeng Chen, Jayaram Raghuram, Jihye Choi, Xi Wu, Yingyu Liang, Somesh Jha. International Conference on Machine Learning (ICML), 2023. [ICML] [arXiv] When and How Does Known Class Help Discover Unknown Ones? Provable Understandings Through Spectral Analysis* Yiyou Sun, Zhenmei Shi, Yingyu Liang, Yixuan Li. International Conference on Machine Learning (ICML), 2023. [ICML] [arXiv] The Trade-off between Universality and Label Efficiency of Representations from Contrastive Learning* Zhenmei Shi, Jiefeng Chen, Kunyang Li, Jayaram Raghuram, Xi Wu, Yingyu Liang, Somesh Jha. International Conference on Learning Representations (ICLR), 2023. (Spotlight) [ICLR] A Theoretical Analysis on Feature Learning in Neural Networks: Emergence from Inputs and Advantage over Fixed Features* Zhenmei Shi, Jenny Wei, Yingyu Liang. International Conference on Learning Representations (ICLR), 2022. [ICLR] Towards Evaluating the Robustness of Neural Networks Learned by Transduction* Jiefeng Chen, Xi Wu, Yang Guo, Yingyu Liang, Somesh Jha. International Conference on Learning Representations (ICLR), 2022. [ICLR] Deep Online Fused Video Stabilization* Zhenmei Shi, F",
  "content_length": 22330,
  "method": "requests",
  "crawl_time": "2025-12-01 14:51:42"
}