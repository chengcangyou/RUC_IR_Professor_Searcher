{
  "name": "J. Krishna Murthy 0001",
  "homepage": "https://krrish94.github.io",
  "status": "success",
  "content": "Krishna Murthy Jatavallabhula Email | Google Scholar CV | PhD Thesis GitHub | LinkedIn I am an incoming assistant professor with the CS department at Johns Hopkins University, where I will direct the SciPhy lab. I strive to build full-stack robotic systems that perceive, reason, and act with human-level efficiency, ultimately surpassing them. My work lies at the perception-action interface, tackling both how robots should represent the world around them, and how they use it for action. Prior to this, I spent time as a research scientist at Meta’s robotics team, as a postdoc at MIT CSAIL and as a PhD candidate at Universite de Montreal. My work has been recognized with PhD fellowship awards from NVIDIA and Google, a best-paper award from IEEE RAL, and an induction to the RSS Pioneers 2020 cohort. News Oct 19, 2025 Announcing the SciPhy lab at Johns Hopkins University, starting Fall 2026. Jun 12, 2025 By Fall 2026, I will join the CS department at Johns Hopkins University as an assistant professor. Sep 15, 2024 I joined FAIR, Meta as an AI research scientist. Sep 13, 2024 I will serve as an area chair for CVPR 2025. Sep 12, 2024 I completed an eventful 2.5-year postdoc stint at MIT CSAIL. May 31, 2024 Serving as OpenReview chair for CoRL 2024 Feb 29, 2024 Speaking at the UMD/Microsoft future leaders in robotics and AI seminar series Jan 28, 2024 6 papers accepted to ICRA 2024. Jan 22, 2024 Serving as associate editor for IROS and RA-L. Sep 27, 2023 Another webpage update, featuring new work, including ConceptGraphs. Feb 11, 2023 Long overdue webpage update, including the featured Conceptfusion work. Mar 14, 2022 I moved to MIT to start my potsdoc with Josh Tenenbaum and Antonio Torralba. Mar 10, 2022 Got my PhD with grade: exceptional! Feb 1, 2022 Serving as associate editor for IROS 2022 Sep 14, 2021 Organizing workshops Diff3D ICCV 2021, and the PRIBR at Neurips 2021. Sep 4, 2021 Teaching the realistic/advanced image synthesis class at McGill university (Fall 2021). Jan 21, 2021 Awarded a Google PhD fellowship (declined) Dec 20, 2020 Organizing the rethinking ML papers workshop at ICLR 2021 Nov 30, 2020 Honored to have received an NVIDIA graduate fellowship for 2021-22 Nov 9, 2020 gradSLAM is available as an open-source PyTorch framework here Sep 2, 2020 Organizing the robot learning seminar series at Mila Aug 31, 2020 Organizing the differentiable vision, graphics, physics workshop at Neurips 2020 Jul 5, 2020 Selected to the RSS pioneers cohort for 2020 Jun 4, 2020 Our paper, MapLite, named best paper, IEEE RAL 2019. Feb 11, 2020 Our paper on fully differentiable dense SLAM will be (virtually) presented at ICRA 2020 Nov 13, 2019 Released NVIDIA Kaolin: a 3D deep learning library Featured publications ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning Qiao Gu*, Alihusein Kuwajerwala*, Sacha Morin*,  Krishna Murthy Jatavallabhula*, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, Chuang Gan, Celso Miguel de Melo, Joshua B. Tenenbaum, Antonio Torralba, Florian Shkurti, and Liam Paull ICRA 2024 PDF Webpage Code Abs Bib Video Contributions For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning. Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations. However, these approaches tend to produce maps with per-point feature vectors, which do not scale well in larger environments, nor do they contain semantic spatial relationships between entities in the environment, which are useful for downstream planning. In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models. We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts. I co-led multiple aspects of this work, building off of our prior work with ConceptFusion. @inproceedings{conceptgraphs, title = {ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning}, author = {Gu, Qiao and Kuwajerwala, Alihusein and Morin, Sacha and Jatavallabhula, {Krishna Murthy} and Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and Gan, Chuang and {de Melo}, {Celso Miguel} and Tenenbaum, {Joshua B.} and Torralba, Antonio and Shkurti, Florian and Paull, Liam}, year = {2024}, booktitle = {ICRA}, featured = {true} } ConceptFusion: Open-set Multimodal 3D Mapping Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, Joshua B. Tenenbaum, Celso Miguel de Melo, Madhava Krishna, Liam Paull, Florian Shkurti, and Antonio Torralba RSS 2023 PDF Webpage Code Abs Bib Video Contributions Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason bout a finite set of concepts, pre-defined at training time. Further, these maps can only be queried using class labels, or in recent work, using text prompts. We address both these issues with ConceptFusion, a scene representation that is: (i) fundamentally open-set, enabling reasoning beyond a closed set of concepts (ii) inherently multi-modal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today’s foundation models pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40 percent margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping. I conceived the idea and led the project. I also wrote much of the code and the paper. I curated and annotated the UnCoCo dataset and helped with the tabletop robot experiments. @inproceedings{conceptfusion, title = {ConceptFusion: Open-set Multimodal 3D Mapping}, author = {Jatavallabhula, {Krishna Murthy} and Kuwajerwala, Alihusein and Gu, Qiao and Omama, Mohd and Chen, Tao and Li, Shuang and Iyer, Ganesh and Saryazdi, Soroush and Keetha, Nikhil and Tewari, Ayush and Tenenbaum, {Joshua B.} and {de Melo}, {Celso Miguel} and Krishna, Madhava and Paull, Liam and Shkurti, Florian and Torralba, Antonio}, year = {2023}, booktitle = {RSS}, dataset and helped with the tabletop robot experiments.}, featured = {true} } gradSim: Differentiable simulation for system identification and visuomotor control Krishna Murthy Jatavallabhula*, Miles Macklin*, Florian Golemo, Vikram Voleti, Linda Petrini, Martin Weiss, Breandan Considine, Jerome Parent-Levesque, Kevin Xie, Kenny Erleben, Liam Paull, Florian Shkurti, Derek Nowrouzezahrai, and Sanja Fidler ICLR 2021 PDF Webpage Code Abs Bib Video TL;DR Contributions In this paper, we tackle the problem of estimating object physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is fundamentally ill-posed due to the loss of information during image formation. Current best solutions to the problem require precise 3D labels which are labor intensive to gather, and infeasible to create for many systems such as deformable solids or cloth. In this work we present gradSim, a framework that overcomes the dependence on 3D supervision by combining differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation. This unique combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them. Furthermore, our unified computation graph across dynamics and rendering engines enables the learning of challenging visuomotor control tasks, without relying on state-based (3D) supervision, while obtaining performance competitive to/better than techniques that require precise 3D labels. Differentiable models of time-varying dynamics and image formation pipelines result in highly accurate physical parameter estimation from video and visuomotor control. This idea was jointly conceived in a meeting which included me, Derek, Breandan, Martin, Bhairav Mehta, and Maxime Chevalier-Boisvert. Martin prototyped an initial differentiable billiards engine. I implemented the first rigid-body engine, integrated it with a differentiable renderer, and setup sys-id experiments. Miles and I then joined forces, with him focusing on the physics engine and me focusing on the physics + rendering combination and overall systems integration. I ran all of the experiments for this paper. Florian (Golemo) and Vikram created the datasets, designed experiments, and also helped with code and the manuscript. All authors participated in writing the manuscript and the author response phase. Florian (Sh",
  "content_length": 12471,
  "method": "requests",
  "crawl_time": "2025-12-01 13:24:38"
}