{
  "name": "Zheng Zhang 0005",
  "homepage": "https://web.ece.ucsb.edu/~zhengzhang",
  "status": "success",
  "content": "Zheng ZHANG's Homepage Home Members Publications Teaching Codes&Data Openings Zheng Zhang's Homepage Full Professor [IEEE-Style Short Biography, Full CV (PDF)] Department of Electrical & Computer Engineering, University of California, Santa Barbara (UCSB) Department of Computer Science (joint appointment), UCSB Department of Mathematics, (joint appointment, effective in 07/2019), UCSB Education Ph.D in EECS, Massachusetts Institute of Technology M.Phil in EE, The University of Hong KongB. Eng in EE, Huazhong University of Science & Technology Contact Email: zhengzhang [AT] ece [dot] ucsb [dot] edu, Phone: 805-893-7294Address: 4109 Harold Frank Hall, University of California, Santa Barbara, CA 93106 Multiple Post-doc Openings: We have multiple openings related to (1) efficient large language model (LLM) pre-training, (2) energy-efficient on-device training, and (3) scientific machine learning for EDA. We are collaborating with leading research groups from industry (Intel, Amazon, Meta, HP Research Labs, Cadence) and government research labs (ANL, NIST) on these topics. Currently, we are looking for candidates with any of the following technical background: Numerical optimization: stochastic optimization (e.g., theory of SGD and its variants), derivative-free optimization (e.g., zeroth-order optimization), distributed optimization. Note: we are looking for candidates with strong theoretical and numerical background. Candidates who work on the engineering application of optimization methods do NOT match this position. High-performance computing (HPC) and GPU optimization: rich experience in parallel/distributed computation for large-scale training of deep learning models or for scientific computation on massive GPUs. Edge AI hardware accelerators: hardware accelerator design with integrated photonics or with FPGA. We focus on training accelerator design rather than inference engines. Scientific machine learning for EDA: physics-informed neural networks and/or operator learning for PDE simulation and/or PDE-constrained optimization, small-data learning for design modeling and optimization, uncertainty quantification and uncertainty-aware optimization for chip design. Interested candidates please send me the following information via email: your CV, representative publications, and the contact information of two referees. We also welcome PhD applicants with prior experience in the above fields. Prior master research experience is a plus but not mandatory. For PhD applicants, please submit your application via our online graduate application system, and mention my name in your application case. Checklist for paper writing: I have prepared a detailed checklist to help science/engineering graduate students improving their paper writing. To prospective PhD students: Please read this document if you are thinking about pursuing a PhD degree. The skill sets required for PhD research are very different from those required for undergraduate study. In undergraduate study, a student learns existing knowledge that were created by others (probably a few hundred years ago). A PhD student is expected to create new knowledge. A student doesn't have to be super smart or to have a perfect GPA in order to be an excellent PhD student, but he/she may need to be self-motivated for scientific research, curious about unknown/new fields, open-minded to different opinions, and persistent when facing research challenges (or even failures). RESEARCH INTERESTS We work at the intersection of computational data science (e.g., uncertainty quantification, tensor computation, scientific machine learning) and hardware systems. Currently we focus on two broad directions: Responsible AI systems: (1) efficient methods for pre-training and fine-tuning of AI foundation models (or large language models)] and for resource-constraint on-device learning; (2) self-healing machine learning systems. Design automation: (1) uncertainty-aware design automation for electronics, photonics, and quantum circuits; (2) small-data and data-free scientific machine learning for multi-physics design of 3D IC and chiplet. Our research is supported by both government funding agencies (e.g., NSF, DOE, NIST) and industries (e.g., Meta, Intel, Amazon and Samsung). We are actively collaborating with industrial research teams (e.g., Meta, Intel, Cadence, HP, Amazon, and NVIDIA) to make practical impact. RECENT NEWS: [DeepOHeat-v1].10/2025: Our DeepOHeat-v1 framework led by Xinling Yu is accepted by IEEE Trans. Components, Packaging and Manufacturing Technology.  This work presented principled approaches to improve the accuracy and training efficiency of our previous DeepOHeat. More importantly, DeepOHeat also introduced a novel framework that uses operator learning in a trustworthy way to perform fast and accurate thermal optimization of semiconductor chip design. [NeuRIPS'2025]. We have 2 papers accepted by NeuRIPS'2025. LaX (led by Ruijie Zhang and Ziyue Liu) presents a residual connection architecture to boost the accuracy of low-rank foundation models in both pre-training and fine-tuning. SharpZO (led by Yifan Yang) proposed a sharpness-aware and hybrid approach for memory-efficient BP-free fine-tuning of vision language models. [EMNLP'2025]. We have 4 papers accepted by EMNLP'2025. CoLA (led by Ziyue Liu, Ruijie Zhang and Zhengyang Wang) presents a novel compute- and memory-efficient low-rank pre-training method for LLM; QuZO (led by Jiajun Zhou) develop an accurate quantized BP-free training method for fine-tuning LLMs; MaZO (led by Zhen Zhang) is a multi-objective BP-free fine-tuning method for LLM; Saten (led by Ryan Solgi) improves the accuracy of tensor-compressed LLMs with sparse augmentation. CoLA is also selected as an oral paper in the main conference. [TCAD'2025] Jiayi Tian's tensor-compressed on-FPGA training accelerator for transformers is accepted by IEEE TCAD. Link to the paper. [ACL'2025]. We have two papers accepted to ACL'2025: Wanda++ (led by Yifan Yang) and FedTT (led by Sajjid Ghiasvand) [NeuRIPS'2024] 09/25/2024: Our work CoMERA (see the paper), a computing- and memory-efficient rank-adaptive tensor-compressed (pre)-training method, is accepted by NeuRIPS'2024. This work was led by our former postdoc Dr. Zi Yang, in collaboration with Amazon and Meta. [EMNLP'2024] 09/20/2024: Yifan Yang's paper (see the draft) about memory-efficient zeroth-order tensor-train adaptation method for LLM fine-tuning is accepted by EMNLP'2024. This is a collaborative work with Amazon Alexa AI. [DeepOHeat Codes] 09/2024: our source codes of DeepOHeat for 3D-IC thermal analysis is released to the public (see the link). This is a collaborative work between our group and Cadence. [NIST research grant] 09/2024: we got a 3-year research grant from NIST to investigate small-data and uncertainty-aware design optimization methods for analog/RF integrated circuits and systems. [AI4Science Pre-training project] 09/2024: we will start a 3-year DOE research project to investigate the theory, algorithm and HPC implementation regarding energy-efficient pre-training of AI4Science foundation models. We will collaborate with Argonne National Labs closely on this project. Besides research funding, DOE will offer the access to hundreds to thousands of state-of-the-art GPUs for us to pre-train extreme-scale AI foundation models. [ISIT Paper on coded tensor computation] 07/2014: our collaborative paper with Prof. Haewon Jeong is presented at IEEE International Symposium on Information Theory (ISIT) held in Athen, Greece. This work was led by Jin Lee (a PhD student of Prof. Jeong), and it investigated an interesting topic: how coded computing can be extended from matrices to tensors to help quantum circuit simulation. [PhD defense] 07/03/2024: Zhuotong Chen finished his thesis defense, and he has joined Amazon to work on large language models (LLMs). Congratulations! [Intel Research Project] 07/01/2024: we just started a new research project with Intel to investigate multi-physics modeling and optimization of 3D integrated circuits and systems. We have some on-going collaboration with Intel in the direction of on-device AI training, and we are excited to expand our research collaboration. [TQE Paper] 06/2024: Zichang He's paper about quantum circuit optimization under imperfect uncertainty description is published by IEEE Trans. Quantum Engineering. [NAACL Oral Paper] 06/16/2024: Yifan Yang and Jiajun Zhou will present their LoRETTA paper at NAACL'2024 held in Mexico City, Mexico. This paper is selected as an oral paper (top 5%) of the whole conference. This paper results from our collaboration with Amazon. [NSF Project with HP Research Labs] 06/13/2024: we will start a 3-year NSF project to collaborate with HP Research Labs on scalable photonic on-device training for scientific computing. We look forward to the research results from this academia-industry collaboration. [TMRL paper] 03/2024: Zhuotong Chen's paper about self-healing methods for robust large-langage models (LLMs) is published by TMLR. [PhD defense] 08/07/2023: Zichang He finished his thesis defense, and he has joined JP Morgan to work on quantum computing. Congratulations! [Faculty job] 07/31/2023: Our postdoc associate Zi Yang has joined SUNY Albany (State University of New York at Albany) as an Assistant Professor of Mathematics and Data Science. Congratulations, Zi! [JMLR paper] 10/04/2022: Zhuotong's journal paper \"Self-healing robust neural networks via closed-loop control\" is accepted by the Journal of Machine Learning Research. SELECTED PUBLICATIONS X. Yu, Z. Liu, H. Li, Y. Li, X. Ai, Z. Zeng, I. Young and Z. Zhang, \"DeepOHeat-v1: Efficient Operator Learning for Fast and Trustworthy Thermal Simulation and Optimization in 3D-IC Design,\" IEEE Trans. Components, Packaging and Manufacturing Technology, accepted in Oct. 2025. R. Zhang*, Z. Liu*, Z. Wang and Z Zhang, \"LaX: Boosting Low-Rank Training of Foundation Models via Latent ",
  "content_length": 16385,
  "method": "requests",
  "crawl_time": "2025-12-01 14:55:02"
}