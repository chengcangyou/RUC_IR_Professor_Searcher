{
  "name": "Yonglu Li 0001",
  "homepage": "https://dirtyharrylyl.github.io",
  "status": "success",
  "content": "Yong-Lu Li Yong-Lu Li Home About Yong-Lu Li News Publications Projects Services Teaching Talks Honors Personal Interests Yong-Lu Li Tenure-Track Associate Professor Email: yonglu_li[at]sjtu[dot]edu[dot]cn Shanghai Jiao Tong University [上海交通大学] Shanghai Innovation Institute[上海创智学院] Team Website: RHOS [School of Artificial Intelligence (SAI)] [人工智能学院] [Zhiyuan Honors Program] [致远学院] [Google Scholar] [Github] [LinkedIn] [ORCID] [ResearchGate] [dblp] [Semantic Scholar] About I'm a tenure-track associate professor at Shanghai Jiao Tong University (SJTU), the PI of the RHOS Lab. I study Physical Reasoning, Embodied AI, and Human Activity Understanding. We are building a reasoning-driven system that enables intelligent agents to perceive, reason, and interact with the physical world. Our open-source projects have garnered over 13,000 stars on GitHub. I received the ICRA 2025 Best Paper Award (HRI, sole corresponding author), AI100 Young Pioneers (MIT review), Baidu Scholarship, WAIC Yunfan Award (twice), Shanghai Overseas High-Level Talent, Wu Wenjun AI Science and Technology Award for Excellent Doctoral Dissertation, Outstanding Reviewer of NeurIPS’20/21. I serve as the Area Chair for NeurIPS’24, NeurIPS’25, ICLR'26, a lecturer for the \"Computer Vision\" course at the ACM Honor Class of SJTU, VALSE EACC, and Deputy Secretary-General of the EAI Committee under the Chinese Association for AI. Before joining SJTU, I worked closely with IEEE Fellow Prof. Chi Keung Tang and Yu-Wing Tai at the Hong Kong University of Science and Technology (HKUST) (2021-2022). I received a Ph.D. degree (2017-2021) in Computer Science from Shanghai Jiao Tong University (SJTU), under the supervision of Prof. Cewu Lu. Prior to that, I worked and studied at the Institute of Automation, Chinese Academy of Sciences (CASIA) (2014-2017) under the supervision of Prof. Yiping Yang and A/Prof. Yinghao Cai. Research interests: Human-Robot-Scene (S) Embodied AI: how to make agents learn skills from humans and interact with the physical world. (S-1) Physical Reasoning: how to mine, capture, and embed the logics, causal relations, and laws from physical phenomenons. (S-2) General Multi-Modal Foundation Models: MLLM, VLA, pure 3D/4D large models. (S-3) Activity Understanding and Generation: how to learn, ground, and generate complex/ambiguous activity concepts (body motion, body-object/body/scene interaction) and object concepts from multi-modal information (2D-3D-4D). (E) Human-Robot Interaction: Teaching, Joint Learning, Cooperation, Healthcare, etc. Recruitment: Actively looking for self-motivated students (master/PhD, 2026 spring & fall), interns/engineers/visitors (CV/ML/ROB/NLP background, always welcome) to join us in Machine Vision and Intelligence Group (MVIG). If you share same/similar interests, feel free to drop me an email with your resume. Click Eng or 中 for more details. News and Olds 2025.08: Recieved Ant Intech Award 2025 (1/10, only one in Embnodied AI)! 2025.08: Pleasure to be an area chair of ICLR 2026! 2025.08: Our GarmageNet will appear at SIGGRAPH Asia 2025! 2025.08: Our exUMI will appear at CoRL 2025! 2025.06: Dense Policy will appear at ICCV 2025! 2025.06: Our paper on robot data selection will appear at IROS 2025! See u in Hangzhou~ 2025.05: Our paper on human-robot joint learning has won the ICRA 2025 Best Paper Award on Human-Robot Interaction! 2025.04: Our paper on human-robot joint learning has been selected as an ICRA 2025 Best Paper Award Finalist. 2025.03: Recieved AI100 Youth Pioneers (AI100青年先锋, MIT Reviews, DeepTech) from MIT Technology Review China. 2025.02: Our works on 3D HOI reconstruction, motion dynamics, garment generation/reconstruction, and dynamic object segmentation will appear at CVPR 2025! 2025.02: Pleasure to be an area chair of NeurIPS 2025! 2025.01: Our work on efficient robot teleoperation will appear at ICRA 2025. 2025.01: Two works on association ability of LLM and human motion will appear at ICLR 2025. 2024.12: Our work GIO: grounding interacted object from videos will appear at AAAI 2025. 2024.09: Two works on articulated object image manipulation, humanoid-object interaction will appear at NeurIPS 2024. 2024.07: Five works on 4D human motion, dataset distillation, embodied AI, and visual reasoning will appear at ECCV 2024. 2024.06: Our work Visual-Text Dataset Distillation will appear at ICML 2024. 2024.03: Pleasure to be an area chair of NeurIPS 2024! 2024.02: Our work Pangea and Video Distillation will appear at CVPR 2024. 2023.09: The advanced HAKE reasoning engine based on LLM (Symbol-LLM) will appear at NeurIPS 2023! 2023.07: Our works on ego-centric video understanding and object concept learning will appear at ICCV 2023! 2023.07: The upgrade version of DCR will appear at IJCV! 2023.07: Recieved Yunfan Award: Shining Star (10 Chinese AI experts under age of 35) from WAIC 2023. 2023.03: Recieved Wu Wenjun Artificial Intelligence Science and Technology Award 2022, Excellent Doctoral Dissertation from Chinese Society for Artificial Intelligence. 2023.01: HAKE is accepted by TPAMI! 2022.11: We release the human body part states and interactive object bounding box annotations upon AVA (2.1 & 2.2): [HAKE-AVA], and a CLIP-based human part state & verb recognizer: [CLIP-Activity2Vec]. 2022.11: AlphaPose will appear at TPAMI! 2022.10: Honored to be a top reviewer in NeurIPS'22! 2022.09: Joined SJTU as a tenure-track assistant professor. 2022.07: Two papers on longtailed learning, HOI detection are accepted by ECCV'22, arXivs and code are coming soon. 2022.03: Five papers on HOI detection/prediction, trajection prediction, 3D detection/keypoints are accepted by CVPR'22, papers and code are coming soon. 2022.02: We release the human body part state labels based on AVA: HAKE-AVA and HAKE 2.0. 2021.10: Recieved Outstanding Reviewer Award from NeurIPS'21. 2021.10: Learning Single/Multi-Attribute of Object with Symmetry and Group is accepted by TPAMI!. 2021.09: Our work Localization with Sampling-Argmax will appear at NeurIPS'21! 2021.05: Selected as the Chinese AI New Star Top-100 (Machine Learning). 2021.02: Upgraded HAKE-Activity2Vec is released! Images/Videos --> human box + ID + skeleton + part states + action + representation. [Demo] [Description] 2021.01: TIN (Transferable Interactiveness Network) is accepted by TPAMI! 2021.01: Recieved Baidu Scholarship (10 recipients globally). 2020.09: Our work HOI Analysis will appear at NeurIPS 2020. 2020.07: Fortunate to recieve WAIC YunFan Award and be among the 2nd A-Class Project. 2020.06: The larger HAKE-Large (>120K images with activity and part state labels) is released! 2020.02: Three papers Image-based HAKE: PaSta-Net, 2D-3D Joint HOI Learning, Symmetry-based Attribute-Object Learning are accepted in CVPR'20! Papers and corresponding resources (code, data) will be released soon. 2019.07: Our paper InstaBoost is accepted in ICCV'19. 2019.06: The Part I of our HAKE : HAKE-HICO which contains the image-level part-state annotations is released! 2019.04: Our project HAKE: Human Activity Knowledge Engine begins trial operation! 2019.02: Our paper on Interactiveness is accepted in CVPR'19. 2018.07: Our paper on GAN & Annotation Generation is accepted in ECCV'18. 2018.05: Presentation (Kaibot Team) in TIDY UP MY ROOM CHALLENGE | ICRA'18. 2018.02: Our paper on Object Part States is accepted in CVPR'18. Publications equal contribution: * corresponding author: * L1 Sample Flow for Efficient Visuomotor Learning Weixi Song, Zhetao Chen, Tao Xu, Xianchao Zeng, Xinyu Zhou, Lixin Yang, Donglin Wang*, Cewu Lu, Yong-Lu Li*. arXiv 2025 [arXiv] [PDF] [Project] [Code] IPR-1: Interactive Physical Reasoner Mingyu Zhang*, Lifeng Zhuo*, Tianxi Tan, Guocan Xie, Xian Nie, Yan Li, Renjie Zhao, Zizhu He, Ziyu Wang, Jiting Cai, Yong-Lu Li*. arXiv 2025 [arXiv] [PDF] [Project] [Code] Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models Zehao Wang, Xinpeng Liu, Xiaoqian Wu, Yudonglin Zhang, Zhou Fang, Yifan Fang, Junfu Pu, Cewu Lu*, Yong-Lu Li*. AAAI 2026 [arXiv] [PDF] [Project] [Code] RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation Yangtao Chen, Zixuan Chen, Nga Teng Chan, Junting Chen, Junhui Yin, Jieqi Shi, Yang Gao, Yong-Lu Li*, Jing Huo*. arXiv 2025 [arXiv] [PDF] [Project] [Code] GarmageNet: a Multimodal Generative Framework for Sewing Pattern Design and Generic Garment Modeling Siran Li, Chen Liu, Ruiyang Liu, Zhendong Wang, Gaofeng He, Yong-Lu Li, Xiaogang Jin, Huamin Wang. TOG (SIGGRAPH Asia) 2025 [arXiv] [PDF] [Project] [Code] exUMI: Extensible System for Robot Teaching with Precise Proprioception and Multi-Modalities Yue Xu, Litao Wei, Pengyu An, Qingyu Zhang, Yong-Lu Li*. CoRL 2025 [arXiv] [PDF] [Project] [Code] Dense Policy: Bidirectional Autoregressive Learning of Actions Yue Su, Xinyu Zhan, Hongjie Fang, Han Xue, Hao-Shu Fang, Yong-Lu Li, Cewu Lu, Lixin Yang. ICCV 2025 [arXiv] [PDF] [Project] [Code] SIME: Enhancing Policy Self-Improvement with Modal-level Exploration Yang Jin, Jun Lv, Wenye Yu, Hongjie Fang, Yong-Lu Li, Cewu Lu. IROS 2025 [arXiv] [PDF] [Project] [Code] Motion Before Action: Diffusing Object Motion as Manipulation Condition Yue Su, Xinyu Zhan, Hongjie Fang, Yong-Lu Li, Cewu Lu, Lixin Yang. RA-L 2025 [arXiv] [PDF] [Project] [Code] Reconstructing In-the-Wild Open-Vocabulary Human-Object Interactions Boran Wen, Dingbang Huang, Zichen Zhang, Jiahong Zhou, Jianbin Deng, Jingyu Gong, Yulong Chen*, Lizhuang Ma*, Yong-Lu Li*. CVPR 2025 [arXiv] [PDF] [Project] [Code] GaPT-DAR: Category-level Garments Pose Tracking via Integrated 2D Deformation and 3D Reconstruction Li Zhang, Mingliang Xu, Jianan Wang, Qiaojun Yu, Lixin Yang, Yong-Lu Li, Cewu Lu, RujingWang, Liu Liu. CVPR 2025 [arXiv] [PDF] [Project] [Code] Design2GarmentCode: Turning Design Concepts to Tangible Garments Through Program Synthesis Feng Zhou, Ruiyang Liu, Chen Liu, Gaofeng He, Yong-Lu Li, Xiaog",
  "content_length": 29236,
  "method": "requests",
  "crawl_time": "2025-12-01 14:52:12"
}