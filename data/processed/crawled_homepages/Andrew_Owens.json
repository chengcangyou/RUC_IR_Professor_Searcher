{
  "name": "Andrew Owens",
  "homepage": "http://andrewowens.com",
  "status": "success",
  "content": "Andrew Owens Andrew Owens Associate Professor of Computer Science Cornell Tech Email: Please turn on JavaScript to view email address. Office: Bloomberg 368 Google Scholar ·  GitHub ·  Twitter ·  CV I'm an associate professor of computer science at Cornell Tech. Prior to that, I spent 5 wonderful years as an assistant professor at the University of Michigan in the EECS department. I did my PhD at MIT CSAIL, where I was advised by William Freeman and Antonio Torralba, and I was a postdoc at UC Berkeley with Alyosha Efros and Jitendra Malik. I was an undergrad at Cornell. My group's research has been generously supported by a Sloan Research Fellowship (2025) and an NSF CAREER Award (2024). Research highlights Multimodal learning: Today's computer vision methods largely require human supervision, such as object labels or natural language, to learn about the world. Humans, on the other hand, learn a great deal from associations between senses: vision trains hearing, touch trains vision, etc. We've been developing models that learn about the world by finding structure in multimodal sensory signals—especially self-supervised computer vision methods that learn from sound and touch. Learning from video: We've been developing methods that learn about the world by watching unlabeled video, such as by learning space-time correspondences through self-supervision. Image forensics: Computer vision researchers face a dilemma: as our methods get better, so do the tools for malicious image manipulation. To address this growing issue, I've been developing methods for detecting fake images. Visual illusions: We've been using diffusion models to generate visual illusions. You can learn more about my research directions here (from 2019). Research group PhD students: Ayush Shrivastava Jeongsoo Park Yiming Dou Samanta Rodriguez (co-advised with Nima Fazeli, NSF Fellow) Chao Feng Xuanchen Lu Former PhD students: Daniel Geng (PhD 2025, NSF Fellow), now at OpenAI Ziyang Chen (PhD 2025), now at Luma AI Former MS/BS students: Zhangxing Bian ·  Oscar de Lima ·  Rui Guo ·  Max Hamilton ·  Xixi Hu ·  Jing Zhu Yuexi Du ·  Chenyang Ma ·  Jiacheng Zhang ·  Kshama Nitin Shah Fengyu Yang (CRA Outstanding Undergraduate Award - Runner-up) ·  Zhaoying Pan Chenhao Zheng (CRA Outstanding Undergraduate Award - Honorable Mention) ·  Yi Liu Resources I co-organized the Sight and Sound workshop at CVPR [2018, 2019, 2020, 2021, 2022, 2023, 2024] Slides covering some of my audio-visual learning work (through 2020) (key, ppt, pdf). This includes slides on learning from impact sounds, self-supervised feature learning, sound source localization/separation, audio-to-video gesture synthesis, audio-visual objects, and active speaker detection. Teaching Current: CS 5670: Introduction to Computer Vision [F25] Earlier teaching at UMich: EECS 442: Computer Vision [F24, F23, F22, F21, F20] EECS 542: Advanced Topics in Computer Vision [W22, W24] EECS 598-012: Unsupervised Visual Learning [W21] EECS 504: Foundations of Computer Vision [W20, F22] Publications GPS as a Control Signal for Image Generation Chao Feng, Ziyang Chen, Aleksander Holynski, Alexei A. Efros, Andrew Owens CVPR 2025 project page · paper · bibtex @article{feng2025gps, title={GPS as a Control Signal for Image Generation}, author={Feng, Chao and Chen, Ziyang and Holynski, Aleksander and Efros, Alexei A and Owens, Andrew}, journal={Computer Vision and Pattern Recognition (CVPR)}, year={2025}, } We generate images conditioned on GPS. Video-Guided Foley Sound Generation with Multimodal Controls Ziyang Chen, Prem Seetharaman, Bryan Russell, Oriol Nieto, David Bourgin, Andrew Owens, Justin Salamon CVPR 2025 project page · paper · bibtex @article{chen2025videoguided, title={Video-Guided Foley Sound Generation with Multimodal Controls}, author={Chen, Ziyang and Seetharaman, Prem and Russell, Bryan and Nieto, Oriol and Bourgin, David and Owens, Andrew and Salamon, Justin}, journal={Computer Vision and Pattern Recognition (CVPR)}, year={2025}, } Generate synchronized audio for silent videos using text or audio conditioning. Community Forensics: Using Thousands of Generators to Train Fake Image Detectors Jeongsoo Park, Andrew Owens CVPR 2025 project page · paper · bibtex @article{park2025community, title={Community Forensics: Using Thousands of Generators to Train Fake Image Detectors}, author={Park, Jeongsoo and Owens, Andrew}, journal={Computer Vision and Pattern Recognition (CVPR)}, year={2025}, } A dataset containing images sampled from thousands of text-to-image diffusion models. We find that increasing diversity of the training data improves generalization. Motion Prompting: Controlling Video Generation with Motion Trajectories Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Chen Sun, Oliver Wang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Andrew Owens, Deqing Sun CVPR 2025 project page · paper · bibtex @article{geng2025motion, title={Motion Prompting: Controlling Video Generation with Motion Trajectories}, author={Geng, Daniel and Herrmann, Charles and Hur, Junhwa and Cole, Forrester and Sun, Chen and Wang, Oliver and Pfaff, Tobias and Lopez-Guevara, Tatiana and Doersch, Carl and Aytar, Yusuf and Rubinstein, Michael and Owens, Andrew and Sun, Deqing}, journal={Computer Vision and Pattern Recognition (CVPR)}, year={2025}, } Control video generators using point tracks. Touch2Touch: Cross-Modal Tactile Generation for Object Manipulation Samanta Rodriguez*, Yiming Dou*, Miquel Oller, Andrew Owens, Nima Fazeli arXiv 2024 project page · paper · bibtex @article{rodriguez2024touch2touch, title={Touch2Touch: Cross-Modal Tactile Generation for Object Manipulation}, author={Rodriguez, Samanta and Dou, Yiming and Oller, Miquel and Owens, Andrew and Fazeli, Nima}, journal={arXiv}, year={2024}, } We learn to translate touch signals acquired from one vision-based touch sensor to another. This allows us to transfer object manipulation algorithms between sensors. Images that Sound: Composing Images and Sounds on a Single Canvas Ziyang Chen, Daniel Geng, Andrew Owens NeurIPS 2024 project page · paper · bibtex @article{chen2024images, title={Images that Sound: Composing Images and Sounds on a Single Canvas}, author={Chen, Ziyang and Geng, Daniel and Owens, Andrew}, journal={Neural Information Processing Systems (NeurIPS)}, year={2024}, } We generate spectrograms that look like natural images by composing together the score functions of audio and visual diffusion networks. Factorized Diffusion: Perceptual Illusions by Noise Decomposition Daniel Geng*, Inbum Park*, Andrew Owens ECCV 2024 project page · paper · bibtex @article{geng2024factorized, title={Factorized Diffusion: Perceptual Illusions by Noise Decomposition}, author={Geng, Daniel and Park, Inbum and Owens, Andrew}, journal={European Conference on Computer Vision (ECCV)}, year={2024}, } Make hybrid images (and other illusions) by linearly filtering the noise during diffusion generation. Self-Supervised Any-Point Tracking by Contrastive Random Walks Ayush Shrivastava, Andrew Owens ECCV 2024 project page · paper · bibtex @article{shrivastava2024selfsupervised, title={Self-Supervised Any-Point Tracking by Contrastive Random Walks}, author={Shrivastava, Ayush and Owens, Andrew}, journal={European Conference on Computer Vision (ECCV)}, year={2024}, } Track a given point in a video using a simple, self-supervised method based on contrastive random walks. Self-Supervised Audio-Visual Soundscape Stylization Tingle Li, Renhao Wang, Po-Yao Huang, Andrew Owens, Gopala Krishna Anumanchipalli ECCV 2024 project page · paper · bibtex @article{li2024selfsupervised, title={Self-Supervised Audio-Visual Soundscape Stylization}, author={Li, Tingle and Wang, Renhao and Huang, Po-Yao and Owens, Andrew and Anumanchipalli, Gopala Krishna}, journal={European Conference on Computer Vision (ECCV)}, year={2024}, } Restyle a sound to fit with another scene, using an audio-visual conditional example taken from that scene. Tactile-Augmented Radiance Fields Yiming Dou, Fengyu Yang, Yi Liu, Antonio Loquercio, Andrew Owens CVPR 2024 project page · paper · code · bibtex @article{dou2024tactileaugmented, title={Tactile-Augmented Radiance Fields}, author={Dou, Yiming and Yang, Fengyu and Liu, Yi and Loquercio, Antonio and Owens, Andrew}, journal={Computer Vision and Pattern Recognition (CVPR)}, year={2024}, } We capture visual-tactile representations of real-world 3D scenes. This representation can estimate the tactile signals for a given 3D position within the scene. Visual Anagrams: Generating Multi-View Optical Illusions with Diffusion Models Daniel Geng, Inbum Park, Andrew Owens CVPR 2024 (Oral) project page · paper · bibtex @article{geng2024visual, title={Visual Anagrams: Generating Multi-View Optical Illusions with Diffusion Models}, author={Geng, Daniel and Park, Inbum and Owens, Andrew}, journal={Computer Vision and Pattern Recognition (CVPR)}, year={2024}, } We generate multi-view optical illusions: images that change their appearance under a transformation, such as a flip or a rotation. Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark Ziyang Chen, Israel D. Gebru, Christian Richardt, Anurag Kumar, William Laney, Andrew Owens, Alexander Richard CVPR 2024 (Highlight) project page · paper · bibtex @article{chen2024real, title={Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark}, author={Chen, Ziyang and Gebru, Israel D and Richardt, Christian and Kumar, Anurag and Laney, William and Owens, Andrew and Richard, Alexander}, journal={Computer Vision and Pattern Recognition (CVPR)}, year={2024}, } A benchmark for real-world audio-visual room acoustics, containing NeRFs with densely sampled audio recordings. Binding Touch to Everything: Learning Unified Multimodal Tactile Representations Fengyu Yang*, Chao Feng*, Ziyang Chen*, Hyoungseob Park, Daniel Wang, Yiming Dou, Ziyao Zeng, Xien Chen, Rit Gangop",
  "content_length": 32623,
  "method": "requests",
  "crawl_time": "2025-12-01 12:57:41"
}