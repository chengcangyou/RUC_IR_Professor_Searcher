{
  "name": "Yingzhen Li",
  "homepage": "http://yingzhenli.net/home/en/?page_id=345",
  "status": "success",
  "content": "Publications Working papers Wuhao Chen, Zijing Ou and Yingzhen Li. Neural Flow Samplers with Shortcut Models. In submission. 2025. Harrison Zhu, Adam Howes, Owen van Eer, Maxime Richard, Yingzhen Li, Dino Sejdinovic and Seth Flaxman. Aggregated Gaussian Processes with Multiresolution Earth Observation Covariates. 2022. Wenbo Gong and Yingzhen Li. Interpreting Diffusion Score Matching using Normalizing Flow. ICML 2021 INNF+ workshop. Refereed conference papers Zijing Ou, Ruixiang Zhang and Yingzhen Li. Discrete Neural Flow Samplers with Locally Equivariant Transformer. Neural Information Processing Systems (NeurIPS), 2025. I. Shavindra Jayasekera, Jacob Si, Filippo Valdettaro, Wenlong Chen, Aldo A. Faisal and Yingzhen Li. Variational Uncertainty Decomposition for In-Context Learning. Neural Information Processing Systems (NeurIPS), 2025. Yohan Jung, Hyungi Lee, Wenlong Chen, Thomas Möllenhoff, Yingzhen Li, Juho Lee, Mohammad Emtiyaz Khan. Compact Memory for Continual Logistic Regression. Neural Information Processing Systems (NeurIPS), 2025. Naoki Kiyohara, Edward Johns and Yingzhen Li. Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions. Neural Information Processing Systems (NeurIPS), 2025. Wenlong Chen*, Naoki Kiyohara*, Harrison Bo Hua Zhu*, Jacob Curran-Sebastian, Samir Bhatt and Yingzhen Li. Recurrent Memory for Online Interdomain Gaussian Processes. Neural Information Processing Systems (NeurIPS), 2025. Xinzhe Luo, Yingzhen Li and Chen Qin. Unsupervised Accelerated MRI Reconstruction via Ground-Truth-Free Flow Matching. Information Processing in Medical Imaging (IPMI), 2025. (oral) Manduchi et al. On the Challenges and Opportunities in Generative AI. Transactions on Machine Learning Research (TMLR), 2025. Carles Balsells Rodas, Xavier Sumba, Tanmayee Narendra, Ruibo Tu, Gabriele Schweikert, Hedvig Kjellstrom and Yingzhen Li. Causal Discovery from Conditionally Stationary Time Series. International Conference on Machine Learning (ICML), 2025. Wenlong Chen, Bolian Li, Ruqi Zhang and Yingzhen Li. Bayesian Computation in Deep Learning. Handbook of Markov Chain Monte Carlo, 2nd Edition (in press, expected 2025/26, refereed chapter) Zijing Ou, Mingtian Zhang, Andi Zhang, Tim Z. Xiao, Yingzhen Li and David Barber. Improving Probabilistic Diffusion Models With Optimal Diagonal Covariance Matching. International Conference on Learning Representations (ICLR), 2025. (oral, 1.5%) Tobias Schröder, Zijing Ou, Yingzhen Li and Andrew Duncan. Energy-Based Modelling for Discrete and Mixed Data via Heat Equations on Structured Spaces. Neural Information Processing Systems (NeurIPS), 2024. Carles Balsells-Rodas, Yixin Wang and Yingzhen Li. On the Identifiability of Switching Dynamical Systems. International Conference on Machine Learning (ICML), 2024. Papamakou et al. Position: Bayesian Deep Learning is Needed in the Age of Large-Scale AI. International Conference on Machine Learning (ICML), 2024. Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark A. Hasegawa-Johnson, Yingzhen Li and Chang D. Yoo. C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion. International Conference on Learning Representations (ICLR), 2024. Tobias Schröder, Zijing Ou, Jen Ning Lim, Yingzhen Li, Sebastian Vollmer and Andrew Duncan. Energy Discrepancies: A Score-Independent Loss for Energy-Based Models. Neural Information Processing Systems (NeurIPS), 2023. Harrison Zhu, Carles Balsells Rodas and Yingzhen Li. Markovian Gaussian Process Variational Autoencoders. International Conference on Machine Learning (ICML), 2023. Wenlong Chen and Yingzhen Li. Calibrating Transformers via Sparse Gaussian Processes. International Conference on Learning Representations (ICLR), 2023. Hee Suk Yoon, Joshua Tian Jin Tee, Gwangsu Kim, Eunseop Yoon, Sunjae Yoon, Yingzhen Li and Chang D. Yoo. ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration Measure. International Conference on Learning Representations (ICLR), 2023. Ryutaro Tanno, Melanie F. Pradier, Aditya Nori and Yingzhen Li. Repairing Neural Networks by Leaving the Right Past Behind. Neural Processing Information Systems (NeurIPS), 2022. Yanzhi Chen, Weihao Sun, Yingzhen Li and Adrian Weller. Scalable Infomin Learning. Neural Processing Information Systems (NeurIPS), 2022. Zijing Ou, Tingyang Xu, Qinliang Su, Yingzhen Li, Peilin Zhao and Yatao Bian. Learning Set Functions Under the Optimal Subset Oracle via Equivariant Variational Inference. Neural Processing Information Systems (NeurIPS), 2022. Hippolyt Ritter, Martin Kukla, Cheng Zhang and Yingzhen Li. Sparse Uncertainty Representation in Deep Learning with Inducing Weights. Neural Processing Information Systems (NeurIPS), 2021. Thomas Henn, Yasukazu Sakamoto, Clément Jacquet, Shunsuke Yoshizawa, Masamichi Andou, Stephen Tchen, Ryosuke Saga, Hiroyuki Ishihara, Katsuhiko Shimizu, Yingzhen Li and Ryutaro Tanno. A Principled Approach to Failure Analysis and Model Repairment: Demonstration in Medical Imaging. International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2021. Wenbo Gong, Kaibo Zhang, Yingzhen Li and José Miguel Hernández-Lobato. Active Slices for Slided Stein Discrepancy. International Conference on Machine Learning (ICML), 2021 Wenbo Gong, Yingzhen Li and José Miguel Hernández-Lobato. Sliced Kernelized Stein Discrepancy. International Conference on Learning Representations (ICLR), 2021. Ruqi Zhang, Yingzhen Li, Chris De Sa, Sam Devlin and Cheng Zhang. Meta-Learning for Variational Inference. International Conference on Artificial Intelligence and Statistics (AISTATS), 2021. Yi Zhu, Ehsan Shareghi, Yingzhen Li, Roi Reichart and Anna Korhonen. Combining Deep Generative Models and Multi-lingual Pretraining for Semi-supervised Document Classification. European Chapter of the Association for Computational Linguistics (EACL), 2021. Andrew Y. K. Foong*, David R. Burt*, Yingzhen Li and Richard E. Turner. On the Expressiveness of Approximate Inference in Bayesian Neural Networks. Neural Processing Information Systems (NeurIPS), 2020. Cheng Zhang, Kun Zhang and Yingzhen Li. A Causal View on Robustness of Neural Networks. Neural Processing Information Systems (NeurIPS), 2020. Maximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek, Cheng Zhang, Sam Devlin and Katja Hofmann. Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck. Neural Processing Information Systems (NeurIPS), 2019. Ehsan Shareghi, Yingzhen Li, Yi Zhu, Roi Reichart and Anna Korhonen. Bayesian Learning for Neural Dependency Parsing. NAACL-HLT 2019. Chao Ma, Yingzhen Li and José Miguel Hernández-Lobato. Variational Implicit Processes. International Conference on Machine Learning (ICML), 2019. code Yingzhen Li, John Bradshaw and Yash Sharma. Are Generative Classifiers More Robust to Adversarial Attacks? International Conference on Machine Learning (ICML), 2019. code cartoon (Generative/Discriminative classifiers, the graphical model defines its nature) (Fusing generative/discriminative behaviours: combining VGG-features and generative models) Adversarial machine learning research has become an arm race, so far the adversary side is taking the lead. We conjecture in this paper that feed-forward DNN's vulnerability to adversarial examples is due to its discriminative nature, so instead we build deep generative models and turn them into classifiers using Bayes' rule. Extensive empirical study shows advantages of generative classifiers over discriminative ones, both in terms of robustness and detection to adversarial examples. We also build a generative-discriminative hybrid for natural image classification, which combines the best from both worlds. Wenbo Gong*, Yingzhen Li* and José Miguel Hernández-Lobato. Meta-Learning for Stochastic Gradient MCMC. International Conference on Learning Representations (ICLR), 2019.  code cartoon (finding the best one from the largest set of provably correct samplers ?) (the meta-learned sampler is trained on an isotropic Gaussian distribution) Recently people are very excited about meta-learning algorithms that are then used to optimise ML models. However naively proposing an arbitrary neural network to do sampling is almost guaranteed to fail. So we really want to meta-learn an MCMC sampler that is provably correct (i.e. the stationary distribution is the target distribution), and we build on top of a seminal work on SG-MCMC to combine the flexibility of neural networks and Hamiltonian Monte Carlo. Yingzhen Li and Stephan Mandt. Disentangled Sequential Autoencoder. International Conference on Machine Learning (ICML), 2018. sprites data architecture cartoon (embarrassingly simple ?) While there are quite a few recent papers talking about learning disentangled representation using some new loss function, I thought: why not just go back to the graphical model land, and directly enforce disentanglement in model design? So there you go a minimalistic generative model for disentangling content and dynamics in a sequence. We have speech recognition example in the paper as well, and we also show using stochastic dynamics could potentially achieve better compression. Part of this project was done as an intern at Disney research. (Previously titled \"A Deep Generative Model for Disentangled Representations of Sequential Data\") Cuong V. Nguyen, Yingzhen Li, Thang D. Bui and Richard E. Turner. Variational Continual Learning. International Conference on Learning Representations (ICLR), 2018.  code cartoon (Well, quite a bit exaggeration actually, we need to make it practical ?) \"Bayesian Inference is **the natural choice** for online/incremental/continual learning.\" Conceptually this is nice, but you know deep learning engineers wouldn't bid for it as exact Bayesian inference takes \"forever\" to run. In this paper we just use variational Bayes to make it tractable and easy to implement for continua",
  "content_length": 16967,
  "method": "requests",
  "crawl_time": "2025-12-01 14:51:42"
}