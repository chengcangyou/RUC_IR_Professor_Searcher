{
  "name": "Guang Chen 0001",
  "homepage": "https://www.embodiment.ai",
  "status": "success",
  "content": "Generalist Embodied AI Lab Lab | People | Research | Publications | Datasets | Positions | Teaching | Contact Chen Guang is a Tenured Professor at the School of Computer Science and Technology, Tongji University, a Full-Time Professor at the Shanghai Innovation Institute, and the Director of the Generalist Embodied AI Lab (GEAI Lab). The lab focuses on research in embodied perception, navigation, collaboration, and decision-making. As an inter-university and inter-institutional laboratory, GEAI Lab's members are from Tongji University, the Shanghai Innovation Institute, and Technical University of Munich. We are seeking highly motivated and self-driven students, research assistants, postdoctoral fellows, and faculty members. If you are interested in collaborating with us, please feel free to send us an email. å®éªå®¤é¿æææ¶æå ·èªé©±åçä½å¹´çº§æ¬ç§çåä¸å°ç§ç é¡¹ç®ä¸­ï¼æ¯å¹´å¼æ¾ç´åãæ¨å ç ç©¶åé¢ï¼æ¬¢è¿ç³è¯·åå£«åãæèï¼å©çææãå¯ææãæµ·ä¼ï¼çç§ç å·¥ä½è èç³»åä½ï¼ Email: guangchen(at)tongji.edu.cn or guang(at)in.tum.de Address: School of Computer Science and Technology, Tongji University, NO.4800, Cao'an Road, Jiading District, Shanghai News 2025.11: Our work is accepted at Nature Computational Science!!!! Kudos to Zhongzhan! 2025.10: Four papers are accepted at NeurIPS 2025! Kudos to Boyang, Ao Zhou, and Weiyi! 2025.10: Two papers are accepted at IJCV! Kudos to Boyang and Dr. Sanqing Qu! 2025.10: One paper is accepted at TPAMI! Kudos to Dr. Sanqing Qu! 2025.03: Two papers are accepted at CVPR 2025 & IJCV 2025! Kudos to Shihang and Sanqing! 2024.09: Two papers are accepted at NeurIPS 2024! Kudos to Weiyi and Tianhang! 2024.09: Our survey paper \"A Review of Safe Reinforcement Learning: Methods, Theory and Applications\" is accepted by IEEE T-PAMI! Kudos to my Shangding! 2024.07: Nine papers are accepted(2@ECCV 2024, 1@IJCAI2024, 1@IEEE T-II, 1@IEEE T-ITS, 1@IEEE T-ASE, 3@IROS2024)! Kudos to my students! 2024.02: Four papers are accepted at CVPR 2024! Kudos to my students! Pre-Prints A Review of Safe Reinforcement Learning: Methods, Theory and Applications Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, Yaodong Yang, Alois Knoll IEEE TPAMIï¼Acceptedï¼, 2024 code / video / arXiv / bibtex @misc{gu2022, title={A Review of Safe Reinforcement Learning: Methods, Theory and Applications}, author={Shangding Gu and Long Yang and Yali Du and Guang Chen and Florian Walter and Jun Wang and Yaodong Yang and Alois Knoll}, year={2022}, eprint={2205.10330}, archivePrefix={arXiv}, primaryClass={cs.AI} } Peer-Reviewed Paper (Selected, 2020-) Divide and Conquer: Exploring Language-centric Tree Reasoning for Video Question-Answering Zhaohe Liao, Jiangtong Li, Siyu Sun, Qingyang Liu, Fengshun Xiao, Tianjiao Li, Qiang Zhang, Guang Chen, Li Niu, Changjun Jiang, Liqing Zhang International Conference on Machine Learning (ICML), 2025 PDF / code / bibtex @inproceedings{du2025rcp, title={Divide and Conquer: Exploring Language-centric Tree Reasoning for Video Question-Answering}, author={Zhaohe Liao, Jiangtong Li, Siyu Sun, Qingyang Liu, Fengshun Xiao, Tianjiao Li, Qiang Zhang, Guang Chen, Li Niu, Changjun Jiang, Liqing Zhang}, booktitle={International Conference on Machine Learning}, year={2025} } In this work, we propose a novel two-stage Languagecentric Tree Reasoning (LTR) framework that enhances the reasoning capabilities and transparency of MLLMs. Experiments across 11 VideoQA benchmarks demonstrate that our LTR framework significantly improves both accuracy and interpretability compared to state-of-the-art MLLMs. To our knowledge, this is the first work to implement a language-centric logical tree to guide MLLM reasoning in VideoQA, paving the way for language-centric video understanding from perception to cognition. RCP-Bench: Benchmarking Robustness for Collaborative Perception Under Diverse Corruptions Shihang Du, Sanqing Qu, Tianhang Wang, Xudong Zhang, Yunwei Zhu, Jian Mao, Fan Lu, Qiao Lin, Guang Chen* IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025 PDF / code / bibtex @inproceedings{du2025rcp, title={RCP-Bench: Benchmarking Robustness for Collaborative Perception Under Diverse Corruptions}, author={Shihang Du, Sanqing Qu, Tianhang Wang, Xudong Zhang, Yunwei Zhu, Jian Mao, Fan Lu, Qiao Lin, Guang Chen}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, year={2025} } In this paper, we introduce RCP-Bench, a comprehensive benchmark for evaluating the robustness of collaborative detection models against real-world corruptions. It includes three new datasets simulating various collaborative scenarios and camera corruptions. Experiments reveal that leading models are significantly affected by these corruptions. To address this, we propose two strategies (RCP-Drop and RCP-Mix) to enhance robustness and identify key factors influencing model performance. Our aim is to advance research on more robust collaborative perception models. General Class-Balanced Multicentric Dynamic Prototype Pseudo-Labeling for Source-Free Domain Adaptation Sanqing Qu, Guang Chen*, Jing Zhang, Zhijun Li, Wei He, Dacheng Tao International Journal of Computer Vision (IJCV), 2025 PDF / code / project / bibtex @article{qu2025general, title={General Class-Balanced Multicentric Dynamic Prototype Pseudo-Labeling for Source-Free Domain Adaptation}, author={Qu, Sanqing and Chen, Guang and Zhang, Jing and Li, Zhijun and He, Wei and Tao, Dacheng}, journal={International Journal of Computer Vision}, pages={1--22}, year={2025}, publisher={Springer} } We promote the vanilla BMD to BMD-v2 by incorporating a consistency-guided reweighting strategy to improve inter-class balanced sampling, and leveraging the silhouettes metric to realize adaptive intra-class multicentric clustering. Extensive experiments conducted on both 2D images and 3D point cloud recognition demonstrate that our proposed BMD strategy significantly improves existing representative methods. LEAD: Learning Decomposition for Source-free Universal Domain Adaptation Sanqing Qu, Tianpei Zou, Lianghua He, Florian RÃ¶hrbein, Alois Knoll, Guang Chen*, Changjun Jiang IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024 arXiv / code / project / bibtex @inproceedings{qu2024lead, title={Lead: Learning decomposition for source-free universal domain adaptation}, author={Qu, Sanqing and Zou, Tianpei and He, Lianghua and R{\\\"o}hrbein, Florian and Knoll, Alois and Chen, Guang and Jiang, Changjun}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={23334--23343}, year={2024} } In this paper, we propose a new idea of LEArning Decomposition (LEAD) for universal domain adaptation, which decouples features into source-known and -unknown components to identify target-private data. This solution leads to elegant views for identifying target-private unknown data without tedious tuning thresholds or relying on iterative clustering. RCDN: Towards Robust Camera-Insensitivity Collaborative Perception via Dynamic Feature-based 3D Neural Modeling Tianhang Wang, Fan Lu, Zehan Zheng, Zhijun Li, Guang Chen*, Changjun Jiang Advances in Neural Information Processing Systems (NeurlPS), 2024 arXiv / code / bibtex @inproceedings{wang2024rcdn, title={RCDN: Towards Robust Camera-Insensitivity Collaborative Perception via Dynamic Feature-based 3D Neural Modeling}, author={Tianhang Wang, Fan Lu, Zehan Zheng, Zhijun Li, Guang Chen, Changjun Jiang}, booktitle={Advances in Neural Information Processing Systems (NeurlPS)}, year={2024} } We propose RCDN, a Robust Camera-insensitivity collaborative perception with a novel Dynamic feature-based 3D Neural modeling mechanism. The key intuition of RCDN is to construct collaborative neural rendering field representations to recover failed perceptual messages sent by multiple agents. To better model collaborative neural rendering field, RCDN first establishes a geometry BEV feature based time-invariant static field with other agents via fast hash grid modeling. Based on the static background field, the proposed time-varying dynamic field can model corresponding motion vectors for foregrounds with appropriate positions. GeoNLF: Geometry guided Pose-Free Neural LiDAR FieldsWeiyi Xue, Zehan Zheng, Fan Lu, Haiyun Wei, Guang Chen, Changjun Jiang Advances in Neural Information Processing Systems (NeurlPS), 2024 arXiv / code / bibtex @article{xue2024geonlf, title={GeoNLF: Geometry guided Pose-Free Neural LiDAR Fields}, author={Xue, Weiyi and Zheng, Zehan and Lu, Fan and Wei, Haiyun and Chen, Guang and others}, journal={Advances in Neural Information Processing Systems}, volume={37}, pages={73672--73692}, year={2024} } We introduce GeoNLF for multi-view registration and novel view synthesis from a sequence of sampled point clouds. We demonstrate the challenges encountered by previous pairwise and multi-view registration methods, as well as the difficulties faced by previous pose-free methods. Through the utilization of our Geo-Optimizer, Graph-based Robust CD, selective-reweighting strategy, and geometric constraints from a 3D perspective, our outlier-aware and geometry-aware GeoNLF demonstrates promising performance in both multi-view registration and novel view synthesis (NVS) tasks. HGL: Hierarchical Geometry Learning for Test-time Adaptation in 3D Point Cloud Segmentation Tianpei Zou, Sanqing Qu, Zhijun Li, Alois Knoll, Lianghua He, Guang Chen*, Changjun Jiang European Conference on Computer Vision (ECCV), 2024 arXiv / code / bibtex @inproceedings{zou2024hgl, title={HGL: Hierarchical Geometry Learning for Test-time Adaptation in 3D Point Cloud Segmentation}, author={Zou, Tianpei and Qu, Sanqing and Li, Zhijun and Knoll, Alois and He, Lianghua and Chen, Guang and Jiang, Changjun}, booktitle={European Conference on Computer Vision (ECCV)}, year={2024} } we delve into TTA in 3D",
  "content_length": 47913,
  "method": "requests",
  "crawl_time": "2025-12-01 13:16:05"
}