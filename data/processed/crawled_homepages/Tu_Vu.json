{
  "name": "Tu Vu",
  "homepage": "https://tuvllms.github.io",
  "status": "success",
  "content": "Tu Vu I am an Assistant Professor at Virginia Tech (VT) and a Faculty Researcher at Google. At VT, I am also affiliated with the Sanghani Center for Artificial Intelligence & Data Analytics. Prior to joining VT, I held the position of Research Scientist at Google DeepMind for a year after receiving my PhD in Computer Science from the University of Massachusetts Amherst, advised by Mohit Iyyer. üîç My research aims to develop effective and efficient methods for advancing and democratizing artificial intelligence in the era of large language models (LLMs). Specific areas of focus include: Deep thinking: Developing models that explore diverse reasoning paths and synthesize novel, creative, and high-quality solutions to complex problems Agentic memory and context engineering: Developing mechanisms that allow agents to store, retrieve, and use information efficiently over long contexts Efficient transfer and adaptation: Reusing learned knowledge across tasks, languages, modalities, or models to adapt effectively in new or low-resource settings with minimal computational cost, data, and storage Efficient model updating: Developing methods that keep models up-to-date and responsive to new or evolving information while reasoning effectively over conflicting or manipulated retrieved inputs ‚≠ê For prospective PhD students I plan to recruit one new PhD student every year. If you are interested in joining my group, please apply to the VT Graduate School and list me as a potential advisor. Please also check out the application deadlines and information for prospective students. Due to the high volume of emails I receive, I may not be able to respond to each one individually; please don't be discouraged ‚Äî I may still review your application. ‚≠ê For Undergraduate and Masters students at VT I am happy to collaborate on research with current VT students who have at least one full academic year until graduation. If you are interested, feel free to email me. I will follow up if there is a good fit. Recent news Oct. 2025 Lightning talk at the Amazon-Virginia Tech AI Workshop Oct. 2025 Received the Amazon - VT faculty research award Oct. 2025 Received research gift awards from Google DeepMind and Google Research Aug. 2025 Gave two invited lectures at The New Turing Institute‚Äôs GStar program Aug. 2025 One paper to appear at EMNLP 2025 on efficient model development! Aug. 2025 Featured invited speaker at the Open AGI Symposium at UC Berkeley, hosted by Sentient Jun. 2025 One paper to appear at TMLR 2025 on model merging at scale! Jun. 2025 Invited guest lecture at The New Turing Institute Jun. 2025 New preprint on a challenge benchmark for LLM reasoning over conflicting evidence Apr. 2025 Received the New Faculty Mentoring Grant from VT Mar. 2025 New preprint on fine-tuning transfer for efficient model development Nov. 2024 Received a research gift award from Adobe Nov. 2024 ‚úàÔ∏è Attended EMNLP 2024 in Miami, Florida üå¥ Nov. 2024 Invited talk at Qualcomm Seminar Series Oct. 2024 Invited talk at Mila / McGill NLP seminar Oct. 2024 New preprint on model merging at scale Sep. 2024 One paper to appear at EMNLP 2024 on foundational autoraters (FLAMe)! Aug. 2024 Started my professorship at Virginia Tech Jul. 2024 New preprint on Foundational Autoraters (FLAMe) May. 2024 FreshLLMs got accepted to ACL 2024 Findings! Feb. 2024 I am now serving as an Area Chair for ACL Rolling Review (ARR) Jan. 2024 Flan-MoE got accepted to ICLR 2024! Nov. 2023 Invited talk at Graph Neural Networks Reading Group, Google Oct. 2023 New preprint on LLM factuality (FreshLLMs) Aug. 2023 Joined Google DeepMind in Mountain View, CA as a Research Scientist Jul. 2023 Successfully defended my PhD thesis! Teaching CS 4804: Introduction to AI (Fall 2025) CS 5624: Natural Language Processing (Spring 2025) Advisees Group: Rituraj Sharma (1st year MS student) Noah Provenzano (2nd year MS student) Weiyuan Chen (1st year PhD student) Rishab Balasubramanian (2nd year PhD student) Thinh Pham (2nd year PhD student) Yu-Min Tseng (1st year PhD student) Quyet Do (2nd year PhD student) Pin-Jie Lin (2nd year PhD student) Jing Chen (1st year PhD student) Nguyen Nguyen (Junior student) Others: Zhenting Qi (Student Researcher @ Google, Summer - Fall 2025 ‚Üí PhD student @ Harvard) Prateek Yadav (Research Intern @ Google DeepMind, Summer 2024 ‚Äî Spring 2025 ‚Üí Research scientist @ Meta Superintelligence Labs) Simeng Han (Student Researcher @ Google DeepMind, Summer 2024 ‚Äî Spring 2025 ‚Üí Postdoc @ Stanford) Salaheddin Alzubi (Masters student @ UMass Amherst, Fall 2022 ‚Äî Spring 2023 ‚Üí Research scientist @ Sentient Labs) Dheeraj Mekala (PhD student @ UCSD, Spring ‚Äî Summer 2022 ‚Üí Research scientist @ Meta Superintelligence Labs) Preprints Preprint SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models Thinh Pham,¬†Nguyen Nguyen,¬†Pratibha Zunjare,¬†Weiyuan Chen,¬†Yu-Min Tseng,¬†and¬†Tu Vu In arXiv preprint arXiv:2506.01062, 2025 Abs arXiv Bib PDF We introduce SealQA, a new challenge benchmark for evaluating SEarch-Augmented Language models on fact-seeking questions where web search yields conflicting, noisy, or unhelpful results. SealQA comes in three flavors: (1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and reasoning capabilities, with Seal-0 focusing on the most challenging questions where chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3) LongSeal, which extends SealQA to test long-context, multi-document reasoning in \"needle-in-a-haystack\" settings. Our evaluation reveals critical limitations in current models: Even frontier LLMs perform poorly across all SealQA flavors. On Seal-0, frontier agentic models equipped with tools like o3 and o4-mini achieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning efforts. We find that advanced reasoning models such as DeepSeek-R1-671B and o3-mini are highly vulnerable to noisy search results. Notably, increasing test-time compute does not yield reliable gains across o3-mini, o4-mini, and o3, with performance often plateauing or even declining early. Additionally, while recent models are less affected by the \"lost-in-the-middle\" issue, they still fail to reliably identify relevant documents in LongSeal when faced with numerous distractors. To facilitate future work, we release SealQA at http://huggingface.co/datasets/vtllms/sealqa. @inproceedings{pham-etal-2025-sealqa, title = {SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models}, author = {Pham, Thinh and Nguyen, Nguyen and Zunjare, Pratibha and Chen, Weiyuan and Tseng, Yu-Min and Vu, Tu}, booktitle = {arXiv preprint arXiv:2506.01062}, year = {2025}, pdf = {https://arxiv.org/pdf/2506.01062}, preprint = {true}, } // Our benchmark dataset has been used by Google‚Äôs Gemini, DeepSeek, and Kimi Selected publications For an up-to-date list of my research papers, please see my Google Scholar profile. * denotes equal contribution. EMNLP Efficient Model Development through Fine-tuning Transfer Pin-Jie Lin,¬†Rishab Balasubramanian,¬†Fengyuan Liu,¬†Nikhil Kandpal,¬†and¬†Tu Vu In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, 2025 Abs arXiv Bib PDF Modern LLMs struggle with efficient updates, as each new pretrained model version requires repeating expensive alignment processes. This challenge also applies to domain- or language-specific models, where fine-tuning on specialized data must be redone for every new base model release. In this paper, we explore the transfer of fine-tuning updates between model versions. Specifically, we derive the diff vector from one source model version, which represents the weight changes from fine-tuning, and apply it to the base model of a different target version. Through empirical evaluations on various open-weight model versions, we show that transferring diff vectors can significantly improve the target base model, often achieving performance comparable to its fine-tuned counterpart. For example, reusing the fine-tuning updates from Llama 3.0 8B leads to an absolute accuracy improvement of 10.7% on GPQA over the base Llama 3.1 8B without additional training, surpassing Llama 3.1 8B Instruct. In a multilingual model development setting, we show that this approach can significantly increase performance on target-language tasks without retraining, achieving an absolute improvement of 4.7% and 15.5% on Global MMLU for Malagasy and Turkish, respectively, compared to Llama 3.1 8B Instruct. Our controlled experiments reveal that fine-tuning transfer is most effective when the source and target models are linearly connected in the parameter space. Additionally, we demonstrate that fine-tuning transfer offers a stronger and more computationally efficient starting point for further fine-tuning. Finally, we propose an iterative recycling-then-finetuning approach for continuous model development, which improves both efficiency and effectiveness. Our findings suggest that fine-tuning transfer is a viable strategy to reduce training costs while maintaining model performance. @inproceedings{lin-etal-2025-efficient, title = {Efficient Model Development through Fine-tuning Transfer}, author = {Lin, Pin-Jie and Balasubramanian, Rishab and Liu, Fengyuan and Kandpal, Nikhil and Vu, Tu}, booktitle = {Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing}, year = {2025}, pdf = {https://arxiv.org/pdf/2503.20110}, } Oral presentation, rating 4.5/5 by area chairs EMNLP Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation Tu Vu*,¬†Kalpesh Krishna*,¬†Salaheddin Alzubi,¬†Chris Tar,¬†Manaal Faruqui,¬†and¬†Yun-Hsuan Sung In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024 Abs arXiv Bib PDF As large language models (LLMs) advance, it becomes more challenging to reliably evaluate their output due to the high costs of human evaluation. To make progress towards better LLM autoraters, we",
  "content_length": 31641,
  "method": "requests",
  "crawl_time": "2025-12-01 14:40:59"
}