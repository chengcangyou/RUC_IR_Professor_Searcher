{
  "name": "Daniel Neider",
  "homepage": "https://www.rc-trust.ai/neider",
  "status": "success",
  "content": "Daniel Neider - Homepage Center for Trustworthy Data Science and Security   |   TU Dortmund University I am the professor for Verification and Formal Guarantees of Machine Learning at the Computer Science department of TU Dortmund University, a member of the Center for Trustworthy Data Science and Security at the University Alliance Ruhr, and principal investigator in several research projects. My group's research seeks to develop practical tools to make artificial intelligence safe, reliable, and trustworthy. To this end, we develop formal methods for artificial intelligence and collaborate with researchers from various disciplines, including IT-Security, Statistics, and Psychology. You can also find me on DBLP, Google Scholar, ORCID (ID 0000-0001-9276-6342), Scopus (ID 26664925200), and the Web of Science (ResearcherID ADV-0056-2022). Please see my ORCID entry for my academic CV. Information for Students My office hours are by appointment. My chair offers various courses in the Computer Science curriculum. Please see the section on teaching for details. Bachelor's and Master's Theses Available We offer Bachelor's and Master's theses on topics at the intersection of formal methods (e.g., verification, logic, constraint solving) and artificial intelligence (e.g., neural networks, reinforcement learning, automated reasoning). If you are an enthusiastic and highly qualified student, please get in touch with me for more details. Preferably, attach a CV and/or a recent transcript. Contact enable javascript cs.tu-dortmund.de Joseph-von-Fraunhofer-Str. 25 Room 212 44227 Dortmund Germany Office Ms Alhida Murati enable javascript tu-dortmund.de +49 231 7557729 (8 AM to 12 PM) Joseph-von-Fraunhofer-Str. 25 Room 218 44227 Dortmund Germany I am the professor for Verification and Formal Guarantees of Machine Learning at the Computer Science department of TU Dortmund University, a member of the Center for Trustworthy Data Science and Security at the University Alliance Ruhr, and principal investigator in several research projects. My group's research seeks to develop practical tools to make artificial intelligence safe, reliable, and trustworthy. To this end, we develop formal methods for artificial intelligence and collaborate with researchers from various disciplines, including IT-Security, Statistics, and Psychology. You can also find me on DBLP, Google Scholar, ORCID (ID 0000-0001-9276-6342), Scopus (ID 26664925200), and the Web of Science (ResearcherID ADV-0056-2022). Please see my ORCID entry for my academic CV. Information for Students My office hours are by appointment. My chair offers various courses in the Computer Science curriculum. Please see the section on teaching for details. Bachelor's and Master's Theses Available We offer Bachelor's and Master's theses on topics at the intersection of formal methods (e.g., verification, logic, constraint solving) and artificial intelligence (e.g., neural networks, reinforcement learning, automated reasoning). If you are an enthusiastic and highly qualified student, please get in touch with me for more details. Preferably, attach a CV and/or a recent transcript. Research My research interests broadly lie in the intersection of machine learning and formal methods. I am especially interested in combining inductive techniques from the area of machine learning and symbolic techniques from the area of logic. My goal is to build automated tools for the design, construction, and analysis of safe artificial intelligence. My chair is working on many topics, including: Verification of learning systems: automatically proving correctness properties (e.g., robustness) of learning systems. Explainability of artificial intelligence: generating provably correct explanations for the decision-making of intelligent systems. Intelligent formal methods: tools that learn correctness and termination proofs for a wide range of systems, including hardware, software, and cyber-physical systems. Learning-based synthesis: automatically generating reactive systems and program code from logical specifications and examples. Specification learning/recommendation: learning-based methods that assist humans in writing formal specifications. Incorporating automata learning and symbolic reasoning in reinforcement learning, inverse reinforcement learning, and transfer learning. Learning theory: developing principled ways to combine learning and symbolic reasoning. Our projects often extend into theoretical computer science, specifically automata theory, game theory, and logic. You can try some of the tools we have developed online: flie: The Formal Language Inference Engine Horn-ICE: A learning-based program verifier Tools As a byproduct of my research, I have developed and contributed to several tools: Alchemist-CS/DT Alchemist-CS/DT synthesizes piece-wise linear integer arithmetic expressions from SyGuS specifications using logical constraint solving and decision tree learning Evrostos: The rLTL Verifier Evrostos is an efficient, NuSMV-based model checker for rLTL properties. The sources are hosted on GitHub. ICE Verification Toolkit ICE is a fully automated program verifier, which extends Microsoft Boogie with various learning-based methods for automatic invariant generation. It won the invariant synthesis track of the Syntax-Guided Synthesis Competition in 2015 and 2016. Horn-ICE Verification Toolkit Horn-ICE is an extension of the ICE tool suite that supports the verification of recursive and concurrent programs. It is also able to synthesize solutions for Constrained Horn clauses. The sources are hosted on GitHub. libalf Libalf is a fast, open-source library for learning finite-state machines. It implements various popular active and passive learning algorithms for deterministic and nondeterministic automata as well as Moore Machines. The sources are available on GitHub. QUGA QUGA (short for Quality Guarantees for Autoencoders) is a tool for the deductive verification of deep autoencoders. The sources are hosted on GitHub. RESCOT RESCOT is a C++ tool (with a Matlab interface) to synthesize resilient controllers for (possibly perturbed) nonlinear control systems with respect to safety and reachability specifications. The sources are available on BitBucket. Traces2LTL Traces2LTL is a collection of algorithms for learning LTL formulas from labeled, infinite words (see the flie web demo). The sources are hosted on GitHub. Web Demos You can try some of our tools online: flie: The Formal Language Inference Engine Horn-ICE: A learning-based program verifier Artifacts Horn-ICE Learning for Synthesizing Invariants and Contracts Tools and benchmarks developed for the paper Horn-ICE Learning for Synthesizing Invariants and Contracts Invariant Synthesis for Incomplete Verification Engines Tools and benchmarks developed for the paper Invariant Synthesis for Incomplete Verification Engines Projects I am principal investigator in the following projects. LeaRNNify The DAAD-supported project LeaRNNify is at the interface of formal methods and artificial intelligence. It aims at bringing together two different kinds of algorithmic learning, namely grammatical inference and learning of neural networks. More precisely, we promote the use of recurrent neural networks (RNNs) in the process of verifying reactive systems, which until now has been reserved for grammatical inference. On the other hand, grammatical inference is finding its way into the field of classical machine learning. In fact, our second goal is to use automata-learning techniques to enhance the verification, explainability, and interpretability of machine learning algorithms and, in particular, RNNs. Also visit our project website. Temporal Logic Sketching The goal of the DFG-funded project Temporal Logic Sketching is to develop computer-aided methods to assist engineers in writing formal specifications. To this end, the project combines inductive and deductive techniques from machine learning, artificial intelligence, and logic. As a byproduct, the project also investigates explainable machine learning, specifically the learning of human-interpretable models. Verification of Anomaly Detectors In the context of the DFG Research Unit \"Deep Learning for Sparse Chemical Process Data\", the sub-project Verification of Anomaly Detectors is concerned with the quality assurance of deep neural networks for anomaly detection. To this end, the project develops new deductive and abstraction-based verification methods for neural networks over sequences, including recurrent neural networks and transformers. Recent Talks and Tutorials Safety Guarantees for Neural Networks. Keynote, 2nd KI-Wissen Open Project Day, January 2023, St. Augustin, Germany Robustness-by-Construction-Synthesis: Adapting to the Environment at Runtime. ISoLA, October 2022, Rhodes, Greece Neuro-Symbolic Verification of Deep neural Networks. IJCAI, July 2022, Vienna, Austria Safety and Explainability of Learning Systems. Invited talk, December 2021, Technical University of Dortmund, Germany (Horn-)ICE Learning: An Inductive Approach to Deductive Software Verification. Invited talk, June 2021, RWTH Aachen University, Germany Logic and Learning: Formal Guarantees for Trustworthy Intelligent Systems. Invited talk, November 2020, University of Oldenburg, Germany Learning Correctness Proofs of Software. Invited talk, January 2020, University of Liverpool, UK Combining Deductive and Inductive Reasoning in Formal Methods. Invited talk, October 2019, University of California, Los Angeles, USA Sorcar: Property-Driven Algorithms for Learning Conjunctive Invariants. SAS, October 2019, Porto, Portugal Deductive Verification, the Inductive Way. Invited tutorial at the ForMaL Spring School on Formal Methods and Machine Learning, June 2019, ENS Paris-Saclay, France Formal Verification Meets Machine Learning. Invited talk, March 2019, University of Bochum, Germany Learning Linear Temporal Properties. Invited talk, Co",
  "content_length": 29417,
  "method": "requests",
  "crawl_time": "2025-12-01 12:57:29"
}