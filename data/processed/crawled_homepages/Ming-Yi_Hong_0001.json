{
  "name": "Ming-Yi Hong 0001",
  "homepage": "https://people.ece.umn.edu/~mhong/mingyi.html",
  "status": "success",
  "content": "Mingyi Hong Mingyi Hong Home Biography Research Group People Publications Activities Projects Funded Projects Research Teaching EE 5239 EE 3015 Mingyi Hong Mingyi Hong Associate Professor Electrical and Computer Engineering University of Minnesota 6-109 Keller Hall University of Minnesota, Minneapolis, MN 55455 Google Scholar citation, CV Email: mhong at umn.edu Group GitHub Page https://github.com/OptimAI-Lab X (Twitter): https://x.com/Mingyi552237 Group X (Twitter): https:x.com/OptimAI_LAB Research Group Our research group, Optimization for AI Lab (OptimAI-Lab), conduct research on contemporary issues in optimization, information processing and foundation models (Large Language Models, diffusion models, etc). See here for our publication, and here for the current projects. Teaching EE 3015 Signal and Systems, Spring 2019, 2022, UMN, ECE Department EE 5239 Nonlinear Optimization, Fall 2017, 2018, 2019, 2020, 2021, 2023,2024 UMN, ECE Department RA and Postdoctoral position available We have research assistants and post doctoral fellow position available, in the general area of optimization, diffusion models, LLM. If you are interested, please contact Dr. Hong via email Living in Minnesota Minneapolis-St. Paul Virtual Tour 27 Photos Of Minneapolis That Will Make You Want To Move There 30 Photos Of Minneapolis That Will Make You Want To Move There Group News Sept. 2025, INFORMS Balas Prize: M. Receives the Egon Balas Prize from INFORMS Optimization Society. This prize is awarded annually to an individual for a body of contributions in the area of optimization. Sept. 2025, new paper on Bilevel optimization: A new paper A Correspondence-Driven Approach for Bilevel Decision-making with Nonconvex Lower-Level Problems, joint work with Xiaotian, Jiaxiang and Shuzhong is available [here]. In this work, we study challenging bilevel optimization problems where the lower-level problem is non-convex. Sept. 2025, new students joining the group: Welcome Zijian Zhang (ECE) and Shuyu Gan (CSE, co-advised with DK) who joined the group as first-year PhD students. July 2025, new grant: A new 3-year grant Collaborative Research: Unregistered Spectral Image Fusion in Remote Sensing: Foundations and Algorithms is awarded by NSF (joint work with Xiao); In this work, we develop theory and algorithms for challenging fusion tasks in remote sensing. July 2025, a talk on bilevel optimization: M. Delivered a semi-plenary talk in ICCOPT 2025. The slides can be find here June 2025, new paper on parameter efficient pertaining: A new paper A Minimalist Optimizer Design for LLM Pretraining, joint work with Thanos, Jiaxiang and Andi is available [here]. In this work, propose an approach that builds efficient pretraining algorithms from scratch. May 2025, new grant: A new 2-year grant Invariance in LLM Unlearning Advancing Optimization Foundations for Machine Unlearning is awarded by [Open Philantrophy] (Technical AI Safety Research, joint work with Sijia and Shiyu); In this work, we develop theory and algorithms for LLM unlearning. May 2025, new paper on RL for agents: A new paper Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment, joint work with Siliang, Quan, William (Prime Intellect), Oana (Morgan Stanley), Yuriy Nevmyvaka (Morgan Stanley) is available [here]. In this work, we show that it is critical to perform credit assignment when training LLMs for multi-turn agent applications; Please find the code [here] May 2025, new paper on unlearning: A new paper BLUR: A Bi-Level Optimization Approach for LLM Unlearning, joint work with Hadi, Sijia and Amazon colleagues is available. In this work, we propose a new formulation of the unlearning problem, based on a (simple) bilevel optimization optimization, which can prioritize the unlearning capabilities while maintaining the desirable content from the LLM output. Our extensive experiments demonstrate that BLUR consistently outperforms all the state-of-the-art algorithms across various unlearning tasks, models, and metrics. Please find the code [here] May 2025, new survey paper on alignment: Our survey paper Aligning Large Language Models with Human Feedback: Mathematical Foundations and Algorithm Design, joint work with Siliang, Luca, Chenliang, Jiaxiang, Volkan (EPFL), Stephano (Stanford), Markus (Google) and Alfredo (TAMU), on algorithmic foundations and perspective from inverse RL for LLM alignment has been uploaded online; Please find the paper here [here] May 2025, new preprint entitled RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models, joint work with Quan, Oscar, Hoi-To, Kati and Yungsuk, is available; see the preprint [here]. In this work, we develop a novel algorithm for performing quantization while doing SFT for LLM finetuning. Our method, named RoSTE, consistently achieves superior performances across various tasks and different LLM architectures as compared with the SOTA algorithms. See code [here]. April 2025: 5 papers accepted by ICML 2025. Congratulations to everyone! “RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models” see the paper [here], joint work with Quan, Oscar, To, Katie and Yungsuk “Inference-Time Alignment of Diffusion Models with Direct Noise Optimization” see the paper [here], joint work with Zhiwei and Jon and Tsung-Hui “Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond” see the paper [here] , join work with Chongyu, Sijia, Anil, Yihua. “On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains” see the paper [here], joint work with Jie, Xun, Ganghua, Xuan and researchers from Cisco. “BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning”, see the paper [here], joint work with Han et al with Zhaoran's group April 2025 new preprint (with Prashant, Ioannis, Yihua and Sijia) entitled A Doubly Stochastically Perturbed Algorithm for Linearly Constrained Bilevel Optimization is available; see the preprint [here]. In this work, we develop a novel perturbation strategy that solves a linearly constrained bilevel problem, and analyzed their convergence guarantees. Feb. 2025:, 3 papers accepted by ICLR 2025. Congratulations to everyone! “DiSK: Differentially Private Optimizer with Simplified Kalman Filter for Noise Reduction” see the paper [here], joint work with Xinwei, Meisam, and Researchers from Google and Amazon “Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs” see the paper [here] , join work with Siyan, Kaixiang, and researchers from Amazon. (Oral paper) “Joint Reward and Policy Learning with Demonstrations and Human Feedback Improves Alignment” see the paper [here], joint work with Siliang, Chenliang, Alfredo, Dongyeop and Zeyi. (Spotlight paper) Jan. 2025 new preprint (with Xiaotian, Jiaxiang and Shuzhong) entitled Barrier Function for Bilevel Optimization with Coupled Lower-Level Constraints: Formulation, Approximation and Algorithms is available; see the preprint [here]; We introduce a novel approach for bilevel optimization tackling: (1) Strongly convex lower-level problems; (2) Linear programming lower levels (e.g., price setting – first to address this!) We require only the most basic assumptions. Our barrier function method guarantees non-asymptomatic convergence. Validated by experiments. The following is the problem we consider, and the reformulated problem we consider: Jan. 2025 : Congratulation to Songtao, who joins the Chinese University of Hong Kong Computer Science Department as an Assistant Professor. Jan. 2025, IEEE Fellow: M. is elected to IEEE Fellow with the citation “for contributions to optimization in signal processing, wireless communication and machine learning”. Jan. 2025, student fellowship: Congratulations to Quan and Xinnan for receiving the Amazon Machine Learning System Fellowship ! This fellowship awards students who will contribute to research advancing the science of computer systems and/or software systems support for machine learning and artificial intelligence. Dec. 2024, new grant: A new 2-year grant ACED: Building Molecule Generative Models for Drug Development via Conditional Diffusion and Multi-Property Optimization is awarded by NSF; In this work, we develop optimization-based computational methods to align diffusion model generation with desired drug properties, so to enable efficient molecule generation and drug discovery. Nov. 2024, new grant: A new 3-year grant Inverse Reinforcement Learning with Heterogeneous Data: Estimation Algorithms with Finite Time and Sample Guarantees is awarded by NSF; In this work, we develop theory and algorithms for LLM alignment (e.g., RLHF, DPO, etc) from inverse reinforcement learning perspective. Nov. 2024:, Congratulations to Siliang and Songtao to receive the prestigious IBM Pat Goldberg Memorial Award (honorable mention), for our 2022 NeurIPS paper A Stochastic Linearized Augmented Lagrangian Method for Decentralized Bilevel Optimization [here]; see the IBM announcement [here]. Oct. 2024, Cisco Research Award: We are honored to be awarded a Cisco research award to investigate inference-time LLM alignment problems. Oct. 2024 new preprint (with Xinwei, Meisam and Google Research collaborators) entitled Disk: Differentially private optimizer with simplified Kalman filter for noise reduction is available; see the preprint [here]; This work propose a novel idea of leveraging Kalman Filter to reduce the noise for the DP algorithms; it achieves new state-of-the-art results across diverse tasks, including vision tasks such as CIFAR100 and ImageNet-1k and language fine-tuning tasks such as GLUE, E2E, and DART. Oct. 2024 paper accepted (OR):, our work Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees (joint work with Alfredo and Siliang) h",
  "content_length": 25350,
  "method": "requests",
  "crawl_time": "2025-12-01 14:00:20"
}