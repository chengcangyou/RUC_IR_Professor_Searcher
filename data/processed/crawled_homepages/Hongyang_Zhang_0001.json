{
  "name": "Hongyang Zhang 0001",
  "homepage": "https://hongyanz.github.io",
  "status": "success",
  "content": "Hongyang Zhang's Homepage Hongyang Zhang   张弘扬 Assistant Professor David R. Cheriton School of Computer Science Faculty of Mathematics University of Waterloo & Vector Institute for AI Office: DC 2641 Email: hongyang.zhang AT uwaterloo.ca [Google Scholar] [Personal GitHub] [Lab GitHub] [DBLP] I am a tenure-track Assistant Professor at University of Waterloo, David R. Cheriton School of Computer Science, leading SafeAI Lab. I am also a faculty member at Vector Institute for AI. I am interested in the problems where beautiful theory and practical methodology meet, which broadly include theories and applications of machine learning and algorithms. I completed my Ph.D. degree in 2019 at Machine Learning Department, Carnegie Mellon University, where I was fortunate to be co-advised by Maria-Florina Balcan and David P. Woodruff. Before joining Waterloo, I was a Postdoc fellow at Toyota Technological Institute at Chicago (TTIC), hosted by Avrim Blum and Greg Shakhnarovich. I graduated from Peking University in 2015. I also spent time at Simons Institute and IPAM. Blogs and Software EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty (adopted by SGLang, SpecForge, vLLM, NVIDIA TensorRT-LLM, NVIDIA NeMo Framework, NVIDIA TensorRT Model Optimizer, AMD ROCm, Hugging Face, AWS NeuronX Distributed Core, Intel Extension for Transformers, Intel LLM Library for PyTorch, DeepSeek V3, CPM.cu, Baidu PaddleNLP, Tencent AngelSlim, and MLC-LLM) [blog] [code] [EAGLE-1] [EAGLE-2] [EAGLE-3] The Matrix: Infinite-Horizon World Generation with Real-Time Moving Control [blog] [paper] AON-PRISMA: All-Or-Nothing Private Similarity Matching [blog] [code] Research Areas Machine Learning, Inference Acceleration, AI Security, World Models. Current research includes: Inference Acceleration: Pioneering advanced algorithms for enhancing AI inference speed. Drastically lowering the deployment costs of foundation models, making them more accessible and efficient. [e.g., EAGLE: the fastest-known speculative sampling] System-2 LLMs: Creating universal algorithms focusing on test-time compute of LLMs for boosted reasoning and alignment. [e.g., RAIN: LLM alignment without finetuning] AI Security: Building foundations of defenses against adversarial attacks, o.o.d. attacks, and privacy attacks. Developing watermarking techniques and prioritizing copyright and privacy protection in LLMs. [e.g., TRADES: SOTA adversarial training methodology with 5-year time test] World Models and Agents: Developing world models that are able to imagine and generate data to train agents for real-world applications such as robotics and self-driving cars. We follow a path of AAA Games -> World Models -> Real World. [e.g., The Matrix: an infinite-length, hyper-realistic world model with real-time, frame-level control] Awards and Honors 2025. Faculty Member at Vector Institute 2025. Google Research Scholar Award 2025. Top Area Chairs Award for NeurIPS 2025 2025. Outstanding Meta Reviewer Award for ICML 2025 (19 people selected worldwide) 2024. Top Area Chairs Award for NeurIPS 2024 2024. AAAI New Faculty Highlights 2023. WAIC Yunfan Award 2023. Amazon Research Award 2021. 1st place (out of 1,559 teams) in CVPR 2021 Security AI Challenger: Unrestricted Adversarial Attacks on ImageNet [certificate] 2020 - present. In the defense benchmark RobustBench, 10 out of top-10 methods use TRADES as training algorithms Jan. 2019 - Dec. 2019. 1st place in Unrestricted Adversarial Example Challenge (hosted by Google) 2018. 1st place (out of 396 teams) in NeurIPS 2018 Adversarial Vision Challenge (Robust Model Track) 2018. 1st place (out of 75 teams) in NeurIPS 2018 Adversarial Vision Challenge (Targeted Attacks Track) 2018. 3rd place (out of 101 teams) in NeurIPS 2018 Adversarial Vision Challenge (Untargeted Attacks Track) News 2025/11/4. I was appointed as an Associate Editor for IEEE TPAMI. 2025/10/23. I was appointed as a Senior Area Chair for ICML 2026. 2025/9/18. Two papers were accepted to NeurIPS 2025. 2025/3/3. EAGLE-3 was released, with up to 1.4x speedup compared to EAGLE-2. [paper] [code] [机器之心] 2025/2/27. One paper was accepted to CVPR 2025. 2024/12/10. I will serve as an area chair for ICML 2025, ACL 2025, and NeurIPS 2025. 2024/12/5. One paper was accepted to IEEE Transactions on Information Forensics & Security. 2024/11/20. The Matrix was released, the first world model for generating infinite-length videos with real-time control. [blog] [paper] [量子位] 2024/9/26. One paper was accepted to NeurIPS 2024. 2024/9/15. One paper was accepted to EMNLP 2024. 2024/8/7. I served as an area chair for ICLR 2025, ALT 2025 and AISTATS 2025. I was promoted to IEEE Senior Member. 2024/6/27. EAGLE-2 was released, with up to 1.4x speedup compared to EAGLE-1. [paper] [code] [机器之心] 2024/5/27. One paper was accepted to ECML PKDD 2024. 2024/5/6. I served as an area chair for NeurIPS 2024 and an oral session chair for ICLR 2024. 2024/5/1. Three papers were accepted to ICML 2024. 2024/4/3. One paper was accepted to ACM CCS 2024. 2024/1/17. EAGLE v1.1 was released, with 6.5x speedup of LLM decoding. [code] 2024/1/16. Two papers were accepted to ICLR 2024. I served on the program committee for COLT 2024. 2023/12/9. I was selected as AAAI New Faculty Highlights, and one paper was accepted to AAAI 2024. 2023/12/8. EAGLE v1.0 was released, with 3x speedup of LLM decoding. [blog] [code] [机器之心] 2023/9/15. I served as an area chair for ICLR 2024 and AISTATS 2024, and an action editor for DMLR. 2023/6/11. One paper was accepted to IEEE Transactions on Information Theory. 2023/4/24. Two papers were accepted to ICML 2023. 2023/3/7. I served as an area chair for NeurIPS 2023 and served on the technical program committee for ACM CCS 2023. 2023/2/28. One paper was accepted to CVPR 2023. 2023/2/22. One paper was accepted to Journal of Machine Learning Research. 2023/1/21. One paper was accepted to ICLR 2023. 2023/1/20. One paper was accepted to AISTATS 2023. 2022/12/11. One paper was accepted to IEEE SaTML 2023. 2022/10/30. One paper was accepted to ITCS 2023. 2022/10/13. One paper was accepted to IEEE Transactions on Pattern Analysis and Machine Intelligence. 2022/10/6. One paper was accepted to EMNLP 2022. 2022/9/14. One paper was accepted to NeurIPS 2022. 2022/5/15. Two papers were accepted to ICML 2022. Selected Publications [Full List of Publications] Inference Acceleration Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang. \"EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test\", NeurIPS 2025, San Diego, USA. [arXiv] [code] Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang. \"EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees\", EMNLP 2024, Miami, USA. [arXiv] [code] Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang. \"EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty\", ICML 2024, Vienna, Austria. [arXiv] [code] System-2 LLMs Yu Du, Fangyun Wei, Hongyang Zhang. \"AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\", ICML 2024, Vienna, Austria. [arXiv] [code] Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, Hongyang Zhang. \"RAIN: Your Language Models Can Align Themselves without Finetuning\", ICLR 2024, Vienna, Austria. [arXiv] [code] AI Security Haochen Sun, Jason Li, Hongyang Zhang. \"zkLLM: Zero Knowledge Proofs for Large Language Models\", ACM CCS 2024, Salt Lake City, USA. [pdf] Avrim Blum, Travis Dick, Naren Manoj, Hongyang Zhang. \"Random Smoothing Might be Unable to Certify L∞ Robustness for High-Dimensional Images\", Journal of Machine Learning Research 21, 2020. [pdf] [code] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, Michael I. Jordan. \"Theoretically Principled Trade-off between Robustness and Accuracy\", ICML 2019 (Long Talk), Long Beach, USA. [arXiv] [code] (Champion of NeurIPS 2018 Adversarial Vision Challenge; Top-10 most influential paper in ICML 2019-2024 according to Google Scholar Metrics) Learning Theory Maria-Florina Balcan, Yi Li, David P. Woodruff, Hongyang Zhang. \"Testing Matrix Rank, Optimally\", SODA 2019, San Diego, USA. [pdf] [arXiv] Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, Hongyang Zhang. “Learning and 1-bit Compressed Sensing under Asymmetric Noise\", COLT 2016, New York, USA. [pdf] World Models Ruili Feng, Han Zhang, Zhantao Yang, Jie Xiao, Zhilei Shu, Zhiheng Liu, Andy Zheng, Yukun Huang, Yu Liu, Hongyang Zhang. \"The Matrix: Infinite-Horizon World Generation with Real-Time Moving Control\", NeurIPS 2025, San Diego, USA. [paper] [blog] Books Zhouchen Lin, Hongyang Zhang. “Low Rank Models in Visual Analysis: Theories, Algorithms and Applications”, Academic Press, Elsevier, 2017. [Press link] Table of Contents Introduction Linear Models (Single Subspace Models, Multiple-Subspace Models, Theoretical Analysis) Non-Linear Models (Kernel Methods, Laplacian and Hyper-Laplacian Methods, Locally Linear Representation, Transformation Invariant Clustering) Optimization Algorithms (Convex Algorithms, Non-Convex Algorithms, Randomized Algorithms) Representative Applications (Video Denoising, Background Modeling, Robust Alignment by Sparse and Low-Rank Decomposition, Transform Invariant Low-Rank Textures, Motion and Image Segmentation, Image Saliency Detection, Partial-Duplicate Image Search, Image Tag Completion and Refinement, Other Applications) Conclusions (Low-Rank Models for Tensorial Data, Nonlinear Manifold Clustering, Randomized Algorithms) Academic Activities (Senior) Area Chair: ICML 2026 (Senior Area Chair), ICLR 2026, ALT 2026, NeurIPS 2025 (Top Area Chairs), ACL 2025, ICML 2025 (Outstanding Meta Reviewer), AISTATS 2025, ICLR 2025, ALT 2025, PRCV 2025 (Senior Area Chair), NeurIPS 2024 (Top Area Chairs), ICML 2024 (TF2M), ICLR 2024, AISTATS 2024, PRCV 2024 (Senior Area Chair), NeurIPS 2023, AAAI 2022, AAAI 2021, VALSE 2021-2025. Associate Editor: IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), Data-centric Machine Learni",
  "content_length": 13482,
  "method": "requests",
  "crawl_time": "2025-12-01 13:21:00"
}