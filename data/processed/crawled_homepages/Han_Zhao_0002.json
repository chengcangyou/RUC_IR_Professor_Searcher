{
  "name": "Han Zhao 0002",
  "homepage": "https://hanzhaoml.github.io",
  "status": "success",
  "content": "Han Zhao's homepage Han Zhao | 赵晗 Assistant Professor Department of Computer Science Department of Electrical and Computer Engineering (affiliated) University of Illinois Urbana-Champaign hanzhao [AT] illinois (DOT) edu 3320 Siebel Center, 201 N Goodwin Ave Urbana, IL, 61801 CV Google Scholar DBLP Github About Me I am an assistant professor at the Department of Computer Science, University of Illinois Urbana-Champaign, affiliated with the Department of Electrical and Computer Engineering. I am also an Amazon scholar at Amazon AI and Search Science. Before joining UIUC, I was a machine learning researcher at D. E. Shaw & Co. I obtained my Ph.D. from the Machine Learning Department, Carnegie Mellon University. Previously, I obtained my BEng degree from the Computer Science Department at Tsinghua University and MMath from the University of Waterloo. I have a broad interest in trustworthy machine learning. In particular, I work on transfer learning (domain adaptation/generalization/distributional robustness, multitask/meta-learning), algorithmic fairness, probabilistic circuits, and their applications in natural language, signal processing and quantitative finance. My long-term goal is to build trustworthy ML systems that are efficient, robust, fair, and interpretable. Acknowledgments Our group's research has been generously supported by Google Research, Meta AI, Amazon AI, Nvidia, IBM Research, the National Science Foundation (NSF), and the Defense Advanced Research Projects Agency (DARPA). Thank you! Prospective students read this For PhD applicants: Thank you for your interest! I am not recruiting any new PhD students in Fall' 26. If you are interested in our CS PhD program, please apply to the UIUC CS graduate program. There is no need to directly contact me regarding PhD admissions as it will be handled by the admission committee. For undergraduate/MS students at UIUC: Please fill out this Google form. Your chance of getting involved is higher if more of the followings are true: you have a high GPA; you did quite well on courses related to math, statistics, and/or machine learning; you are able to commit 12+ hours per week on research; you have strong programming skills. Publications [ show selected / show by date ] A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning Y. Hu, F. Wu, H. Ye, D. Forsyth, J. Zou, N. Jiang, J. W. Ma, H. Zhao In Proceedings of the 39th Advances in Neural Information Processing Systems (NeurIPS 2025, Oral) [abs] [pdf] Online reinforcement learning (RL) excels in complex, safety-critical domains, yet it faces challenges such as sample inefficiency, training instability, and a lack of interpretability. Data attribution offers a principled way to trace model behavior back to individual training samples. However, in online RL, each training sample not only drives policy updates but also influences future data collection, violating the fixed dataset assumption in existing attribution methods. In this paper, we initiate the study of data attribution for online RL, focusing on the widely used Proximal Policy Optimization (PPO) algorithm. We start by establishing a local attribution framework, interpreting model checkpoints with respect to the records in the recent training buffer. We design two target functions, capturing agent action and cumulative return respectively, and measure each record's contribution through gradient similarity between its training loss and these targets. We demonstrate the power of this framework through three concrete applications: diagnosis of learning, temporal analysis of behavior formation, and targeted intervention during training. Leveraging this framework, we further propose an algorithm, iterative influence-based filtering (IIF), for online RL training that iteratively performs experience filtering to refine policy updates. Across standard RL benchmarks (classic control, navigation, locomotion) to RLHF for large language models, IIF reduces sample complexity, speeds up training, and achieves higher returns. Overall, these results advance interpretability, efficiency, and effectiveness of online RL. Understanding and Improving Adversarial Robustness of Neural Probabilistic Circuits W. Chen, H. Zhao In Proceedings of the 39th Advances in Neural Information Processing Systems (NeurIPS 2025) [abs] [pdf] [code] Neural Probabilistic Circuits (NPCs), a new class of concept bottleneck models, comprise an attribute recognition model and a probabilistic circuit for reasoning. By integrating the outputs from these two modules, NPCs produce compositional and interpretable predictions. While offering enhanced interpretability and high performance on downstream tasks, the neural-network-based attribute recognition model remains a black box. This vulnerability allows adversarial attacks to manipulate attribute predictions by introducing carefully crafted subtle perturbations to input images, potentially compromising the final predictions. In this paper, we theoretically analyze the adversarial robustness of NPC and demonstrate that it only depends on the robustness of the attribute recognition model and is independent of the robustness of the probabilistic circuit. Moreover, we propose RNPC, the first robust neural probabilistic circuit against adversarial attacks on the recognition module. RNPC introduces a novel class-wise integration for inference, ensuring a robust combination of outputs from the two modules. Our theoretical analysis demonstrates that RNPC exhibits provably improved adversarial robustness compared to NPC. Empirical results on image classification tasks show that RNPC achieves superior adversarial robustness compared to existing concept bottleneck models while maintaining high accuracy on benign inputs. Neural Probabilistic Circuits: Enabling Compositional and Interpretable Predictions through Logical Reasoning W. Chen, S. Yu, H. Shao, L. Sha, H. Zhao arXiv preprint [abs] [pdf] [code] End-to-end deep neural networks have achieved remarkable success across various domains but are often criticized for their lack of interpretability. While post hoc explanation methods attempt to address this issue, they often fail to accurately represent these black-box models, resulting in misleading or incomplete explanations. To overcome these challenges, we propose an inherently transparent model architecture called Neural Probabilistic Circuits (NPCs), which enable compositional and interpretable predictions through logical reasoning. In particular, an NPC consists of two modules: an attribute recognition model, which predicts probabilities for various attributes, and a task predictor built on a probabilistic circuit, which enables logical reasoning over recognized attributes to make class predictions. To train NPCs, we introduce a three-stage training algorithm comprising attribute recognition, circuit construction, and joint optimization. Moreover, we theoretically demonstrate that an NPC's error is upper-bounded by a linear combination of the errors from its modules. To further demonstrate the interpretability of NPC, we provide both the most probable explanations and the counterfactual explanations. Empirical results on four benchmark datasets show that NPCs strike a balance between interpretability and performance, achieving results competitive even with those of end-to-end black-box models while providing enhanced interpretability. Efficient Utility-Preserving Machine Unlearning with Implicit Gradient Surgery S. Zhou, T. Yu, Z. Zhang, H. Chang, X. Zhou, D. Wu, H. Zhao In Proceedings of the 39th Advances in Neural Information Processing Systems (NeurIPS 2025) [abs] [pdf] Machine unlearning (MU) aims to efficiently remove sensitive or harmful memory from a pre-trained model. The key challenge is to balance the potential tradeoff between unlearning efficacy and utility preservation, which involves forgetting undesirable information as defined while maintaining the model's original performance. One potential way to tackle this problem is to use multi-objective optimization to jointly optimize both the unlearning and utility preservation objectives. However, existing multi-objective methods only guarantee finding a Pareto-optimal solution without fine-grained control, which causes under-optimization of the unlearning objective. To this end, we first model MU as a constrained optimization problem, that is, optimizing the unlearning objective under the constraint of a bounded increase for utility loss. We then show that solving this optimization problem is equivalent to unilateral gradient surgery on the unlearning objective. To resolve the additional computational cost brought by gradient surgery, we propose an implicit gradient surgery method, which approximates the solution to the aforementioned constrained optimization problem via only one backpropagation, thereby achieving efficient utility-preserving MU. Theoretically, we provide a tight convergence analysis of the algorithm. Empirically, our extensive experiments show that the proposed algorithm achieves better tradeoff results than existing baselines. Theoretically, we provide a tight convergence analysis of the algorithm. Empirically, our extensive experiments show that the proposed algorithm achieves better tradeoff results than existing baselines. Taming Hyperparameter Sensitivity in Data Attribution: Practical Selection Without Costly Retraining W. Wang, J. Deng, Y. Hu, S. Zhang, X. Jiang, R. Zhang, H. Zhao, J. W. Ma In Proceedings of the 39th Advances in Neural Information Processing Systems (NeurIPS 2025) [abs] [pdf] Data attribution methods, which quantify the influence of individual training data points on a machine learning model, have gained increasing popularity in data-centric applications in modern AI. Despite a recent surge of new methods developed in this space, the impact of hyperparameter tuning in these methods remains under-explored. In this work, w",
  "content_length": 190670,
  "method": "requests",
  "crawl_time": "2025-12-01 13:17:48"
}