{
  "name": "Hao Wang 0014",
  "homepage": "http://www.wanghao.in",
  "status": "success",
  "content": "Hao Wang, Assistant Professor, Rutgers CS Hao Wang (çç) Home Publication Research Teaching Services Press Awards & Misc Google Scholar About Me I am an assistant professor in the Department of Computer Science at Rutgers University, where I direct a Machine Learning Lab. Previously I was a Postdoctoral Research Associate at Computer Science & Artificial Intelligence Lab of Massachusetts Institute of Technology, working with Prof. Dina Katabi and Prof. Tommi Jaakkola. I obtained Ph.D degree in CSE department, Hong Kong University of Science and Technology. My supervisor was Prof. Dit-Yan Yeung. I was a visiting scholar working with Prof. Eric Xing's group in Machine Learning department of Carnegie Mellon University. I am also a Microsoft Fellow and received the Baidu Research Fellowship. Before my Ph.D, I got my BS degree from Shanghai Jiao Tong University, 2013 under the supervision of Prof. Wu-Jun Li. Email: hoguewang AT gmail.com / hw488 AT cs.rutgers.edu / hogue.wang AT rutgers.edu / hwang87 AT mit.edu Recruiting: I am recruiting PhD students starting from Fall 2025 as well as interns. Send me an email if you are interested in working with me at Rutgers. [Twitter] [Facebook] [Github] [LinkedIn] [Medium] [CV] Research Interest My research interest focuses on statistical machine learning, deep learning, and large language models (LLMs). Currently, I mainly work on Bayesian deep learning, probabilistic methods, game-theoretic approaches, and their applications in trustworthy & safe AI (interpretability, robustness, alignment, etc.), healthcare, recommender systems, computer vision (including multimodal LLMs), natural language processing (including LLMs), network analysis, and data mining. Updates Our papers on Bayesian LLMs, diffusion models, and their uncertainty, imbalance, and safety, \"Training-Free Bayesianization for Low-Rank Adapters of Large Language Models\", \"PoGDiff: Product-of-Gaussians Diffusion Models for Imbalanced Text-to-Image Generation\", and \"LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs\" are accepted at NeurIPS (9/18/25). Honored to receive the ACM SIGKDD 2025 Test of Time Award for our KDD 2015 paper, \"Collaborative Deep Learning for Recommender Systems.\" This is the award talk (8/4/25). Our paper on interpretable ML and healthcare, \"Phenotypic Prediction of Missense Variants via Deep Contrastive Learning\", is accepted at Nature Biomedical Engineering (7/30/25). I gave a keynote talk at ICML Workshop on Foundation Models for Structured Data (7/18/25). Our survey paper on continual learning of large language models, \"Continual Learning of Large Language Models: A Comprehensive Survey\", is accepted at ACM Computing Surveys (5/8/25). Our papers on language language models, interpretability & safety, and domain adaptation, \"The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steerings\" and \"Concept-Based Unsupervised Domain Adaptation\" are accepted at ICML (5/1/25). Our five papers on Bayesian deep learning, language language models, domain adaptation, and their interpretability & safety, \"Towards Domain Adaptive Neural Contextual Bandits\" \"Implicit In-Context Learning\" \"GenVP: Generating Visual Puzzles with Contrastive Hierarchical VAEs\" \"NetFormer: An Interpretable Model for Recovering Dynamical Connectivity in Neuronal Population Dynamics\" and \"On Calibration of LLM-based Guard Models for Reliable Content Moderation\" are accepted at ICLR (1/22/25). Our paper on benchmarking multimodal large language models, \"Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models\", is accepted at NAACL (1/22/25). Our papers on Bayesian large language models and natural counterfactual inference, \"BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models\" and \"Natural Counterfactuals With Necessary Backtracking\" are accepted at NeurIPS (9/25/24). Our paper on Bayesian deep learning for interpretable large language models, \"Variational Language Concepts for Interpreting Foundation Language Models\", is accepted at Findings of EMNLP (9/21/24). Grateful to receive the NSF CAREER Award on Robustifying AI with Bayesian Deep Learning (07/10/24). Grateful to receive an NIH R01 Award as PI, \"Counterfactual Explanations for AI-Assisted Cancer Diagnosis and Subtyping\" (07/01/24). Our paper on large language models for medical education, \"Benchmarking Large Language Models on Communicative Medical Coaching: A Dataset and a Novel System\", is accepted at Findings of ACL (5/16/24). Our papers on safe & trustworthy large language models and Bayesian deep learning, \"Probabilistic Conceptual Explainers: Towards Trustworthy Conceptual Explanations for Vision Foundation Models\" and \"Delving into Differentially Private Transformer\" are accepted at ICML (5/1/24). We are organizing the ICML 2024 Workshop on \"Foundation Models in the Wild\" (3/27/24). Our paper on safe & trustworthy large language models, \"LatticeGen: Hiding Generated Text in a Lattice for Privacy-Aware Large Language Model Generation on Cloud\", is accepted at Findings of NAACL (3/13/24). Grateful to receive the Microsoft Research AI & Society Fellowship (03/01/24). Our papers on safe & trustworthy large language models, Bayesian deep learning, domain adaptation, and interpretability, \"Detecting Text from Large Language Models via Rewriting\", \"Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Probabilistic Interpretations\", \"Continuous Invariance Learning\" are accepted at ICLR (1/16/24). Our paper on multi-domain active learning and domain adaptation, \"Composite Active Learning: Towards Multi-Domain Active Learning with Theoretical Guarantees\", is accepted at AAAI (11/9/23). Our papers on Bayesian deep learning, domain adaptation, and continual learning, \"A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm\", and \"Variational Imbalanced Regression: Fair Uncertainty Quantification via Probabilistic Smoothing\" are accepted at NeurIPS (9/21/23). Our paper on learning optimization landscape, \"Landscape Learning for Neural Network Inversion\", is accepted at ICCV (7/14/23). Our papers on Bayesian deep learning, causality, interpretable ML, robustness, and domain adaptation, \"Self-Interpretable Time Series Prediction with Counterfactual Explanations\", \"Taxonomy-Structured Domain Adaptation\", and \"Robust Perception through Equivariance\" are accepted at ICML (4/24/23). Our paper on Bayesian deep learning for domain adaptation, \"Domain-Indexing Variational Bayes for Domain Adaptation\", is accepted at ICLR (1/20/23). Our Nature Medicine paper on machine learning for health, \"Artificial Intelligence-Enabled Detection and Assessment of Parkinsonâs Disease Using Nocturnal Breathing Signals\", has been selected as one of the Ten Notable Advances in 2022 by Nature Medicine (1/10/23). We are organizing the CVPR 2023 Workshop on \"New Frontiers in Visual Language Reasoning: Compositionality, Prompts and Causality\" (12/15/22). Our paper on Bayesian deep learning for speech recognition and education, \"Unsupervised Mismatch Localization in Cross-Modal Sequential Data with Application to Mispronunciations Localization\", is accepted at TMLR (12/8/22). Our paper on Bayesian deep learning for federated learning, \"FedNP: Towards Non-IID Federated Learning via Federated Neural Propagation\", is accepted at AAAI (11/18/22). Our paper on causal and counterfactual recommender systems, \"Collaborative Counterfactual Reasoning\", is accepted at WSDM (10/18/22). Our papers on Bayesian deep learning, continuously streaming domain adaptation, and spatio-temporal forecasting, \"Extrapolative Continuous-Time Bayesian Neural Network for Fast Training-Free Test-Time Adaptation\" and \"Earthformer: Exploring Space-Time Transformers for Earth System Forecasting\" are accepted at NeurIPS (09/14/22). Our paper on multi-domain imbalanced learning and deep learning for health, \"Artificial Intelligence-Enabled Detection and Assessment of Parkinsonâs Disease Using Nocturnal Breathing Signals\", is accepted at Nature Medicine (8/2/22). Our papers on multi-domain imbalanced learning and relational forecasting, \"On Multi-Domain Long-Lailed Recognition, Generalization and Beyond\" and \"Social ODE: Multi-Agent Trajectory Forecasting with Neural Ordinary Differential Equations\" are accepted at ECCV (07/03/22). Our paper, \"OrphicX: A Causality-Inspired Latent Variable Model for Interpreting Graph Neural Networks\", is a best paper finalist at CVPR 2022 (6/24/22). Our paper on domain adaptation Transformer, \"Domain Adaptation for Time Series Forecasting via Attention Sharing\", is accepted at ICML (05/13/22). Our paper on Bayesian deep learning and interpretable ML for healthcare, \"'My Nose is Running.' 'Are you Also Coughing?': Building a Medical Diagnosis Agent with Interpretable Inquiry Logics\", is accepted at IJCAI (04/20/22). Our three papers on causality, interpretable ML and Bayesian deep learning, \"Causal Transportability for Visual Recognition\", \"OrphicX: A Causality-Inspired Latent Variable Model for Interpreting Graph Neural Networks\", and \"Bayesian Invariant Risk Minimization\", are accepted at CVPR (03/03/22). Our paper, \"Graph-Relational Domain Adaptation\", is accepted at ICLR (1/20/22). We are organizing the ICLR 2022 Workshop on \"PAIR^2Struct: Privacy, Accountability, Interpretability, Robustness, Reasoning on Structured Data\" (12/06/21). Our two papers on uncertainty estimation, \"Context Uncertainty in Contextual Bandits with Applications to Recommender Systems\" and \"Training-Free Uncertainty Estimation for Dense Regression: Sensitivity as a Surrogate\", are accepted at AAAI (12/01/21). Grateful to receive NSF grant IIS-2127918 as PI, \"RI: Small: Enabling Interpretable AI via Bayesian Deep Learning\" (08/25/21). Our two papers, \"Adversarial Attacks Are Reversible with Natural Super",
  "content_length": 14201,
  "method": "requests",
  "crawl_time": "2025-12-01 13:18:25"
}