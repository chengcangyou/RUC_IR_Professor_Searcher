{
  "name": "Patanamon Thongtanunam",
  "homepage": "http://patanamon.com",
  "status": "success",
  "content": "Patanamon (Pick) Thongtanunam Dr. Patanamon Thongtanunam (or Pick) is a Senior Lecturer (equivalent to Associate Professor) at the School of Computing and Information Systems, the University of Melbourne. Pick’s research interests include empirical software engineering, data mining, and data-driven techniques to support software engineering tasks. Her primary research goals are directed towards uncovering empirical evidence, extracting knowledge from data recorded in software repositories, gleans actionable insights for software engineering management, and developing automated approaches to support developers. Her research work and endeavour has received numerous prestigious awards including an Australian Research Councile (ARC) Discovery Early Career Research Award (2021 - 2024), Japan Society for the Promotion of Science Research Fellowship (2016 - 2018), ACM SIGSOFT Distinguished Paper award, IEEE Computer Society TCSE Distinguished Paper Award, as well as distinguished reviewer awards. Contact: patanamon{dot}t{at}unimelb.edu.au Recent Publications 2025 IJCNLP-AACL Hallucinations in Code Change to Natural Language Generation: Prevalence and Evaluation of Detection Metrics Chunhua Liu, Hong Yi Lin, and Patanamon Thongtanunam In Proceedings of the International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics, 2025 Bib HTML @inproceedings{LiuAACL2025, title = {Hallucinations in Code Change to Natural Language Generation: Prevalence and Evaluation of Detection Metrics}, author = {Liu, Chunhua and Lin, Hong Yi and Thongtanunam, Patanamon}, booktitle = {Proceedings of the International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics}, pages = {to appear}, doi = {}, note = {}, year = {2025} } ASE What Types of Code Review Comments Do Developers Most Frequently Resolve? Saul Goldman, Hong Yi Lin, Jirat Pasuksmit, Patanamon Thongtanunam, Kla Tantithamthavorn, and 9 more authors In Proceedings of the IEEE International Conference on Automated Software Engineering, 2025 Bib HTML @inproceedings{banyongrakkulICSME2025, title = {What Types of Code Review Comments Do Developers Most Frequently Resolve?}, author = {Goldman, Saul and Lin, Hong Yi and Pasuksmit, Jirat and Thongtanunam, Patanamon and Tantithamthavorn, Kla and Wang, Zhe and Zhang, Ray and Behnaz, Ali and Jiang, Fan and Siers, Michael and Jiang, Ryan and Buller, Mike and Jeong, Minwoo and Wu, Ming}, booktitle = {Proceedings of the IEEE International Conference on Automated Software Engineering}, pages = {to appear}, doi = {}, note = {}, year = {2025} } TOSEM Leveraging Reviewer Experience in Code Review Comment Generation Hong Yi Lin, Patanamon Thongtanunam, Christoph Treude, Michael W Godfrey, Chunhua Liu, and 1 more author ACM Transactions on Software Engineering and Methodology, 2025 Abs Bib HTML PDF Modern code review is a ubiquitous software quality assurance process aimed at identifying potential issues within newly written code. Despite its effectiveness, the process demands large amounts of effort from the human reviewers involved. To help alleviate this workload, researchers have trained deep learning models to imitate human reviewers in providing natural language code reviews. Formally, this task is known as code review comment generation. Prior work has demonstrated improvements in this task by leveraging machine learning techniques and neural models, such as transfer learning and the transformer architecture. However, the quality of the model generated reviews remain sub-optimal due to the quality of the open-source code review data used in model training. This is in part due to the data obtained from open-source projects where code reviews are conducted in a public forum, and reviewers possess varying levels of software development experience, potentially affecting the quality of their feedback. To accommodate for this variation, we propose a suite of experience-aware training methods that utilise the reviewers’ past authoring and reviewing experiences as signals for review quality. Specifically, we propose experience-aware loss functions (ELF), which use the reviewers’ authoring and reviewing ownership of a project as weights in the model’s loss function. Through this method, experienced reviewers’ code reviews yield larger influence over the model’s behaviour. Compared to the SOTA model, ELF was able to generate higher quality reviews in terms of accuracy, informativeness, and comment types generated. The key contribution of this work is the demonstration of how traditional software engineering concepts such as reviewer experience can be integrated into the design of AI-based automated code review models. @article{LinTOSEM2025, title = {Leveraging Reviewer Experience in Code Review Comment Generation}, author = {Lin, Hong Yi and Thongtanunam, Patanamon and Treude, Christoph and Godfrey, Michael W and Liu, Chunhua and Charoenwet, Wachiraphan}, year = {2025}, journal = {ACM Transactions on Software Engineering and Methodology}, pages = {to appear}, doi = {}, } ICSME From Release to Adoption: Challenges in Reusing Pre-trained AI Models for Downstream Developers Peerachai Banyongrakkul, Mansooreh Zahedi, Patanamon Thongtanunam, Christoph Treude, and Haoyu Gao In Proceedings of the IEEE International Conference on Software Maintenance and Evolution, 2025 Acceptance rate: 30% (45/146) Bib HTML @inproceedings{banyongrakkulICSME2026, title = {From Release to Adoption: Challenges in Reusing Pre-trained AI Models for Downstream Developers}, author = {Banyongrakkul, Peerachai and Zahedi, Mansooreh and Thongtanunam, Patanamon and Treude, Christoph and Gao, Haoyu}, booktitle = {Proceedings of the IEEE International Conference on Software Maintenance and Evolution}, pages = {to appear}, doi = {}, note = {Acceptance rate: 30% (45/146)}, year = {2025} } SCAM Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification Linh Nguyen, Chunhua Liu, Hong Yi Lin, and Patanamon Thongtanunam In Proceedings of tIEEE International Conference on Source Code Analysis & Manipulation, 2025 Bib HTML @inproceedings{NguyenSCAM2025, title = {Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification}, author = {Nguyen, Linh and Liu, Chunhua and Lin, Hong Yi and Thongtanunam, Patanamon}, booktitle = {Proceedings of tIEEE International Conference on Source Code Analysis & Manipulation}, pages = {to appear}, doi = {}, note = {}, year = {2025} } ACL CodeReviewQA: The Code Review Comprehension Assessment for Large Language Models Hong Yi Lin, Chunhua Liu, Haoyu Gao, Patanamon Thongtanunam, and Christoph Treude In Findings of the Association for Computational Linguistics: ACL 2025, 2025 Abs Bib State-of-the-art large language models (LLMs) have demonstrated impressive code generation capabilities but struggle with real-world software engineering tasks, such as revising source code to address code reviews, hindering their practical use. Code review comments are often implicit, ambiguous, and colloquial, requiring models to grasp both code and human intent. This challenge calls for evaluating large language models’ ability to bridge both technical and conversational contexts. While existing work has employed the automated code refinement (ACR) task to resolve these comments, current evaluation methods fall short, relying on text matching metrics that provide limited insight into model failures and remain susceptible to training data contamination.To address these limitations, we introduce a novel evaluation benchmark, \\textbfCodeReviewQA that enables us to conduct fine-grained assessment of model capabilities and mitigate data contamination risks.In CodeReviewQA, we decompose the generation task of code refinement into \\textbfthree essential reasoning steps: \\textitchange type recognition (CTR), \\textitchange localisation (CL), and \\textitsolution identification (SI). Each step is reformulated as multiple-choice questions with varied difficulty levels, enabling precise assessment of model capabilities, while mitigating data contamination risks. Our comprehensive evaluation spans 72 recently released large language models on \\textbf900 manually curated, high-quality examples across nine programming languages. Our results show that CodeReviewQA is able to expose specific model weaknesses in code review comprehension, disentangled from their generative automated code refinement results. @inproceedings{linACL2025, title = {{C}ode{R}eview{QA}: The Code Review Comprehension Assessment for Large Language Models}, author = {Lin, Hong Yi and Liu, Chunhua and Gao, Haoyu and Thongtanunam, Patanamon and Treude, Christoph}, booktitle = {Findings of the Association for Computational Linguistics: ACL 2025}, year = {2025}, publisher = {Association for Computational Linguistics}, url = {https://aclanthology.org/2025.findings-acl.476/}, doi = {10.18653/v1/2025.findings-acl.476}, pages = {9138--9166}, isbn = {979-8-89176-256-5} } ICSE Human-In-the-Loop Software Development Agents Wannita Takerngsaksiri, Jirat Pasuksmit, Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Ruixiong Zhang, and 5 more authors In Proceedings of the ACM/IEEE International Conference on Software Engineering, 2025 Abs Bib HTML PDF Recently, Large Language Models (LLMs)-based multi-agent paradigms for software engineering are introduced to automatically resolve software development tasks (e.g., from a given issue to source code). However, existing work is evaluated based on historical benchmark datasets, does not consider human feedback at each stage of the automated software development process, and has not been deployed in practice. In this paper, we introduce a Human-in-the-loop LLM-based Agents framework (HULA) for software development that allows software engineers to refine and guide LLMs when generating coding plans",
  "content_length": 18413,
  "method": "requests",
  "crawl_time": "2025-12-01 14:09:15"
}