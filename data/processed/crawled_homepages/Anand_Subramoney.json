{
  "name": "Anand Subramoney",
  "homepage": "https://anandsubramoney.com",
  "status": "success",
  "content": "Dr. Anand Subramoney | This is the homepage of Anand Subramoney. About Me I am a Lecturer (Assistant Professor) at the Department of Computer Science at Royal Holloway, University of London. I was a post-doc at the Institut für Neuroinformatik at the Ruhr-Universität Bochum. I finished my PhD with Prof. Wolfgang Maass at the Institute for Theoretical Computer Science at Technische Universität Graz, working on biologically plausible learning and meta-learning in spiking neural networks. I have a Masters in computer science from the University of Texas at Austin where I worked with Prof. Risto Miikkulainen on using neuro-evolution and task-decomposition to learn complex tasks. I worked as a Software Development Engineer at Amazon.com in the DynamoDB team for a couple of years right after my Masters. I received my undergraduate degree at IIT Madras and worked at Indian Institute of Science, Bangalore as a Research Assistant with Prof. K Gopinath right after my undergrad. My Erdős number is 3. Research Interests I am broadly interested in learning and intelligence, both algorithmic and biological. My current research interest focuses on understanding the principles of scalability through the following questions: What properties does an algorithm need to have to scale in terms of task performance and energy consumption? Why do the current deep learning architectures work so well? How can we design algorithms that can take scaling to the next level, approaching biological levels of intelligence? How can we learn from biology about principles of scalability and use them? Much of my work derives inspiration from neuroscience and biology in the quest to build a better and more general artificial intelligence. Join Us If you’re interested in starting a collaboration, don’t hesitate to reach out and get in touch I’m happy to support your application for externally funded post-doc fellowships for working with me. Here’s a non-exhaustive list of post-doc fellowships: Newton International Fellowship Leverhume early career Fellowships Marie Skłodowska-Curie Actions Postdoctoral Fellowship For those interested in externally funded PhD programs, internships, or remote internships, feel free to email me. Stay tuned, as I’ll be regularly updating the site with other open positions as they become available. Mentoring I commit a few hours per month to mentor underrepresented groups in academia. If you need guidance on career choices, research directions, PhD applications, or anything else, please book a slot on my calendar to talk to me. Publications Link to my Google Scholar Profile * denotes equal contributions Jain A, Subramoney A, Miikkulainen R. \"Task decomposition with neuroevolution in extended predator-prey domain\". In: Proceedings of Thirteenth International Conference on the Synthesis and Simulation of Living Systems. East Lansing, MI, USA; 2012. conference (url) (pdf) (bibtex) Petrovici MA, Schmitt S, Klähn J, Stöckel D, Schroeder A, Bellec G, Bill J, Breitwieser O, Bytschok I, Grübl A, Güttler M, Hartel A, Hartmann S, Husmann D, Husmann K, Jeltsch S, Karasenko V, Kleider M, Koke C, Kononov A, Mauch C, Müller E, Müller P, Partzsch J, Pfeil T, Schiefer S, Scholze S, Subramoney A, Thanasoulis V, Vogginger B, Legenstein R, Maass W, Schüffny R, Mayr C, Schemmel J, Meier K. \"Pattern representation and recognition with accelerated analog neuromorphic systems\". In: 2017 IEEE International Symposium on Circuits and Systems (ISCAS). 2017. p. 1–4. conference (url) (pdf) (bibtex) Kaiser J, Stal R, Subramoney A, Roennau A, Dillmann R. \"Scaling up liquid state machines to predict over address events from dynamic vision sensors\". Bioinspiration & Biomimetics. June 2017; journal (url) (pdf) (bibtex) Bellec* G, Salaj* D, Subramoney* A, Legenstein R, Maass W. \"Long short-term memory and Learning-to-learn in networks of spiking neurons\". In: Advances in Neural Information Processing Systems 31. Curran Associates, Inc.; 2018. p. 795–805. conference (url) (pdf) (bibtex) Kaiser* J, Hoff* M, Konle A, Vasquez Tieck JC, Kappel D, Reichard D, Subramoney A, Legenstein R, Roennau A, Maass W, Dillmann R. \"Embodied Synaptic Plasticity With Online Reinforcement Learning\". Frontiers in Neurorobotics. 2019;13:81. journal (url) (bibtex) Subramoney A, Scherr F, Maass W. \"Learning to learn motor prediction by networks of spiking neurons\". In: Worshop on Robust Artificial Intelligence For Neurorobotics, Edinburgh. 2019. workshop (url) (bibtex) Bellec* G, Scherr* F, Subramoney A, Hajek E, Salaj D, Legenstein R, Maass W. \"A solution to the learning dilemma for recurrent networks of spiking neurons\". Nature Communications. July 2020;11(1):3625. journal (url) (preprint) (bibtex) Subramoney A, Scherr F, Maass W. \"Reservoirs Learn to Learn\". In: Nakajima K, Fischer I, editors. Reservoir Computing: Theory, Physical Implementations, and Applications. Singapore: Springer; 2021. p. 59–76. (Natural Computing Series). bookchapter (url) (preprint) (bibtex) Rao* A, Legenstein* R, Subramoney A, Maass W. \"A normative framework for learning top-down predictions through synaptic plasticity in apical dendrites\". bioRxiv. March 2021; preprint (preprint) (bibtex) Salaj* D, Subramoney* A, Kraišniković* C, Bellec G, Legenstein R, Maass W. \"Spike Frequency Adaptation Supports Network Computations on Temporally Dispersed Information\". eLife. July 2021;10:e65459. journal (url) (preprint) (bibtex) Yegenoglu A, Subramoney A, Hater T, Jimenez-Romero C, Klijn W, Pérez Martı́n Aarón, Vlag M van der, Herty M, Morrison A, Diaz Pier S. \"Exploring parameter and hyper-parameter spaces of neuroscience models on high performance computers with Learning to Learn\". Frontiers in Computational Neuroscience. May 2022;:46. journal (url) (preprint) (bibtex) Subramoney A, Nazeer KK, Schöne M, Mayr C, Kappel D. \"Efficient Recurrent Architectures through Activity Sparsity and Sparse Back-Propagation through Time\". In: International Conference on Learning Representations. 2023. conference Spotlight (notable-top-25%) presentation (url) (preprint) (talk) (bibtex) Subramoney A. \"Efficient Real Time Recurrent Learning through Combined Activity and Parameter Sparsity\". In: ICLR 2023 Workshop: Sparsity in Neural Networks (SNN). arXiv; 2023. workshop (url) (preprint) (bibtex) Grappolini EW, Subramoney A. \"Beyond Weights: Deep learning in Spiking Neural Networks with pure synaptic-delay training\". In: Proceedings of the 2023 International Conference on Neuromorphic Systems. New York, NY, USA: Association for Computing Machinery; 2023. (ICONS ’23). conference (url) (preprint) (bibtex) Mukherji R, Schöne M, Nazeer KK, Mayr C, Subramoney A. \"Activity Sparsity Complements Weight Sparsity for Efficient RNN Inference\". In: NeurIPS 2023 Workshop: ML with New Compute Paradigms (MLNCP). 2023. workshop (preprint) (bibtex) Nazeer KK, Schöne M, Mukherji R, Vogginger B, Mayr C, Kappel D, Subramoney A. \"Language Modeling on a SpiNNaker2 Neuromorphic Chip\". In: 2024 IEEE 6th International Conference on AI Circuits and Systems (AICAS). IEEE; 2024. p. 492–6. conference (preprint) (bibtex) Subramoney A, Bellec G, Scherr F, Legenstein R, Maass W. \"Fast learning without synaptic plasticity in spiking neural networks\". Scientific Reports. April 2024;14(1):8557. journal (url) (preprint) (bibtex) Zhuge J, Mayr C, Subramoney A, Kappel D. \"Single Train Multi Deploy on Topology Search Spaces using Kshot-Hypernet\". In: 2nd Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ ICML 2024). 2024. workshop (url) (bibtex) Mukherji R, Schöne M, Nazeer KK, Mayr C, Kappel D, Subramoney A. \"Weight Sparsity Complements Activity Sparsity in Neuromorphic Language Models\". Proceedings of the 2024 International Conference on Neuromorphic Systems. August 2024; conference Full talk/Oral (url) (preprint) (bibtex) Schöne M, Sushma NM, Zhuge J, Mayr C, Subramoney A, Kappel D. \"Scalable Event-by-event Processing of Neuromorphic Sensory Signals With Deep State-Space Models\". Proceedings of the 2024 International Conference on Neuromorphic Systems. August 2024; conference Best Paper Award (url) (preprint) (bibtex) Sushma NM, Tian Y, Mestha H, Colombo N, Kappel D, Subramoney A. \"State-space models can learn in-context by gradient descent\". arXiv preprint arXiv:241011687. October 2024; preprint (preprint) (bibtex) Fokam CT, Nazeer KK, König L, Kappel D, Subramoney A. \"Asynchronous Stochastic Gradient Descent with Decoupled Backpropagation and Layer-Wise Updates\". arXiv preprint arXiv:241005985. October 2024; preprint (preprint) (bibtex) Schöne M, Bhisikar Y, Bania K, Nazeer KK, Mayr C, Subramoney A, Kappel D. \"STREAM: A Universal State-Space Model for Sparse Geometric Data\". arXiv; 2024. preprint (preprint) (bibtex) Schiewer R, Subramoney* A, Wiskott* L. \"Exploring the Limits of Hierarchical World Models in Reinforcement Learning\". Scientific Reports. November 2024;14(1):26856. journal (url) (preprint) (bibtex) Kudithipudi D, Schuman C, Vineyard CM, Pandit T, Merkel C, Kubendran R, Aimone JB, Orchard G, Mayr C, Benosman R, Hays J, Young C, Bartolozzi C, Majumdar A, Cardwell SG, Payvand M, Buckley S, Kulkarni S, Gonzalez HA, Cauwenberghs G, Thakur CS, Subramoney A, Furber S. \"Neuromorphic Computing at Scale\". Nature. January 2025;637(8047):801–12. journal (url) (bibtex) Subramoney A. \"Efficient Large Language Model with Analog In-Memory Computing\". Nature Computational Science. January 2025;:1–2. journal News & Views article (url) (bibtex) Teguemne Fokam C, Khan Nazeer K, Mayr C, Subramoney A, Kappel D. \"A Variational Framework for Local Learning with Probabilistic Latent Representations\". In: ESANN 2025 Proceedings. Bruges (Belgium) and online: Ciaco - i6doc.com; 2025. p. 165–70. conference Oral (url) (preprint) (bibtex) Sedghi R, Pal AK, Subramoney A, Kappel D. \"Early Prediction of Dynamic Sparsity in Large Language Models\". In: ESANN 2025 Proceedings. Bruges (Belgium) and online: Ciaco - i6doc.",
  "content_length": 12060,
  "method": "requests",
  "crawl_time": "2025-12-01 12:56:18"
}