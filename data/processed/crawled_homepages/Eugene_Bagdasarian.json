{
  "name": "Eugene Bagdasarian",
  "homepage": "https://people.cs.umass.edu/~eugene",
  "status": "success",
  "content": "Eugene Bagdasarian's website Eugene Bagdasarian Assistant Professor at UMass Amherst CICS eugene@umass.edu Copied! Office: E451 Resume Papers About I am an Assistant Professor of Computer Science at UMass Amherst CICS. In our group, we look at security and privacy attack vectors for AI systems deployed in real-life. I co-lead AI Security Lab at UMass with Amir Houmansadr and co-lead AI Safety Initiative with Shlomo Zilberstein. I am also a senior research scientist (part-time) at Google working on agentic privacy and security. I completed my PhD at Cornell Tech advised by Vitaly Shmatikov and Deborah Estrin. My research was recognized by Apple Scholars in AI/ML and Digital Life Initiative fellowships, and Usenix Security Distinguished Paper Award. At UMass, we received Schmidt Sciences AI Safety Grant. Before going to the grad school, I got an engineering degree from Baumanka and worked at Cisco as a software engineer. I grew up in Tashkent and play water polo. Recruitment: I am looking for new PhD students (apply) and post-docs to work on attacks on LLM agents and generative models. Check the new gorgeous UMass CICS Building, our incredible AI Security Lab, and exciting AI Safety Initiative. Please reach out over email and fill the form. Research Agents We proposed AirGapAgent(CCS'24) for privacy and Conseca(HotOS'25) for security of agents leveraging the theory of Contextual Integrity. Recently, we studied prompt leakage in research agents and investigated throttling to prevent denial of service or scraping by web agents. Finally, our Terrarium framework enables careful studies of multi-agent systems. Backdoors We worked on backdoor attacks in federated learning (AISTATS'20) and proposed frameworks Backdoors101 (USENIX Security'20) and Mithridates (CCS'24) for understanding and defending against backdoor attacks. We also studied backdooring bias (USENIX Security'25) into diffusion models. Multi-modal Security We studied vulnerabilities in multi-modal systems including self-interpreting images (USENIX Security'25) and adversarial illusions (USENIX Security'24, Distinguished Paper) that can manipulate vision-language models. Our work demonstrates novel attack vectors in systems that process both visual and textual information. LLM and Reasoning Attacks We developed attacks on large language models including an attack (S&P'22) on generative language models that enables propaganda generation. We recently proposed the OverThink attack on reasoning models that exploits their chain-of-thought mechanisms. Privacy We applied Contextual Integrity to form-filling tasks (TMLR'25) and investigated context ambiguity (NeurIPS'25) in privacy reasoning. We worked on aspects of differential privacy including fairness trade-offs (NeurIPS'19), applications to location heatmaps (PETS'21), and tokenization methods (ACL FL workshop'22) for private federated learning. We built the Ancile (WPES'19) system that enforces use-based privacy of user data. PhD students: Abhinav Kumar, June Jeong (co-advised w Amir Houmansadr), Dzung Pham (co-advised w Amir Houmansadr). Recent collaborators: Amir Houmansadr, Shlomo Zilberstein, Sahar Abdelnabi, Brian Levine, Kyle Wray, Ali Naseh, Jaechul Roh, Mason Nakamura, Saaduddin Mahmud, and many others. Impact Google uses our work on agents (contextual security and privacy system designs) for Secure AI Framework and our paper applying contextual integrity to AI Assistants in the approach to protecting privacy in the age of AI. Our research has been covered by VentureBeat and The Economist. Our work on adversarial illusions received a Distinguished Paper Award at USENIX Security'24. NIST Taxonomy on Adversarial Machine Learning uses our work on Federated Learning and AI Agents for their agenda. Multiple key research roadmaps (e.g., DP and FL ) use our work as a foundation for future studies. Current Funding: Teaching Courses CS 360: Intro to Security, SP 25. CS 692PA: Seminar on Privacy and Security for GenAI models, FA'24, SP'25, FA'25 CS 690: Trustworthy and Responsible AI, FA'25 (link) Service Academic Service Recent Program Committees: ACM CCS'24/'25, ICLR'24/25/26, IEEE S&P'26 Workshop Organizer: Backdoors Workshop at NeurIPS'23 Broadening Participation I co-organize PLAIR (Pioneer Leaders in AI and Robotics), an outreach program that introduces high school students across Western Massachusetts to the world of robotics and AI safety. Please reach out if you are interested in joining. Recent News October 2025 I gave a keynote at the 18th Workshop on AI Security (CCS'25) on protecting Future AI Agents with Contextual Rules. October 2025 We released Terrarium, a framework for multi-agent safety, privacy, and security studies. Play with code! Sep 2025 I gave a seminar talk at the Northeastern Security Group. August 2025 I gave a keynote at the Supa Workshop. June 2025 Two papers accepted to USENIX Security'25: one on backdooring bias and one on self-interpreting images. May 2025 I gave an invited talk at IEEE S&P'25 SAGAI Workshop on contextual defenses for agents. [slides] May 2025 I gave invited talks at Apple PPML Workshop, Brave Research, Service Now. June 2025 Together with Shlomo Zilberstein, we received Schmidt Sciences Grant to study multi-agent safety. [news] March 2025 I gave a tutorial at AAAI'25 PPAI Workshop on contextual integrity for AI agents. March 2025 I gave a keynote at AAAI'25 Deployable AI Workshop about attacks on inference-heavy pipelines. Oct 2024 - Two papers accepted to CCS'24: one on resisting backdoor attacks, and one on privacy-conscious agents. May 2024 â€“ Adversarial Illusions received Distinguished Paper Award at USENIX Security'24.",
  "content_length": 5680,
  "method": "requests",
  "crawl_time": "2025-12-01 13:08:01"
}