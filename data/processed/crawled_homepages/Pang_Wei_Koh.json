{
  "name": "Pang Wei Koh",
  "homepage": "https://koh.pw",
  "status": "success",
  "content": "Pang Wei Koh Our group develops methods for making AI systems more useful, responsible, and reliable in the real world. Our goal is to enable AI to make a positive impact in ways it could not do before, e.g., to accelerate scientific discovery or provide universal access to medical advice. We are part of UW ML and NLP, and I'm also a research scientist at AI2. I received my PhD in Computer Science from Stanford, advised by Percy Liang. Before that, I was the 3rd employee and Director of Partnerships at Coursera. I received my BS/MS from Stanford as well, advised by Andrew Ng and Daphne Koller. Previously, I worked on computational biology with Anshul Kundaje at Stanford and then at Calico Life Sciences. Interested in joining us? Please read this! I am recruiting for the 2025-2026 cycle and am looking for students and postdocs who are doing core ML/NLP research and/or are interested in AI for science broadly construed. Current students Scott Geng PhD student Jacqueline He PhD student (with Luke Zettlemoyer) Rulin Shao PhD student (with Luke Zettlemoyer) Rui Xin PhD student (with Sewoong Oh) Ian Magnusson PhD student (with Noah Smith) Zhiyuan Zeng PhD student (with Hanna Hajishirzi) Joseph Lee PhD student (with Hanna Hajishirzi) Chang Ma Incoming postdoc (with Hanna Hajishirzi) Molly Park Undergrad Yufei Zhang Undergrad Gregory Lau Visiting PhD student Jina Kim Visiting undergrad Alumni Qiao Rui (Visiting PhD 2024, now research scientist at Meta) Irena Gao (MS 2023, now PhD student at Stanford) Kendrick Shen (MS 2022, now ML research engineer at Phonic) Henrik Marklund (MS 2021, now PhD student at Stanford) Kai-Siang Ang (MS 2021, now Technical Lead Manager at Nuro) Erik Jones (MS 2020, now researcher at Anthropic) Hubert Teo (MS 2019, now senior software engineer at CodeSignal) Thao Nguyen (BS 2019, now PhD student at the University of Washington) Yew-Siang Tang (BS 2019, now staff software engineer at You.com) Publications * = equal contribution. DR Tulu: Reinforcement learning with evolving rubrics for deep research Rulin Shao*, Akari Asai*, Shannon Zejiang Shen*, Hamish Ivison*, Varsha Kishore, Jingming Zhuo, Xinran Zhao, Molly Park, Samuel Finlayson, David Sontag, Tyler Murray, Sewon Min, Pradeep Dasigi, Luca Soldaini, Faeze Brahman, Wen-tau Yih, Tongshuang Wu, Luke Zettlemoyer, Yoon Kim, Hannaneh Hajishirzi, and Pang Wei Koh arXiv 2025 (paper) (code) RLVE: Scaling up reinforcement learning for language models with adaptive verifiable environments Zhiyuan Zeng*, Hamish Ivison*, Yiping Wang*, Lifan Yuan*, Shuyue Stella Li, Zhuorui Ye, Siting Li, Jacqueline He, Runlong Zhou, Tong Chen, Chenyang Zhao, Yulia Tsvetkov, Simon Shaolei Du, Natasha Jaques, Hao Peng, Pang Wei Koh, and Hannaneh Hajishirzi arXiv 2025 (paper) (code) Personalized reasoning: Just-in-time personalization and why LLMs fail at it Shuyue Stella Li, Avinandan Bose, Faeze Brahman, Simon Shaolei Du, Pang Wei Koh, Maryam Fazel, and Yulia Tsvetkov arXiv 2025 (paper) Spurious rewards: Rethinking training signals in RLVR Rulin Shao*, Shuyue Stella Li*, Rui Xin*, Scott Geng*, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, and Luke Zettlemoyer arXiv 2025 (paper) (code) Frustratingly simple retrieval improves challenging, reasoning-intensive benchmarks Xinxi Lyu, Michael Duan, Rulin Shao, Pang Wei Koh, and Sewon Min arXiv 2025 (paper) FlexOlmo: Open language models for flexible data use Weijia Shi, Akshita Bhagia, Kevin Farhat, Niklas Muennighoff, Pete Walsh, Jacob Morrison, Dustin Schwenk, Shayne Longpre, Jake Poznanski, Allyson Ettinger, Daogao Liu, Margaret Li, Dirk Groeneveld, Mike Lewis, Wen-tau Yih, Luca Soldaini, Kyle Lo, Noah A Smith, Luke Zettlemoyer, Pang Wei Koh, Hannaneh Hajishirzi, Ali Farhadi, and Sewon Min NeurIPS 2025 Spotlight paper (paper) Precise information control in long-form text generation Jacqueline He, Howard Yen, Margaret Li, Shuyue Stella Li, Zhiyuan Zeng, Weijia Shi, Yulia Tsvetkov, Danqi Chen, Pang Wei Koh, and Luke Zettlemoyer NeurIPS 2025 (paper) (code) The Delta Learning hypothesis: Preference tuning on weak data can yield strong gains Scott Geng, Hamish Ivison, Chun-Liang Li, Maarten Sap, Jerry Li, Ranjay Krishna, and Pang Wei Koh COLM 2025 (paper) EvalTree: Profiling language model weaknesses via hierarchical capability trees Zhiyuan Zeng, Yizhong Wang, Hannaneh Hajishirzi, and Pang Wei Koh COLM 2025 (paper) (code) (website) ReasonIR: Training Retrievers for Reasoning Tasks Rulin Shao*, Rui Qiao*, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-tau Yih, Pang Wei Koh, and Luke Zettlemoyer COLM 2025 (paper) (code) Establishing task scaling laws via compute-efficient model ladders Akshita Bhagia, Jiacheng Liu, Alexander Wettig, David Heineman, Oyvind Tafjord, Ananya Harsh Jha, Luca Soldaini, Noah A Smith, Dirk Groeneveld, Pang Wei Koh, Jesse Dodge, and Hannaneh Hajishirzi COLM 2025 (paper) ParaPO: Aligning language models to reduce verbatim reproduction of pre-training data Tong Chen, Faeze Brahman, Jiacheng Liu, Niloofar Mireshghallah, Weijia Shi, Pang Wei Koh, Luke Zettlemoyer, and Hannaneh Hajishirzi COLM 2025 (paper) 2 OLMo 2 Furious Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi COLM 2025 (paper) (website) Fluid language model benchmarking Valentin Hofmann, David Heineman, Ian Magnusson, Kyle Lo, Jesse Dodge, Maarten Sap, Pang Wei Koh, Chun Wang, Hannaneh Hajishirzi, and Noah A. Smith COLM 2025 Oral presentation (paper) The curious case of factuality finetuning: Models' internal beliefs can improve factuality Benjamin Newman, Abhilasha Ravichander, Jaehun Jung, Rui Xin, Hamish Ivison, Yegor Kuznetsov, Pang Wei Koh, and Yejin Choi arXiv 2025 (paper) (code) A false sense of privacy: Evaluating textual data sanitization beyond surface-level privacy leakage Rui Xin*, Niloofar Mireshghallah*, Shuyue Stella Li, Michael Duan, Hyunwoo Kim, Yejin Choi, Yulia Tsvetkov, Sewoong Oh, Pang Wei Koh arXiv 2025 (paper) DataDecide: How to predict best pretraining data with small experiments Ian Magnusson*, Nguyen Tai*, Ben Bogin*, David Heineman, Jena D Hwang, Luca Soldaini, Akshita Bhagia, Jiacheng Liu, Dirk Groeneveld, Oyvind Tafjord, Noah A Smith, Pang Wei Koh, and Jesse Dodge ICML 2025 (paper) (code) (website) NICE: Non-differentiable evaluation metric-based data selection for instruction tuning Jingtan Wang, Xiaoqiang Lin, Rui Qiao, Pang Wei Koh, Chuan-Sheng Foo, and Bryan Kian Hsiang Low ICML 2025 (paper) OLMoTrace: Tracing language model outputs back to trillions of training tokens Jiacheng Liu, Taylor Blanton, Yanai Elazar, Sewon Min, YenSung Chen, Arnavi Chheda-Kothary, Huy Tran, Byron Bischoff, Eric Marsh, Michael Schmitz, Cassidy Trier, Aaron Sarnat, Jenna James, Jon Borchardt, Bailey Kuehl, Evie Cheng, Karen Farley, Sruthi Sreeram, Taira Anderson, David Albright, Carissa Schoenick, Luca Soldaini, Dirk Groeneveld, Rock Yuren Pang, Pang Wei Koh, Noah A Smith, Sophie Lebrecht, Yejin Choi, Hannaneh Hajishirzi, Ali Farhadi, and Jesse Dodge ACL 2025 Best demo award (paper) (code) (website) Large-scale data selection for instruction tuning Hamish Ivison, Muru Zhang, Faeze Brahman, Pang Wei Koh, and Pradeep Dasigi arXiv 2025 (paper) (code) S4S: Solving for a diffusion model solver Eric Frankel, Sitan Chen, Jerry Li, Pang Wei Koh, Lillian J Ratliff, and Sewoong Oh ICML 2025 (paper) Metabolically purified human stem cell-derived hepatocytes reveal distinct effects of Ebola and Lassa viruses Joseph B Prescott, Kevin J Liu, Angelika Lander, Nicole Min Qian Pek, Sawan Kumar Jha, Marcel Bokelmann, Manali Begur, Pang Wei Koh, Henry Yang, Bing Lim, Kristy Red-Horse, Irving L Weissman, Kyle M Loh, Lay Teng Ang bioRxiv 2025 (paper) Exploring how generative MLLMs perceive more than CLIP with the same vision encoder Siting Li, Pang Wei Koh, and Simon Shaolei Du ACL 2025 (paper) OLMoE: Open Mixture-of-Experts language models Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh Hajishirzi ICLR 2025 (paper) (code) Language models scale reliably with over-training and on downstream tasks Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Luca Soldaini, Alexandros G. Dimakis, Gabriel Ilharco, Pang Wei Koh, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, and Ludwig Schmidt ICLR 2025 (paper) Group-robust sample reweighting for subpopulation shifts via influence functions Rui Qiao, Zhaoxuan Wu, Jingtan Wang, Pang Wei Koh, and Bryan Kian Hsiang Low ICLR 2025 (paper) Use large language models to promote equity Emma Pierson*, Divya Shanmugam*, Rajiv Movva*, Jon Kleinberg*, Monica Agrawal, Mark Dredze, Kadija Ferryman, Judy Wawira Gichoya, Dan Jurafsky, Pang Wei Koh, Karen Levy, Sendhil Mullainathan, Ziad Obermeyer, Harini Suresh, and Keyon Vafa NEJM AI 2025 (paper) ICONS: Influence Consensus for Vision-Language Data Selection Xindi Wu, Mengzhou Xia, Rulin S",
  "content_length": 26659,
  "method": "requests",
  "crawl_time": "2025-12-01 14:08:43"
}