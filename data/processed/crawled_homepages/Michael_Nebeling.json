{
  "name": "Michael Nebeling",
  "homepage": "http://www.michael-nebeling.de",
  "status": "success",
  "content": "Michael Nebeling DR. MICHAEL NEBELING I'm an Associate Professor at the University of Michigan, where I direct the Information Interaction Lab. I'm interested in designing and studying novel methods and tools to create next-generation user interfaces. My focus over the last few years has been on augmented, virtual, and mixed reality (XR) experiences, with the goal of making them easy, safe, and beneficial to use. My research contributions often involve designing and building new toolkits, authoring tools, and interaction techniques, in an effort to balance usability, accessibility, and privacy needs. At UM, I lead the Information Interaction Lab in the School of Information (UMSI). I've also been involved with the Center for Academic Innovation where I created the Extended Reality for Everybody specialization on Coursera (see below). I'm faculty adviser to the student-led Alternate Reality Initiative (ARI). I also co-direct our HCI seminar series as part of the Michigan Interactive and Social Computing (MISC) research group, teach introductory and advanced AR/VR courses (SI 559/659), and lead or contribute to several UM and externally funded research projects. I regularly serve on the program and organizational committees of the ACM CHI, UIST, and EICS conferences. For example, I am going to be UIST 2026 general co-chair with my colleague and friend, Dr. Steve Oney, and CHI 2026 subcommittee co-chair with Dr. Hasti Seifi. Previously, I was EICS 2024 general co-chair, UIST 2021 Papers co-chair, and EICS 2015 papers co-chair. I received a Disney Research Faculty Award (for excellence in science), a Mozilla Research Award (for reinventing the browser as an AR/VR application delivery platform), an Epic MegaGrant (for educational tools in XR), and a Meta Reality Labs gift (for my work on fairness and inclusion in XR). Working with UM's Center for Academic Innovation, I started my role as XR Faculty Innovator-in-Residence with the UM wide XR Initiative in 2019. I created the XR MOOC series, a three-course AR/VR specialization on Coursera. I joined UM in 2016 after completing a postdoc in the Human-Computer Interaction Institute at Carnegie Mellon University and a PhD in the Department of Computer Science at ETH Zurich. Upon my promotion to Associate Professor in 2022, I spent my sabbatical at Meta Reality Labs‐Research. Curriculum Vitae Why XR Research Matters As an HCI researcher, I've become interested in XR in 2016 when I started my new lab at the University of Michigan. I want AR/VR to become useful and effective interaction modes that bring real benefits to users taking into account their context, task, abilities (or disabilities), and preference. In terms of application domains, I'm particularly interested in the role XR technologies can play to improve education and accessibility. Read more... Personally, I found it quite hard to get started in XR and did everything in a learning by doing style. Seeing how my students and I struggled, my mission initially was to lower the barrier to entry in an effort to democratize AR/VR research and design. For a while, I've used the tools and toolkits I created in research to support my own teaching, but it has become hard to maintain them and so I mostly just teach the same principles and methods but with whatever the latest tools are. For a few years, I've taught rapid prototyping for XR courses with colleagues and friends, and it's been fun! Since then, I've branched out and worked on all kinds of XR issues (there are so many!). I've explored both the positive and negative sides of XR. For example, I investigated how XR can support rapid prototyping of novel design concepts [CHI'18a, CHI'19a, CHI'20a], improve instructional experiences [CHI'20b, CHI'21, CHI'22a], make physical environments more inclusive [CHI Play'19, CHI'24W], ensure safer use of XR particularly in social settings [CHI'23, UIST'23], and tell sensitive narratives without overstimulation [DIS'22]. While I'm an enthusiast and see a bright future for XR, in particular AR, I've also looked at the key barriers to entry and major challenges in XR design [CHI'20c, CHI'22b] as well as responsible design for XR [CHI'20W, CSCW'20W, CHI'22W] and how XR could be accidentally or intentionally misused, such as through deceptive design patterns which could lead to more severe outcomes given the immersive qualities of XR [TOCHI'24]. I like reflecting on the role and impact of research and often debate the challenges of working in certain areas or using particular methods. For example, in Playing the Tricky Game of Toolkits Research I discuss the issues I faced finding my research community during my doctoral studies at ETH Zurich. I also like complaining a lot about XR methods and tools (I consider it part of my job :P). For example, in The Trouble with Augmented Reality/Virtual Reality Authoring Tools I do just that. While I like to complain, I also try to help pave a way forward. For example, in XR Tools and Where They Are Taking Us I've analyzed a decade of research on AR/VR tools and identified what I called forward vectors for the research community to pursue. Extended Reality for Everybody (2020) Below is a video I created in celebration of the 5 year anniversary of the launch of my XR MOOC Specialization on Coursera in Fall 2020. It contains lots of demos and is labelled by course and topic demonstrated in each scene. While the Extended Reality for Everybody specialization is now five years old, all the concepts and techniques are still extremely relevant. The majority of tools is still the same, although some have gone and others have joined. The most important topic these days is the role of AI in XR, and this is something that I have been giving lectures about in person, and perhaps also soon online again. Join our XR MOOC specialization on Coursera (free for UM students and alums through Michigan Online)! PhD in Technical HCI Office Hour (2024) I'm always interested in talking to students about doing a PhD in technical HCI. Below is a video of my online office hour with several faculty friends, going through a list of crowdsourced questions that I received from prospective PhD students. I've met a lot of great students this way and really enjoyed this. This year was the 6th edition. You can find the ones from 2019‐2023 in my YouTube playlist. Research at the University of Michigan (2020-today) Since starting the Information Interaction Lab in September 2016, I have developed a new research focus on AR/VR interfaces. My earlier XR research contributed new techniques, tools, and technologies to make AR/VR interface design and development easier and faster. My vision was that anyone without 3D modeling, animation, or programming background can be an active participant in AR/VR design. My initial focus was on rapid prototyping, but I have since expanded scope to promote more accessible and safer XR design. For example, below is a video of Reframe, an AR storyboarding tool prompting interaction designers to address S&P issues by design. This project was led by my PhD student Shwetha Rajaram and presented at UIST 2023. Show more research... Research at the University of Michigan (2016-2020) MRAT (CHI'20 paper): a system to collect user interaction data of AR/VR users and visualize it in-situ using mixed reality visualizations XRDirector (CHI'20 paper): an collaborative immersive authoring system that adapts roles from filmmaking for virtual production of 3D and AR/VR scenes with interactive content 360proto (CHI'19 paper): a 360 paper prototyping tool to rapidly create AR/VR prototypes from paper and bring them to life on AR/VR devices based on a set of emerging paper prototyping templates specifically for AR/VR immersive environments, AR marker overlays and face masks, VR controller models and menus, and 2D screens and HUDs ProtoAR (CHI'18 paper): a rapid physical-digital prototyping tool for mobile augmented reality apps that enables rapid transition from physical prototyping using paper wireframes and Play-doh models to digital prototyping based on multi-layer cross-device authoring and interactive capture tools XD-AR (EICS'18 paper): an augmented reality platform that we are developing for multi-user multi-device augmented reality experiences that allow users to edit the physical world in realtime 360Anywhere (EICS'18 paper): a platform enabling multi-user collaboration scenarios using augmented reality projections of 360 video streams Please find out more on the Information Interaction Lab web site. Research at Carnegie Mellon University (2015-2016) Using my Swiss NSF mobility grant at CMU, I investigated ways of orchestrating multiple devices and crowds to enable complex information seeking, sensemaking and productivity tasks on small mobile and wearable devices. I also contributed to the Google IoT project led by CMU. This work led to three papers at ACM CHI 2016: WearWrite (CHI'16 paper, CHI'16 talk, UIST'15 demo, tech report on arXiv): a wearable interface enabling smartwatch users to orchestrate crowd workers on more powerful devices to complete writing tasks on their behalf XDBrowser (CHI'16 paper, CHI'16 talk, CHI'17 paper, CHI'17 talk): a new cross-device web browser that I used to elicit 144 multi-device web page designs for five popular web interfaces leading to seven cross-device web page design patterns Snap-To-It (CHI'16 paper): a mobile app allowing users to opportunistically interact with appliances in multi-device environments simply by taking a picture of them Research at ETH Zurich (2009-2015) Beyond Responsive Design XCML (WWW journal article): a domain-specific language that tightly integrates context-aware concepts and multi-dimensional adaptivity mechanisms using context matching expressions based on a formally-defined context algebra jQMetrics (CHI'11 paper, jQMetrics on GitHub): an evaluation tool for web developers and designers to analyze web page layout and perform measurements alo",
  "content_length": 26000,
  "method": "requests",
  "crawl_time": "2025-12-01 13:58:42"
}