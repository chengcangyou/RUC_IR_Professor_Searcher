{
  "name": "Andrew Gordon Wilson",
  "homepage": "https://cims.nyu.edu/~andrewgw",
  "status": "success",
  "content": "Andrew Gordon Wilson Note to prospective PhD students: in the upcoming application cycle (admission for 2026-2027), I am primarily interested in the theory and empirical science of deep learning. The following papers are representative of these interests: (1) generalization bounds for understanding scaling laws; (2) the science of scaling; (3) numerical methods for deep learning. A rigorous mathematics or physics training is typically the best fit for research with me. I aim to develop a prescriptive approach to building autonomous intelligent systems. This effort involves a variety of different research initiatives, which cumulatively work together towards achieving this vision. A major theme that unifies many of these initiatives is a desire for an actionable understanding, so that we can select for particular properties aligned with our goals. These areas, and some example papers, include: • Understanding deep learning models, including LLMs and vision models, generalization theory, and reasoning [e.g., 1, 2, 3, 4, 29, 30, 31] • Uncertainty representation, Bayesian methods, online decision making [e.g., 1, 5, 6, 7] • Distribution shifts, spurious correlations [e.g., 8, 9, 10, 11] • Encoding and learning inductive biases (e.g., equivariances) [e.g., 12, 13, 14, 15] • Linear algebra as a foundation for ML [e.g., 16, 33, 34, 17, 18, 19, 20] • Machine learning for physics, and physics for ML [e.g., 21, 22, 13, 20, 15] • Simple practical methods [e.g., 23, 24, 25, 26, 4] • Scientific discovery (protein engineering, materials design) [e.g., 27, 28, 32] Outside of work, I am a classical pianist who particularly enjoys Glenn Gould's playing of Bach. I can be reached at andrewgw@cims.nyu.edu, and on Bluesky and Twitter as @andrewgwils. Andrew Gordon Wilson Professor Courant Institute of Mathematical Sciences and Center for Data Science 60 Fifth Ave New York University Papers Google scholar page This listing of papers is out of date. Please see my CV for an updated list. PAC-Bayes Compression Bounds So Tight That They Can Explain Generalization Sanae Lotfi, Marc Finzi, Sanyam Kapoor, Andres Potapczynski, Micah Goldblum, Andrew Gordon Wilson Advances in Neural Information Processing Systems (NeurIPS), 2022. [PDF (Coming Soon!), arXiv (Coming Soon!), code (Coming Soon!), BibTeX] Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors Ravid Shwartz-Ziv, Micah Goldblum, Hossein Souri, Sanyam Kapoor, Chen Zhu, Yann LeCun, Andrew Gordon Wilson Advances in Neural Information Processing Systems (NeurIPS), 2022. [PDF, arXiv, code, BibTeX] On Feature Learning in the Presence of Spurious Correlations Pavel Izmailov, Polina Kirichenko, Nate Gruver, Andrew Gordon Wilson Advances in Neural Information Processing Systems (NeurIPS), 2022. [PDF, arXiv, code, BibTeX] On Uncertainty, Tempering, and Data Augmentation in Bayesian Classification Sanyam Kapoor, Wesley Maddox, Pavel Izmailov, Andrew Gordon Wilson Advances in Neural Information Processing Systems (NeurIPS), 2022. [PDF, arXiv, code, BibTeX] Chroma-VAE: Mitigating Shortcut Learning with Generative Classifiers Wanqian Yang, Polina Kirichenko, Micah Goldblum, Andrew Gordon Wilson Advances in Neural Information Processing Systems (NeurIPS), 2022. [PDF (Coming Soon), arXiv (Coming soon), code (Coming Soon), BibTeX] Bayesian Model Selection, the Marginal Likelihood, and Generalization Sanae Lotfi, Pavel Izmailov, Gregory Benton, Micah Goldblum, Andrew Gordon Wilson International Conference on Machine Learning (ICML), 2022. Outstanding Paper Award [PDF, arXiv, code, BibTeX] Accelerating Bayesian Optimization for Biological Sequence Design with Denoising Autoencoders Samuel Stanton, Wesley Maddox, Nate Gruver, Phil Maffettone, Emily Delaney, Peyton Greenside, Andrew Gordon Wilson International Conference on Machine Learning (ICML), 2022 [PDF, arXiv, code, BibTeX] Low-Precision Stochastic Gradient Langevin Dynamics Ruqi Zhang, Andrew Gordon Wilson, Christopher De Sa International Conference on Machine Learning (ICML), 2022 [PDF, arXiv, code, BibTeX] Volatility Based Kernels and Moving Average Means for Accurate Forecasting with Gaussian Processes Gregory Benton, Wesley Maddox, Andrew Gordon Wilson International Conference on Machine Learning (ICML), 2022 [PDF, arXiv, code, BibTeX] Low Precision Arithmetic for Fast Gaussian Processes Wesley Maddox, Andres Potapczynski, Andrew Gordon Wilson Uncertainty in Artificial Intelligence (UAI), 2022 [PDF, arXiv, code, BibTeX] Harnessing Interpretable and Unsupervised Machine Learning to Address Big Data from Modern X-ray Diffraction Jordan Venderley et. al Proceedings of the National Academy of Sciences (PNAS), 2022 [PDF, arXiv, code, BibTeX] Deconstructing the Inductive Biases of Hamiltonian Neural Networks Nate Gruver, Marc Finzi, Andrew Gordon Wilson International Conference on Learning Representations (ICLR), 2022 [PDF, arXiv, code, BibTeX] Dangers of Bayesian Model Averaging under Covariate Shift Pavel Izmailov, Patrick Nicholson, Sanae Lotfi, Andrew Gordon WilsonAdvances in Neural Information Processing Systems (NeurIPS), 2021 [PDF, arXiv, code, BibTeX] Bayesian Optimization with High-Dimensional Outputs Wesley Maddox, Max Balandat, Andrew Gordon Wilson, Eytan Bakshy.Advances in Neural Information Processing Systems (NeurIPS), 2021 [PDF, arXiv, code, BibTeX] Does Knowledge Distillation Really Work?Samuel Stanton, Pavel Izmailov, Polina Kirichenko, Alexander Alemi, Andrew Gordon WilsonAdvances in Neural Information Processing Systems (NeurIPS), 2021 [PDF, arXiv, BibTeX] Residual Pathway Priors for Soft Equivariance Constraints Marc Finzi, Gregory Benton, Andrew Gordon WilsonAdvances in Neural Information Processing Systems (NeurIPS), 2021 [PDF (Coming Soon), arXiv (Coming Soon), code (Coming Soon), BibTeX] Conditioning Sparse Variational Gaussian Processes for Online Decision-making Samuel Stanton, Pavel Izmailov, Polina Kirichenko, Andrew Gordon WilsonAdvances in Neural Information Processing Systems (NeurIPS), 2021 [PDF (Coming Soon), arXiv (Coming Soon), code (Coming Soon), BibTeX] What Are Bayesian Neural Network Posteriors Really Like?Pavel Izmailov, Sharad Vikram, Matthew D. Hoffman, Andrew Gordon WilsonInternational conference on Machine Learning (ICML), 2021Long oral presentation [PDF, arXiv, code, BibTeX] A Practical Method for Constructing Equivariant Multilayer Perceptrons for Arbitrary Matrix GroupsMarc Finzi, Max Welling, Andrew Gordon WilsonInternational Conference on Machine Learning (ICML), 2021Long oral presentation [PDF, arXiv, code, documentation, examples, BibTeX] SKIing on Simplices: Kernel Interpolation on the Permutohedral Lattice for Scalable Gaussian ProcessesSanyam Kapoor, Marc Finzi, Alex Wang, Andrew Gordon WilsonInternational Conference on Machine Learning (ICML), 2021Long oral presentation [PDF, arXiv, code, BibTeX] Loss Surface Simplexes for Mode Connecting Volumes and Fast EnsemblingGreg Benton, Wesley Maddox, Sanae Lotfi, Andrew Gordon WilsonInternational Conference on Machine Learning (ICML), 2021[PDF, arXiv, code, BibTeX] Scalable Variational Gaussian Processes via Harmonic Kernel DecompositionShengyang Sun, Jiaxin Shi, Andrew Gordon Wilson, Roger GrosseInternational Conference on Machine Learning (ICML), 2021[PDF, arXiv, code, BibTeX] On the model-based stochastic value gradient for continuous reinforcement learning Brandon Amos, Sam Stanton, Denis Yarats, Andrew Gordon WilsonLearning for Dynamics and Control (L4DC), 2021[PDF, code, BibTeX] Kernel Interpolation for Scalable Online Gaussian ProcessesSam Stanton, Wesley Maddox, Ian Delbridge, Andrew Gordon WilsonArtificial Intelligence and Statistics (AISTATS), 2021[PDF, arXiv, code, BibTeX] Fast Adaptation with Linearized Neural NetworksWesley Maddox, Shuai Tang, Pablo Moreno, Andrew Gordon Wilson, Andreas DamianouArtificial Intelligence and Statistics (AISTATS), 2021[PDF, arXiv, code, BibTeX] Bayesian Deep Learning and a Probabilistic Perspective of GeneralizationAndrew Gordon Wilson, Pavel IzmailovAdvances in Neural Information Processing Systems (NeurIPS), 2020[PDF, arXiv, code, BibTeX] Why Normalizing Flows Fail to Detect Out-of-Distribution DataPolina Kirichenko, Pavel Izmailov, Andrew Gordon WilsonAdvances in Neural Information Processing Systems (NeurIPS), 2020[PDF, arXiv, code, BibTeX] Simplifying Hamiltonian and Lagrangian Neural Networks via Explicit ConstraintsMarc Finzi, Ke Alexander Wang, Andrew Gordon WilsonAdvances in Neural Information Processing Systems (NeurIPS), 2020[PDF, arXiv, code, BibTeX] Learning Invariances in Neural NetworksGreg Benton, Marc Finzi, Pavel Izmailov, Andrew Gordon WilsonAdvances in Neural Information Processing Systems (NeurIPS), 2020[PDF, arXiv, code, BibTeX] BoTorch: A Framework for Efficient Monte-Carlo Bayesian OptimizationMaximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew Gordon Wilson, Eytan BakshyAdvances in Neural Information Processing Systems (NeurIPS), 2020[PDF, arXiv, code, BibTeX] Improving GAN Training with Probability Ratio Clipping and Sample ReweightingYue Wu, Pan Zhou, Andrew Gordon Wilson, Eric Xing, Zhiting HuAdvances in Neural Information Processing Systems (NeurIPS), 2020[PDF, arXiv, code, BibTeX] Generalizing Convolutional Networks for Equivariance to Lie Groups on Arbitrary Continuous Data Marc Finzi, Samuel Stanton, Pavel Izmailov, Andrew Gordon WilsonInternational Conference on Machine Learning (ICML), 2020[PDF, arXiv, code, BibTeX] Semi-Supervised Learning with Normalizing Flows Pavel Izmailov, Polina Kirichenko, Marc Finzi, Andrew Gordon Wilson International Conference on Machine Learning (ICML), 2020[PDF, arXiv, code, BibTeX] Randomly Projected Additive Gaussian Processes for Regression Ian Delbridge, David S. Bindel, Andrew Gordon Wilson International Conference on Machine Learning (ICML), 2020[PDF, arXiv, code, BibTeX] Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, ",
  "content_length": 21271,
  "method": "requests",
  "crawl_time": "2025-12-01 12:57:31"
}