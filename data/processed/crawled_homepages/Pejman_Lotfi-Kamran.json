{
  "name": "Pejman Lotfi-Kamran",
  "homepage": "http://cs.ipm.ac.ir/~plotfi",
  "status": "success",
  "content": "Pejman Lotfi-Kamran's Homepage Pejman Lotfi-Kamran Associate Professor School of Computer Science, IPM Farmanieh Building, Tehran, 19538-33511 Email: plotfi AT ipm DOT ir Main Research Publications Teaching Awards Hobbies I am an associate professor of computer science, the head of the School of Computer Science, and the director of Turin Cloud Services at the Institute for Research in Fundamental Sciences (IPM). I am interested in computer architecture, computer systems, approximate computing, and cloud computing (CV). I earned my Ph.D. in Computer Science from EPFL in 2013, and my Masters and Bachelors in Electrical and Computer Engineering from the University of Tehran in 2005 and 2002, respectively. I am interested in cross-stack and technology-driven innovations for improving performance and energy efficiency of computer systems for big-data applications. Students Projects Tools and Benchmark Suites Press Talks Students Paria Darbani (with Prof. Hakem Beitollahi) Alumni Ali Ansari (with Prof. Hamid Sarbazi-Azad) Mohammad Bakhshalipour (with Prof. Hamid Sarbazi-Azad) Farid Samandi (with Prof. Hamid Sarbazi-Azad) Projects Divide and Conquer Frontend Bottleneck The frontend stalls caused by instruction and BTB misses are a significant source of performance degradation in server processors. Prefetchers are commonly employed to mitigate frontend bottleneck. However, next-line prefetchers, which are available in server processors, are incapable of eliminating a considerable number of L1 instruction misses. Temporal instruction prefetchers, on the other hand, effectively remove most of the instruction and BTB misses but impose significant area overhead. Recently, an old idea of using BTB-directed instruction prefetching is revived to address the limitations of temporal instruction prefetchers. While this approach leads to prefetchers with low area overhead, it requires significant changes to the frontend of a processor. Moreover, as this approach relies on the BTB content for prefetching, BTB misses stall the prefetcher, and likely lead to costly instruction misses. In this work, we divide the frontend bottleneck into three categories and use a divide-and-conquer approach to propose simple and effective solutions for each one. Our proposal, SN4L+Dis+BTB, imposes the same area overhead as the state-of-the-art BTB-directed prefetcher, and at the same time, outperforms it by 5% on average and up to 16%. [ISCA'20] [SRC] Bingo Spatial Data Prefetcher To select an access pattern for prefetching, existing spatial prefetchers associate observed access patterns to either a short event with a high probability of recurrence or a long event with a low probability of recurrence. Consequently, the prefetchers either offer low accuracy or lose significant prediction opportunities. In this work, we make a case for associating the observed spatial patterns to both short and long events to achieve high accuracy while not losing prediction opportunities. We propose Bingo spatial data prefetcher in which short and long events are used to select the best access pattern for prefetching. We design Bingo in such a way that just one history table is needed to maintain the association between the access patterns and the long and short events. We show that Bingo improves system performance by 60% over a baseline with no data prefetcher and 11% over the best-performing prior spatial data prefetcher. [HPCA'19] Domino Temporal Data Prefetcher While temporal prefetching techniques are effective at reducing the number of data misses, there is a significant gap between what they offer and the opportunity. This work aims to improve the effectiveness of temporal prefetching techniques. We identify the lookup mechanism of existing temporal prefetchers responsible for the large gap between what they offer and the opportunity. Existing lookup mechanisms either not choose the right stream in the history, or unnecessarily delay the stream selection, and hence, miss the opportunity at the beginning of every stream. In this work, we introduce Domino prefetcher that logically looks up the history with both one and two last miss addresses to find a match for prefetching. We show that Domino prefetcher captures more than 90% of the temporal opportunity and offers 16% higher performance over a system with no data prefetcher and 6% over the state-of- the-art temporal data prefetcher. [HPCA'18] Near-Ideal Networks-on-Chip for Servers Most of the NOC delay is due to per-hop resource allocation. In this work, we take advantage of proactive resource allocation (PRA) to eliminate per-hop resource allocation time in single-cycle multi-hop networks to reach a near-ideal network for servers. PRA is undertaken during (1) the time interval in which it is known that LLC has the requested data, but the data is not yet ready, and (2) the time interval in which a packet is stalled in a router because the required resources are dedicated to another packet. Through detailed evaluation targeting a 64-core processor and a set of server workloads, we show that our proposal improves system performance by 12% over the state-of-the-art single-cycle multi-hop mesh NOC. [HPCA'17] Neural Accelerators for Graphics Processing Units Graphics processing units (GPUs) accelerate the execution of diverse classes of applications, such as recognition, gaming, data analytics, weather prediction, and multimedia. Many of these applications are amenable to approximate execution. This application characteristic provides an opportunity to improve the performance and efficiency of GPUs. We studied the effectiveness of neural approximate acceleration for GPU workloads. We showed that applying CPU neural accelerators to GPUs leads to high area overhead. Therefore, we defined a low overhead neurally accelerated architecture for GPUs that enables scalable integration of neural acceleration on the large number of GPU cores. Our evaluation indicates 2.4Ã speed up and 2.8Ã energy reduction with 10% quality loss and less than 1% area overhead. [MICRO'15] Organization of Scale-Out Processors I proposed NOC-Out, a many-core organization for Scale-Out Processors that has low area overhead and provides fast access to the last-level cache (LLC) for delivering high performance. While existing many-core organizations offer an uneasy compromise between low area overhead and fast access to the LLC, NOC-Out offers both features simultaneously. The proposed organization is based on one critical observation: there is almost no core-to-core communication in scale-out workloads. Based on this observation, NOC-Out decouples cores and the last-level cache, eliminates all unneeded core-to-core links, and uses specialized core-to-LLC networks to connect cores to the last-level cache and vice versa. [MICRO'12] Scale-Out Processors I proposed a methodology for the design of highly efficient many-core processors for scale-out workloads. This research relies on two critical observations with regard to such many-core processors. First, large LLCs waste precious silicon real estate. Second, the organization of a many-core processor has a significant impact on its performance. Existing many-core chips, such as those offered by Tilera, sacrifice much of the on-die real estate to LLC and employ a tiled organization that incurs a high on-chip communication overhead. In contrast, I proposed a many-core processor based on the notion of pods. A pod is a module that tightly couples many cores to a modestly sized LLC through a low-latency interconnect. The proposed processor integrates many pods wherein each pod is a self-contained server-on-a-chip running a full software stack. I formulated a methodology to determine the optimal number of cores and LLC capacity to integrate in a pod for peak throughput. The proposed design, called the Scale-Out Processor, delivers peak throughput in today's process technology and affords near-ideal scalability as the technology scales. [ISCA'12] Area- and Energy-Efficient Coherence Directories for Many-Core CMPs We proposed the Cuckoo directory, an energy- and area-efficient scalable directory organization. Existing directory organizations suffer from the lack of energy or area efficiency at high core counts due to wide associative lookups or capacity overprovisioning. Rather than significantly over-provisioning storage capacity to avoid storage conflicts in a traditional lookup table, the Cuckoo directory uses the Cuckoo Hashing algorithm to relocate conflicting entries within the directory to alternate non-conflicting locations. Leveraging the mathematically robust properties of Cuckoo Hashing enables compact and energy-efficient coherence directories with predictable asymptotic behavior. [HPCA'11] Lookup Filtering to Reduce Power Consumption of Coherence Directories I proposed the TurboTag filtering mechanism to reduce the energy usage of coherence directories in many-core processors. Coherence directories dissipate a significant fraction of their power on unnecessary lookups when running commercial server and scientific workloads. As coherence enforcement is a known performance bottleneck of multi-threaded software, data sharing in optimized high-performance software is minimal. Consequently, the majority of the accesses to the coherence directory find no sharers in the directory because the data are not available in the on-chip private caches, effectively wasting power on the coherence checks. TurboTag reduces power consumption of coherence directories by filtering (almost all) needless directory lookups. [ISLPED'10] Cost-Effective Globally Aware Dynamic Routing Protocols to Avoid Congestion in NOCs I proposed low-cost adaptive routing algorithms for network-wide congestion avoidance. This research enhanced conventional adaptive routings that only rely on local indicators for congestion estimation with low-cost, non-local (global) indicators. [JSA'10] Data Structure for Efficient RT-Level Represe",
  "content_length": 21332,
  "method": "requests",
  "crawl_time": "2025-12-01 14:10:50"
}