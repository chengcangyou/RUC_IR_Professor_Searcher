{
  "name": "Elmar Nöth",
  "homepage": "https://lme.tf.fau.de/person/noeth",
  "status": "success",
  "content": "Elmar Nöth - Pattern Recognition Lab Simulate organization breadcrumb open Simulate organization breadcrumb close Friedrich-Alexander-Universität Pattern Recognition Lab PRL Elmar Nöth Elmar Nöth Prof. Dr.-Ing. Elmar NöthDepartment of Computer ScienceChair of Computer Science 5 (Pattern Recognition)Room: Room 09.136Martensstrasse 391058 ErlangenGermanyPhone number: +49 9131 85-27888Fax number: +49 9131 85-27270Email: elmar.noeth@fau.deWebsite: https://lme.tf.fau.de/person/noeth/ Academic CV Since 02/1985: Researcher at LME Since 10/1977: Student at FAU & MIT Projects 2018 Deep Learning Applied to Animal Linguistics (FAU Funds) Term: April 1, 2018 - April 1, 2022 Abstract Deep Learning Applied to Animal Linguistics in particular the analysis of underwater audio recordings of marine animals (killer whales): For marine biologists, the interpretation and understanding of underwater audio recordings is essential. Based on such recordings, possible conclusions about behaviour, communication and social interactions of marine animals can be made. Despite a large number of biological studies on the subject of orca vocalizations, it is still difficult to recognize a structure or semantic/syntactic significance of orca signals in order to be able to derive any language and/or behavioral patterns. Due to a lack of techniques and computational tools, hundreds of hours of underwater recordings are still manually verified by marine biologists in order to detect potential orca vocalizations. In a post process these identified orca signals are analyzed and categorized. One of the main goals is to provide a robust and automatic method which is able to automatically detect orca calls within underwater audio recordings. A robust detection of orca signals is the baseline for any further and deeper analysis. Call type identification and classification based on pre-segmented signals can be used in order to derive semantic and syntactic patterns. In connection with the associated situational video recordings and behaviour descriptions (provided by several researchers on site) can provide potential information about communication (kind of a language model) and behaviors (e.g. hunting, socializing). Furthermore, orca signal detection can be used in conjunction with a localization software in order to provide researchers on the field with a more efficient way of searching the animals as well as individual recognition. For more information about the DeepAL project please contact christian.bergler@fau.de. →More information Modelling the progression of neurological diseases (Third Party Funds Group – Sub project) Overall project: Training Network on Automatic Processing of PAthological SpeechTerm: since May 1, 2018Funding source: Innovative Training Networks (ITN) Abstract Develop speech technology that can allow unobtrusive monitoring of many kinds of neurological diseases. The state of a patient can degrade slowly between medical check-ups. We want to track the state of a patient unobtrusively without the feeling of constant supervision. At the same time the privacy of the patient has to be respected. We will concentrate on PD and thus on acoustic cues of changes. The algorithms should run on a smartphone, track acoustic changes during regular phone conversations over time and thus have to be low-resource. No speech recognition will be used and only some analysis parameters of the conversation are stored on the phone and transferred to the server. →More information 2017 Development of a digital therapy tool as an exercise supplement for speech disorders and facial paralysis (Third Party Funds Single) Term: June 1, 2017 - December 31, 2019Funding source: Bundesministerium für Wirtschaft und Energie (BMWE) Abstract Dysarthrien sind neurologisch bedingte, erworbene Störungen des Sprechens. Dabei sind vor allem die Koordination und Ausführung der Sprechbewegungen, aber auch die Mimik betroffen. Besonders häufig tritt eine Dysarthrie nach einem Schlaganfall, Schädel-Hirn-Trauma oder bei neurologischen Erkrankungen wie Parkinson auf. Ähnlich wie in allen Sprechtherapien erfordert auch die Behandlung der Dysarthrie ein intensives Training. Anhaltende Effekte der Dysarthrie-Therapie stellen sich deshalb nur nach einem umfangreichen Behandlungsprogramm über mehrere Wochen hinweg ein. Bisher gibt es jedoch kaum Möglichkeiten zur Selbstkontrolle für Patienten noch therapeutische Anleitung in einem häuslichen Umfeld. Auch die Rückmeldung an Ärzte / Therapeuten über den Therapieerfolg ist eher lückenhaft. Das Projekt DysarTrain setzt genau hier an und will ein interaktives, digitales Therapieangebot für das Sprechtraining schaffen, damit Patienten ihre Übungen im häuslichen Umfeld durchführen können. In enger Abstimmung mit Ärzten, Therapeuten und Patienten werden zuerst die passenden Therapieinhalte zur Behandlung von Dysarthrien ausgewählt und digitalisiert. In einem zweiten Schritt wird eine Therapieplattform mit den geeigneten Kommunikations-, Interaktions- und Supervisionsfunktionen aufgebaut. Für die Durchführung des Trainings werden anschließend Assistenzfunktionen und Feedbackmechanismen entwickelt. Das Programm soll automatisch rückmelden, ob eine Übung gut absolviert wurde und was ggf. noch verbessert werden kann. Eine automatisierte Auswertung der Therapiedaten erlaubt es Ärzten und Therapeuten, die Therapieform auf möglichst einfache Weise zu individualisieren und an den jeweiligen Therapiestand anzupassen. Dieses Angebot wird mit Ärzten, Therapeuten und Patienten in den Behandlungsprozess integriert und evaluiert. →More information 2010 Automatisches Lerner-Feedback-System (Third Party Funds Single) Term: July 1, 2010 - December 31, 2011Funding source: Bundesministerium für Wirtschaft und Energie (BMWE) Abstract AUWL (Automatisches webbasiertes Lerner-Feedback-System) ist der Nachfolger des Forschungsprojekts C-AuDiT, mit den Beteiligten digital publishing und FAU. Ziel ist die Entwicklung von Methoden für Aussprache- und Dialogtraining für das Fremdsprachenlernen am Beispiel von Englisch als Fremdsprache. Den Lernern wird ein dialogue of the day präsentiert, bei dem man die unterschiedlichen Rollen einnehmen und üben kann, indem man z.B. die Äußerung eines Tutors nachspricht (parroting) oder mitspricht (shadowing). Die Aussprache der Lerner wird automatisch bewertet, und das Ergebnis an die Lerner zurückgemeldet. Neben der Entwicklung neuer Methoden zur Aussprachebewertung liegt der zweite Forschungsschwerpunkt darauf, wie geeignetes feedback automatisch erstellt werden kann, das auf die speziellen Befürfnisse und Probleme des Lerners eingeht. →More information Automatische Analyse von Lautbildungsstörungen bei Kindern und Jugendlichen mit Lippen-Kiefer-Gaumenspalten (LKG) (Third Party Funds Single) Term: April 1, 2010 - March 31, 2013Funding source: DFG-Einzelförderung / Sachbeihilfe (EIN-SBH) Abstract Zur Bewertung von Sprechstörungen von Patienten mit Lippen-Kiefer-Gaumenspalten fehlen bisher objektive, validierte und einfache Verfahren. Im klinischen Alltag werden Lautbildungsstörungen bisher üblicherweise durch eine subjektive, auditive Bewertung erfasst. Diese ist für die klinische und v.a. wissenschaftliche Nutzung nur bedingt geeignet. Die automatische Sprachanalyse, wie sie für Spracherkennungssysteme genutzt wird, hat sich bereits bei Stimmstörungen als objektive Methode der globalen Bewertung erwiesen, nämlich zur Quantifizierung der Verständlichkeit. Dies ließ sich in Vorarbeiten auch auf Sprachaufnahmen von Kindern mit Lippen-Kiefer-Gaumenspalten übertragen. In dem vorliegenden Projekt wird ein Verfahren zur automatischen Unterscheidung und Quantifizierung verschiedener typischer Lautbildungsstörung wie Hypernasalität, Verlagerung der Artikulation und Veränderung der Artikulationsspannung bei Kindern und Jugendlichen mit Lippen-Kiefer-Gaumenspalten entwickelt und validiert. Dies stellt die Basis für die Ermittlung ihres Einflusses auf die Verständlichkeit sowie zur Erfassung der Ergebnisqualität verschiedener therapeutischer Konzepte dar. →More information Publications 2026 Conference Contributions Mercado-Agudelo JF., Escobar-Grisales D., Ríos-Urrego CD., García AM., Bocanegra Y., Moreno L., Nöth E., Orozco-Arroyave JR.:Verb Motility Dynamics Reveals Cognitive Impairment in Parkinson’s Disease: A Speech-Language Fusion Approach28th International Conference on Text, Speech, and Dialogue, TSD 2025 (Erlangen, DEU, August 25, 2025 - August 28, 2025)In: Kamil Ekštein, Miloslav Konopík, Ondrej Pražák, František Pártl (ed.): Lecture Notes in Computer Science 2026DOI: 10.1007/978-3-032-02551-7_25BibTeX: Download 2025 Journal Articles Perez Toro PA., Dineley J., Iniesta R., Zhang Y., Matcham F., Siddi S., Lamers F., Haro JM., Penninx BW., Folarin AA., Arias-Vergara T., Orozco-Arroyave JR., Nöth E., Maier A., Wykes T., Vairavan S., Dobson R., Narayan VA., Hotopf M., Cummins N.:Exploring biases related to the use of large language models in a multilingual depression corpusIn: Scientific Reports 15 (2025), Article No.: 36197ISSN: 2045-2322DOI: 10.1038/s41598-025-19980-xBibTeX: Download Barnhill A., Towers JR., Shaw T., Arias M., Becares A., Doniol-Valcroze T., von Fersen L., Genoves R., Rörup T., Sutton GJ., Thornton S., Weiss M., Maier A., Nöth E., Bergler C.:Advances in deep learning-driven photo identification and meta analysis of cetaceans in large data repositoriesIn: Ecological Informatics 91 (2025), Article No.: 103396ISSN: 1574-9541DOI: 10.1016/j.ecoinf.2025.103396BibTeX: Download Conference Contributions Hemmerling D., Zakrzewski M., Wodzinski M., Dudek M., Gaciarz F., Wojcik-Pedziwiatr M., Orozco-Arroyave JR., Nöth E., Sztaho D., Rumezhak T.:Improving AI Interpretability for Multilingual Parkinson’s Disease Classification through Voice Analysis1st AAAI Bridge Program on AI for Medicine and Healthcare (Philadelphia, PA, USA, February 25, 2025)In: Junde Wu, Jiayuan Zhu, Min Xu, Yueming Jin (ed.): Proceedings of Machine Learni",
  "content_length": 181891,
  "method": "requests",
  "crawl_time": "2025-12-01 13:06:10"
}