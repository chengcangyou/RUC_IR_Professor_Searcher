{
  "name": "Tianxing He",
  "homepage": "https://iiis.tsinghua.edu.cn/en/People/Faculty/HeTianxing.htm",
  "status": "success",
  "content": "ï»¿ He Tianxing-Institute for Interdisciplinary Information Sciences (IIIS) Home IIIS Headlines Institute Seminars About IIIS Introduction Message From Dean Current Leadership History News Latest News Media Coverage People Faculty Research Honorary Professors Adjunct Instructors Postdocs Administrative Staff Lab Assistants Yao Class About Yao Class Admission Graduate Graduate Programs Graduate Admission CQI Introduction Faculty Research Overview CQI News Research Organization Research Groups Latest Research Research Report Campus Campus Events Student Life Join Us Faculty Recruitment Postdoctoral Recruitment Staff Recruitment Alumni IIIS Alumni Community Giving People Faculty Research Honorary Professors Adjunct Instructors Postdocs Administrative Staff Lab Assistants Tianxing He AI Security, AI Simulation, and AI in Games I have joined THU-IIIS (Institute for Interdisciplinary Information Science, Tsinghua University) as AP in Sep 2024. I plan to work on AI + simulation / gaming / safety. I also work part-time at Shanghai Qi Zhi Institute. My UW email no longer works (July 1 2024)! Please use my tsinghua email if you want to contact. About Graduate Admission in 2025 Long-term available : Full-time research intern positions at the Shanghai Qi Zhi Institute. Students planning to take a gap year are especially encouraged to apply. In 2024 I was a postdoc at UW, supervised by Yulia Tsvetkov, who runs the Tsvetshop. A while ago, I was a PhD student at MIT, supervised by Prof. James Glass, who runs the SLS group. I did my bachelor and master degree at Shanghai Jiao Tong University, and my research there was supervised by Prof. Kai Yu, who runs the SJTU SpeechLab. At SJTU I was in the ACM honored class. Twitter: https://x.com/TianxingH Research Interests: AI-related simulation, gaming, or safety. Most of my works during my PhD is focused on neural language generation.Projects: Listed below. Representative papers are highlighted (they are also the projects I lead). * means co-first-author. Towards Black-Box Membership Inference Attack for Diffusion ModelsJingwei Li, Jing Dong, Tianxing He, Jingzhao ZhangArxivWe propose a surprisingly simple black-box MIA algorithm for diffusion models. Learning Syntax without Planting Trees: Understanding When and Why Transformers Generalize HierarchicallyKabir Ahuja, Vidhisha Balachandran, Madhur Panwar, Tianxing He, Noah A.Smith, Navin Goyal, Yulia TsvetkovArxivWe extensively experiment with transformer models trained on multiple synthetic datasets and with different training objectives and show that while other objectives e.g., sequence-to-sequence modeling, prefix language modeling, often failed to lead to hierarchical generalization, models trained with the language modeling objective consistently learned to generalize hierarchically. MiniAgents: A Visualization Interface for SimulacraTianxing HeDemo project. Currently only Mac is supported.I'm using the Unity game engine to build a visualization interface for simulacra. It's now functional and you are welcome to try it. It's still a work-in-progress. Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under AttacksYichen Wang, Shangbin Feng, Abe Bohan Hou, Xiao Pu, Chao Shen, Xiaoming Liu, Yulia Tsvetkov, Tianxing HeACL 2024We comprehensively study the robustness of popular machine-generated text detectors under attacks from diverse categories: editing, paraphrasing, prompting, and co-generating. On the Blind Spots of Model-Based Evaluation Metrics for Text GenerationTianxing He*, Jingyu Zhang*, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James Glass, Yulia TsvetkovACL 2023, selfcontained-oral-slideIn this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. Further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation. SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text GenerationAbe Bohan Hou*, Jingyu Zhang*, Tianxing He*, Yichen Wang, Yung-Sung Chuang, Hongwei Wang, Lingfeng Shen, Benjamin Van Durme, Daniel Khashabi, Yulia TsvetkovNAACL 2024Existing watermarking algorithms are vulnerable to paraphrase attacks because of their token-level design. To address this issue, we propose SemStamp, a robust sentence-level semantic watermarking algorithm based on locality-sensitive hashing (LSH), which partitions the semantic space of sentences. LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on CloudMengke Zhang*, Tianxing He*, Tianle Wang, Lu Mi, Fatemehsadat Mireshghallah, Binyi Chen, Hao Wang, Yulia TsvetkovNAACL 2024 FindingsIn the current user-server interaction paradigm for prompted generation, there is zero option for users who want to keep the generated text to themselves. We propose LatticeGen, a cooperative framework in which the server still handles most of the computation while the user controls the sampling operation. In the end, the server does not know what exactly is generated. The key idea is that the true generated sequence is mixed with noise tokens by the user and hidden in a noised lattice. k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated TextAbe Bohan Hou, Jingyu Zhang, Yichen Wang, Daniel Khashabi, Tianxing HeACL-Findings 2024We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means clustering as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language ModelsShangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, Yulia TsvetkovICLR 2024Reviewer Scores: 8/8/8We propose Knowledge Card, a community-driven initiative to empower black-box LLMs with modular and collaborative knowledge. By incorporating the outputs of independently trained, small, and specialized LMs, we make LLMs better knowledge models by empowering them with temporal knowledge update, multi-domain knowledge synthesis, and continued improvement through collective efforts. On the Zero-Shot Generalization of Machine-Generated Text DetectorsXiao Pu, Jingyu Zhang, Xiaochuang Han, Yulia Tsvetkov, Tianxing HeEMNLP-Findings 2023How will the detectors of machine-generated text perform on outputs of a new generator, that the detectors were not trained on? We begin by collecting generation data from a wide range of LLMs, and train neural detectors on data from each generator and test its performance on held-out generators. While none of the detectors can generalize to all generators, we observe a consistent and interesting pattern that the detectors trained on data from a medium-size LLM can zero-shot generalize to the larger version. Can Language Models Solve Graph Problems in Natural Language?Heng Wang*, Shangbin Feng*, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, Yulia TsvetkovNeurIPS 2023Are language models graph reasoners? We propose the NLGraph benchmark, a test bed for graph-based reasoning designed for language models in natural language. We find that LLMs are preliminary graph thinkers while the most advanced graph reasoning tasks remain an open research question. Learning Time-Invariant Representations for Individual Neurons from Population DynamicsLu Mi*, Trung Le*, Tianxing He, Eli Shlizerman, Uygar SÃ¼mbÃ¼lNeurIPS 2023We use implicit dynamical models to learn time-invariant representation for individual neurons from population dynamics and enable mapping functional activity to cell types. PCFG-based Natural Language Interface Improves Generalization for Controlled Text GenerationJingyu Zhang, James Glass, Tianxing HeThe 2022 Efficient Natural Language and Speech Processing Workshop (NeurIPS ENLSP 2022)The Best Paper Award at the WorkshopThe 12th Joint Conference on Lexical and Computational Semantics (StarSEM 2023)We propose a natural language (NL) interface for controlled text generation, where we craft a PCFG to embed the control attributes into natural language commands, and propose variants of existing CTG models that take commands as input. Controlling the Focus of Pretrained Language Generation ModelsJiabao Ji, Yoon Kim, James Glass, Tianxing HeACL-Findings 2022Different focus in the context leads to different generation! We develop the \"focus vector\" method to control the focus of a pretrained language model. Exposure Bias versus Self-Recovery: Are Distortions Really Incremental for Autoregressive Text Generation?Tianxing He, Jingzhao Zhang, Zhiming Zhou, James GlassEMNLP 2021By feeding the LM with different types of prefixes, we could assess how serious exposure bias is. Surprisingly, our experiments reveal that LM has the self-recovery ability, which we hypothesize to be countering the harmful effects from exposure bias. Joint Energy-based Model Training for Better Calibrated Natural Language Understanding ModelsTianxing He, Bryan McCann, Caiming Xiong, Ehsan Hosseini-AslEACL 2021We explore joint energy-based model (EBM) training during the finetuning of pretrained text encoders (e.g., Roberta) for natural language understanding (NLU) tasks. Our experiments show that EBM training can help the model reach a better calibration that is competitive to strong baselines, with little or no loss in accuracy. An Empirical Study on Few-shot Knowledge Probing for Pretrained Language ModelsTianxing He, Kyunghyun Cho, James GlassOn ArxivWe compare a variety of approaches under a few-shot knowledge probing setting, where only a small number (e.g., 10 or 20) of example triples are available. In addition, we create a new datase",
  "content_length": 13446,
  "method": "requests",
  "crawl_time": "2025-12-01 14:38:24"
}