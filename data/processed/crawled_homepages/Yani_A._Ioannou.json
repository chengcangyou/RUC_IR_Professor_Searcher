{
  "name": "Yani A. Ioannou",
  "homepage": "https://www.calgaryml.com",
  "status": "success",
  "content": "Calgary Machine Learning Lab The Calgary Machine Learning Lab is a research group led by Yani Ioannou within the Schulich School of Engineering at the University of Calgary. The lab has a research focus on improving Deep Neural Network (DNN) training and models. Topics of research include: Sparse Neural Network Training, Bias and Robustness of Efficient Deep Learning methods and Efficient Inference with Large Language Models. CML had 6 different works being presented by 5 students at one of the top machine learning conferences, International Conference on Machine Learning (ICML) 2025, across the workshops and main conference. news Nov 01, 2025 We were awarded one of only ten NSERC/ANR Canada-France AI Research Grants for our project titled “GHOST: Generative modeling, Heavy tails, Outliers, Sparse Training”. This project is a collaboration with Dr. Umut Simsekli at INRIA - SIERRA team and École Normale Supérieure, Computer Science Department in Paris, France. Jul 09, 2025 The Calgary Machine Learning Lab, will have 6 different works being presented by 5 students across the International Conference on Machine Learning (ICML) 2025 workshops (HiLD, ES-FoMo III, DIG-BUGS, WiML, MusiML) and main conference! Please see our full conference schedule here for details. May 01, 2025 Adnan Mohammed and Rohan Jain’s work on “Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry” (Adnan et al., 2025) has been accepted at the International Conference on Machine Learning (ICML), 2025. This work explores the Lottery Ticket Hypothesis (LTH) and sparse training from a random initialization through the lens of weight and permutation symmetry, proposing a novel approach to improve LTH mask generalization across new random initialization. latest blog posts Jul 14, 2025 Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry Mar 31, 2025 Beyond Compression: How Knowledge Distillation Impacts Fairness and Bias in AI Models May 07, 2024 Dynamic Sparse Training with Structured Sparsity selected publications ICML Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry Mohammed Adnan, Rohan Jain, Ekansh Sharma, Rahul Krishnan, and Yani Ioannou In Proceedings of the 42nd International Conference on Machine Learning (ICML), Jul 2025 arXiv Bib Video Blog Code OpenReview Poster Website @inproceedings{mohammed2025sparsetraining, author = {Adnan, Mohammed and Jain, Rohan and Sharma, Ekansh and Krishnan, Rahul and Ioannou, Yani}, title = {Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry}, booktitle = {Proceedings of the 42nd International Conference on Machine Learning (ICML)}, pages = {477--498}, editor = {Singh, Aarti and Fazel, Maryam and Hsu, Daniel and Lacoste-Julien, Simon and Berkenkamp, Felix and Maharaj, Tegan and Wagstaff, Kiri and Zhu, Jerry}, volume = {267}, series = {Proceedings of Machine Learning Research}, publisher = {PMLR}, month = jul, year = {2025}, url = {https://proceedings.mlr.press/v267/adnan25a.html}, arxivid = {2505.05143}, eprint = {2505.05143}, eprinttype = {arXiv}, venue = {{Vancouver, BC, Canada}}, eventdate = {2025-07-13/2025-07-19}, } TMLR What is Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias Aida Mohammadshahi, and Yani Ioannou Transactions on Machine Learning Research (TMLR), Mar 2025 arXiv Bib PDF Video Blog OpenReview Slides @article{mohammadshahi2025leftafterdistillation, author = {Mohammadshahi, Aida and Ioannou, Yani}, title = {What is Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias}, journal = {Transactions on Machine Learning Research (TMLR)}, year = {2025}, month = mar, arxivid = {2410.08407}, eprint = {2410.08407}, eprinttype = {arXiv}, } NeurIPS Navigating Extremes: Dynamic Sparsity in Large Output Spaces Nasib Ullah, Erik Schultheis, Mike Lasby, Yani Ioannou, and Rohit Babbar In 38th Annual Conference Neural Information Processing Systems (NeurIPS), Dec 2024 arXiv Bib PDF Code OpenReview Website @inproceedings{nasib2024navigating, author = {Ullah, Nasib and Schultheis, Erik and Lasby, Mike and Ioannou, Yani and Babbar, Rohit}, booktitle = {{38th Annual Conference Neural Information Processing Systems (NeurIPS)}}, venue = {{Vancouver, BC, Canada}}, eventdate = {2024-12-10/2024-12-16}, title = {Navigating Extremes: Dynamic Sparsity in Large Output Spaces}, month = dec, year = {2024}, arxivid = {2411.03171}, eprint = {2411.03171}, eprinttype = {arXiv}, url = {https://nips.cc/virtual/2024/poster/95193}, } ICLR Dynamic Sparse Training with Structured Sparsity Mike Lasby, Anna Golubeva, Utku Evci, Mihai Nica, and Yani Ioannou In International Conference on Learning Representations (ICLR), May 2024 arXiv Bib PDF Blog Code OpenReview Poster Slides Website @inproceedings{lasby2024srigl, author = {Lasby, Mike and Golubeva, Anna and Evci, Utku and Nica, Mihai and Ioannou, Yani}, booktitle = {{International Conference on Learning Representations (ICLR)}}, venue = {{Vienna, Austria}}, eventdate = {2024-05-07/2024-05-11}, title = {Dynamic Sparse Training with Structured Sparsity}, month = may, year = {2024}, arxivid = {2305.02299}, eprint = {2305.02299}, eprinttype = {arXiv}, url = {https://iclr.cc/virtual/2024/poster/17975}, dimensions = {1157762278}, }",
  "content_length": 5369,
  "method": "requests",
  "crawl_time": "2025-12-01 14:50:16"
}