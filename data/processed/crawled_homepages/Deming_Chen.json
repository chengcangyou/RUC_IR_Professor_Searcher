{
  "name": "Deming Chen",
  "homepage": "https://www.ece.illinois.edu/directory/profile/dchen",
  "status": "success",
  "content": "Deming Chen | Electrical & Computer Engineering | Illinois Deming Chen Deming Chen Administrative TitlesAbel Bliss Professor of Engineering Professor (217) 244-3922 dchen@illinois.edu 250 Coordinated Science Lab For More Information Professor Chen's Home Page Education Ph.D. in Computer Science, University of California at Los Angeles, 2005 B.S. Computer Science, University of Pittsburgh, Pittsburgh, Pennsylvania, 1995 BiographyDr. Deming Chen obtained his BS in computer science from University of Pittsburgh, Pennsylvania in 1995, and his MS and PhD in computer science from University of California at Los Angeles in 2001 and 2005 respectively. He worked as a software engineer between 1995-1999 and 2001-2002. He joined the ECE department of University of Illinois at Urbana-Champaign in 2005 and has been a full professor in the same department since 2015. He is a research professor in the Coordinated Science Laboratory and an affiliate professor in the CS department. His current research interests include reconfigurable computing, cloud computing, system-level and high-level synthesis, machine learning and IoT, and hardware security. He has given more than 160 invited talks sharing these research results worldwide. His work has had a significant impact, with open-source solutions adopted by industry, such as FCUDA, DNNBuilder, CSRNet, SkyNet, ScaleHLS, and Medusa. Notably, Medusa has been integrated into Nvidia's TensorRT-LLM, improving the speed of large language model (LLM) execution by 1.9-3.6x. Dr. Chen has been a technical committee member for a series of top conferences and symposia on EDA, FPGA, low-power design, and embedded systems design. He has also served as General or TPC Chair, Track Chair, Session Chair, Panelist, Panel Organizer, or Moderator for many of these conferences. He has been an associated editor for IEEE TCAD, ACM TODAES, IEEE TVLSI, ACM TRETS, IEEE TCAS-I and TCAS-II, IEEE Design & Test, IET Cyber-Physical Systems, JCSC, and JOLPE. He obtained the Achievement Award for Excellent Teamwork from Aplus Design Technologies in 2001, the Arnold O. Beckman Research Award from UIUC in 2007, the NSF CAREER Award in 2008, ten Best Paper Awards, a TCFPGA Hall-of-Fame paper award, and a few Best Poster Awards. He also received the ACM SIGDA Outstanding New Faculty Award in 2010, IBM Faculty Award in 2014 and 2015, and Google Faculty Award in 2020. In 2017 and 2019 respectively, he led a team to win the First Place Winner Award of DAC International System Design Contest. He is the Donald Willett Faculty Scholar and the Abel Bliss Professor of the Grainger College of Engineering, an IEEE Fellow, an ACM Distinguished Speaker, and the former Editor-in-Chief of ACM Transactions on Reconfigurable Technology and Systems (TRETS). Under his leadership, the impact factor of ACM TRETS has increased by 3.8 times. He is the Director of the AMD-Xilinx Center of Excellence and the Co-Director of the IBM-Illinois Discovery Accelerator Institute. He has given a series of Keynote or Plenary speeches at various conferences. He is also included in the List of Teachers Ranked as Excellent in 2008 and 2017 from UIUC. Dr. Chen was involved in several startup companies. He implemented his published algorithm on CPLD technology mapping when he was a software engineer in Aplus Design Technologies, Inc. in 2001, and the software was exclusively licensed by Altera (now part of Intel) and distributed to many customers of Altera worldwide. He is one of the inventors of the xPilot High Level Synthesis package developed at UCLA, which was licensed to AutoESL Design Technologies, Inc. Aplus was acquired by Magma in 2003, and AutoESL was acquired by Xilinx in 2011. In 2016, he co-founded a new startup, Inspirit IoT, Inc. Professional Highlights New! SnapKV: SnapKV is an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications. Available since 2024. Download: https://github.com/FasterDecoding/SnapKV (210 stars so far) New! Medusa: Medusa is an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa substantially reduces the number of decoding steps required. Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. We evaluate Medusa on models of various sizes and training procedures, and our experiments demonstrate that Medusa can improve the LLM execution speed on GPUs by 2.2-3.6x. Available since 2024. Download: https://github.com/FasterDecoding/Medusa (2.4k stars so far) New! ISDC: ISDC is a novel feedback-guided iterative system of difference constraints (SDC) scheduling algorithm for high-level synthesis (HLS). ISDC leverages subgraph extraction-based low-level feedback from downstream tools like logic synthesizers to iteratively refine HLS scheduling. Technical innovations include: (1) An enhanced SDC formulation that effectively integrates low-level feedback into the linear-programming (LP) problem; (2) A fanout and window-based subgraph extraction mechanism driving the feedback cycle; (3) A no-human-in-loop ISDC flow compatible with a wide range of downstream tools and process design kits (PDKs). Evaluation shows that ISDC reduces register usage by 28.5% against an industrial-strength open-source HLS tool. Available since 2024. Download: https://github.com/google/xls NEW! PandoGen: An ability to forecast future viral individuals at the sequence level enables advance preparation by characterizing the sequences and closing vulnerabilities in current preventative and therapeutic methods. In this work, we explore, in the context of a viral pandemic, the problem of generating complete instances of undiscovered viral protein sequences, which have a high likelihood of being discovered in the future using protein language models. Our novel method, called PandoGen, trains protein language models towards the pandemic protein forecasting task. PandoGen combines techniques such as synthetic data generation, conditional sequence generation, and reward-based learning, enabling the model to forecast future sequences, with a high propensity to spread. Applying our method to modeling the SARS-CoV-2 Spike protein sequence, we find empirically that our model forecasts twice as many novel sequences with five times the case counts compared to a model that is 30× larger. Our method forecasts unseen lineages months in advance. Available since 2024. Download: https://github.com/UIUC-ChenLab/PandoGen New! NimBlock: This project focuses on enabling virtualization features to facilitate fine-grained FPGA sharing. We employ an overlay architecture which enables arbitrary, independent user logic to share portions of a single FPGA by dividing the FPGA into independently reconfigurable slots. We then explore scheduling possibilities to effectively time- and space-multiplex the virtualized FPGA. The Nimblock scheduling algorithm balances application priorities and performance degradation to improve response time and reduce deadline violations. We achieve up to 5.7× lower average response times when compared to a no-sharing and no-virtualization scheduling algorithm and up to 2.1× average response time improvement over competitive scheduling algorithms that support sharing within our virtualization environment. Available since 2023. Download: https://github.com/UIUC-ChenLab/Nimblock NEW! ScaleHLS+HIDA: ScaleHLS is a High-level Synthesis (HLS) framework on MLIR. ScaleHLS can compile HLS C/C++ or PyTorch model to optimized HLS C/C++ in order to generate high-efficiency RTL design using downstream tools, such as AMD Vitis HLS. By using the MLIR framework that can be better tuned to particular algorithms at different representation levels, ScaleHLS is more scalable and customizable towards various applications coming with intrinsic structural or functional hierarchies. Working with a set of neural networks modeled in PyTorch, ScaleHLS-generated hardware designs provide up to 3825x higher performances compared to the baseline designs that do not contain pragma directives and are only optimized by Xilinx Vivado HLS. Furthermore, HIDA (ScaleHLS 2.0) achieves an 8.54x higher throughput on average compared to that of ScaleHLS. Meanwhile, despite being fully automated and able to handle various applications, HIDA achieves a 1.29x higher throughput over DNNBuilder, a state-of-the-art RTL-based neural networ",
  "content_length": 24707,
  "method": "requests",
  "crawl_time": "2025-12-01 13:00:59"
}