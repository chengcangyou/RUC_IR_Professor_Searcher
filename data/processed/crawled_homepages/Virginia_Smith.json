{
  "name": "Virginia Smith",
  "homepage": "http://users.ece.cmu.edu/~smithv",
  "status": "success",
  "content": "Virginia Smith Virginia Smith Research · Bio · CV · Email smithv[ at ]cmu[ dot ]edu I am the Leonardo Associate Professor of Machine Learning at Carnegie Mellon University, and a courtesy faculty member in the Electrical and Computer Engineering Department. My research explores challenges related to safety and efficiency in machine learning systems. Recent topics include: federated and collaborative learning, efficient training, finetuning & inference, data privacy, and AI safety. Recent News I'm co-organizing a semester program on Federated and Collaborative Learning at the Simons Institute I'm honored to receive an AFOSR Young Investigator Award See our blog posts on LLM unlearning evaluation and benign relearning I'm serving as a Program Chair for ICML 2025, along with Maryam Fazel, Daniel Hsu, and Simon Lacoste-Julien Gave talks at Stanford, UC Berkeley, DeepMath, UPenn, and the Boston-Area Privacy Day on AI Safety I'm honored to receive a Sloan Research Fellowship and be named a 2023 Samsung AI Researcher of the Year ... Co-organized a workshop on Federated and Collaborative Learning at the Simons Institute See our blog posts on privacy-preserving cross-silo FL and LLM evaluation Our solution won first place in the UK-US Privacy-Enhancing Technologies pandemic forecasting challenge I gave talks at the Apple PPML Workshop and Argonne AI Distinguished Lecture Series on privacy-preserving ML I'm honored to receive an NSF CAREER Award, and be named one of Intel's 2022 Rising Stars I enjoyed discussing some of our recent work on the TWIML podcast I'm honored to be selected as one of MIT Technology Review's 35 Innovators Under 35 See our blog post, white paper, and NeurIPS tutorial on federated learning PhD Students & Postdocs Steven Kolawole Kevin Kuo Aashiq Muhamed Qi Pang (with Wenting Zheng) Amrith Setlur Pratiksha Thaker (with Steven Wu) Chhavi Yadav Alumni Don Dennis (PhD 2025; Meta) Yiwei Fu (MS 2025; PhD at UIUC) Neel Guha (MS 2020; JD/PhD at Stanford) Shengyuan Hu (PhD 2025; Meta) Michael Kuchnik (with George Amvrosiadis) (PhD 2023; Meta) Oscar Li (PhD 2025; Jane Street) Tian Li (PhD 2023; Asst Professor at University of Chicago) Ken Liu (MS 2023; PhD at Stanford) Arian Raje (MS 2024; PhD at CMU) Qiqi Xu (MS 2021; Google) Teaching 10-718 (Machine Learning in Practice), Spring 2025 10-605/10-805 (PhD/MS ML with Large Datasets), Fall 2024 10-718 (Machine Learning in Practice), Spring 2024 10-719 (Federated and Collaborative Learning), Fall 2023 10-405/10-605 (Undergraduate/MS ML with Large Datasets), Spring 2022 10-605/10-805 (PhD/MS ML with Large Datasets), Fall 2021 10-405/10-605 (Undergraduate/MS ML with Large Datasets), Spring 2021 10-605/10-805 (PhD/MS ML with Large Datasets), Fall 2020 10-405/10-605 (Undergraduate/MS ML with Large Datasets), Spring 2020 18-461/18-661 (Intro to ML for Engineers), Fall 2018 Publications Preprints Research in Collaborative Learning Does Not Serve Cross-Silo Federated Learning in PracticeK. Kuo, C. Yadav, V. Smith [abstract] [arxiv] Cross-silo federated learning (FL) is a promising approach to enable cross-organization collaboration in machine learning model development without directly sharing private data. Despite growing organizational interest driven by data protection regulations such as GDPR and HIPAA, the adoption of cross-silo FL remains limited in practice. In this paper, we conduct an interview study to understand the practical challenges associated with cross-silo FL adoption. With interviews spanning a diverse set of stakeholders such as user organizations, software providers, and academic researchers, we uncover various barriers, from concerns about model performance to questions of incentives and trust between participating organizations. Our study shows that cross-silo FL faces a set of challenges that have yet to be well-captured by existing research in the area and are quite distinct from other forms of federated learning such as cross-device FL. We end with a discussion on future research directions that can help overcome these challenges. e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMsA. Setlur, M. Yang, C. Snell, J. Greer, I. Wu, V. Smith, M. Simchowitz, A. KumarBest Paper Award at ICML 2025 Exploration in AI Workshop [abstract] [arxiv] Test-time scaling offers a promising path to improve LLM reasoning by utilizing more compute at inference time; however, the true promise of this paradigm lies in extrapolation (i.e., improvement in performance on hard problems as LLMs keep \"thinking\" for longer, beyond the maximum token budget they were trained on). Surprisingly, we find that most existing reasoning models do not extrapolate well. We show that one way to enable extrapolation is by training the LLM to perform in-context exploration: training the LLM to effectively spend its test time budget by chaining operations (such as generation, verification, refinement, etc.), or testing multiple hypotheses before it commits to an answer. To enable in-context exploration, we identify three key ingredients as part of our recipe e3: (1) chaining skills that the base LLM has asymmetric competence in, e.g., chaining verification (easy) with generation (hard), as a way to implement in-context search; (2) leveraging \"negative\" gradients from incorrect traces to amplify exploration during RL, resulting in longer search traces that chains additional asymmetries; and (3) coupling task difficulty with training token budget during training via a specifically-designed curriculum to structure in-context exploration. Our recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25 scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not only attains high pass@1 scores, but also improves pass@k over the base model. BLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain OverlapS. Hu, N. Kale, P. Thaker, Y. Fu, Z. S. Wu, V. Smith [abstract] [arxiv] Machine unlearning has the potential to improve the safety of large language models (LLMs) by removing sensitive or harmful information post hoc. A key challenge in unlearning involves balancing between forget quality (effectively unlearning undesirable information) and retain quality (maintaining good performance on other, general tasks). Unfortunately, as we show, current LLM unlearning benchmarks contain highly disparate forget and retain sets -- painting a false picture of the effectiveness of LLM unlearning methods. This can be particularly problematic because it opens the door for benign perturbations, such as relearning attacks, to easily reveal supposedly unlearned knowledge once models are deployed. To address this, we present BLUR: a benchmark for LLM unlearning that provides more realistic scenarios of forget-retain overlap. BLUR significantly expands on existing unlearning benchmarks by providing extended evaluation tasks, combined forget/retain queries, and relearning datasets of varying degrees of difficulty. Despite the benign nature of the queries considered, we find that the performance of existing methods drops significantly when evaluated on BLUR, with simple approaches performing better on average than more recent methods. These results highlight the importance of robust evaluation and suggest several important directions of future study. Our benchmark is publicly available at: https://huggingface.co/datasets/forgelab/BLUR. Membership Inference Attacks for Unseen ClassesP. Thaker, N. Kale, Z. S. Wu, V. Smith [abstract] [arxiv] Shadow model attacks are the state-of-the-art approach for membership inference attacks on machine learning models. However, these attacks typically assume an adversary has access to a background (nonmember) data distribution that matches the distribution the target model was trained on. We initiate a study of membership inference attacks where the adversary or auditor cannot access an entire subclass from the distribution -- a more extreme but realistic version of distribution shift than has been studied previously. In this setting, we first show that the performance of shadow model attacks degrades catastrophically, and then demonstrate the promise of another approach, quantile regression, that does not have the same limitations. We show that quantile regression attacks consistently outperform shadow model attacks in the class dropout setting -- for example, quantile regression attacks achieve up to 11x the TPR of shadow models on the unseen class on CIFAR-100, and achieve nontrivial TPR on ImageNet even with 90% of training classes removed. We also provide a theoretical model that illustrates the potential and limitations of this approach. Exact Unlearning of Finetuning Data via Model Merging at ScaleK. Kuo, A. Setlur, K. Srinivas, A. Raghunathan, V. Smith [abstract] [arxiv] Approximate unlearning has gained popularity as an approach to efficiently update an LLM so that it behaves (roughly) as if it was not trained on a subset of data to begin with. However, existing methods are brittle in practice and can easily be attacked to reveal supposedly unlearned information. To alleviate issues with approximate unlearning, we instead propose SIFT-Masks (SIgn-Fixed Tuning-Masks), an exact unlearning method based on model merging. SIFT-Masks addresses two key limitations of standard model merging: (1) merging a large number of tasks can severely harm utility; and (2) methods that boost utility by sharing extra information across tasks make exact unlearning prohibitively expensive. SIFT-Masks solves these issues by (1) applying local masks to recover task-specific performance; and (2) constraining finetuning to align with a global sign vector as a lightweight approach to determine masks independently before merging. Across four settings where we merge up to 500 models, SIFT-Masks improves accuracy by 5-80% over naive merging and uses up to 250x less compute for exact unlearning compared to other merging baselines. Federated ",
  "content_length": 96490,
  "method": "requests",
  "crawl_time": "2025-12-01 14:44:30"
}