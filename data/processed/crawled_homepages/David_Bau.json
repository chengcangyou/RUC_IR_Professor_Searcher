{
  "name": "David Bau",
  "homepage": "https://baulab.info",
  "status": "success",
  "content": "David Bau: Interpretation of Deep Networks David Bau Knowing What Neural Networks Know Northeastern University Khoury College of Computer Sciences A brief interview with David: why we study deep network internals. I am an Assistant Professor of Computer Science at Northeastern Khoury College. My lab studies the structure and interpretation of deep networks. We think that understanding the rich internal structure of deep networks is a grand and fundamental research question with many practical implications. We aim to lay the groundwork for human-AI collaborative software engineering, where humans and machine-learned models both teach and learn from each other. Want to come to Boston to work on deep learning with me? Apply to Khoury here and contact me if you are interested in joining as a graduate student or postdoc. Also check out NDIF engineering fellowships. Publication List. (PNAS; NeurIPS; ICLR; TPAMI; CVPR; SIGGRAPH; EMNLP; ECCV; ICCV.) Curriculum Vitae. (PhD MIT EECS, thesis; Cornell; Harvard; Google; Microsoft. Sloan fellowship, Spira teaching award) Publication pages on Dblp and Google Scholar. Talks at the Bau Lab ; What is AI interpretability for? Lab Members Can Rager Natalie Shapira Chris Wendler David Atkinson Sheridan Feucht Rohit Gandikota Arnab Sen Sharma Eric Todd Koyena Pal Nikhil Prakash Jaden Fiotto-Kaufman Alex Loftus Grace Proebsting Andy Arditi Jiuding Sun Aaron Mueller Sam Marks In the News Why nobody can see inside AI's black box \"People can have a variety of motivations for understanding internals... I'm motivated by transparency because we have responsibility for the systems we make...\" At the most fundamental level, the tech companies building these AI systems don't fully understand how their models work internally—a challenge inherent to the technology itself. But there's a second, distinct barrier to transparency: Developers aren't making the data they train these systems with available to those outside their organizations... Grasping the fundamental internals of AI models is important because it could enable precise interventions when needed... With access to AI system's training data and methods and the computing resources needed to independently study these systems, academic researchers could help fill this knowledge gap. Bulletin of the Atomic Scientists by Abi Olvera, January 27, 2025. How does ChatGPT think? \"Researchers are striving to reverse-engineer artificial intelligence and scan the 'brains' of LLMs to see what they are doing, how and why... Researchers want explanations so that they can create safer, more efficient and more accurate AI. Users want explanations so that they know when to trust a chatbot's output. And regulators want explanations so that they know what AI guard rails to put in place.... Bau and his colleagues have also developed methods to scan and edit AI neural networks, including a technique they call causal tracing....\" This news article in Nature surveys the emerging field of deep network interpretation. The increasing complexity of generative AI systems such as ChatGPT has spawned a new research subfield that cracks open these large models and interprets their emergent internal structure. The article surveys perspectives on this new research area from several relevant researchers including David Bau, Mor Geva, Martin Wattenberg, Chris Olah, Thilo Hagendorff, Sam Bowman, Sandra Wachter, Andy Zou, and Peter Hase. The article also highlights research from Kenneth Li, Jason Wei, Miles Turpin, Kevin Meng and Roger Grosse. Nature news feature by Matthew Hutson, May 14, 2024. We have launched the National Deep Inference Fabric (NDIF) project. Large-scale AI presents fundamental open scientific questions and major societal impacts that are not yet well-understood—and they are both difficult and expensive to study. NDIF is a major investment in scientific infrastructure to help meet the challenge, with $9m of funding from the National Science Foundation to develop large-scale AI inference software aimed at enabling cutting-edge research. Questions in the public interest, such as \"how can we explain an AI decision?\" or \"what can improve the safety and robustness of AI?\" The goal of NDIF is to provide a robust and transparent AI inference service to enable scientists in every part of the country in every field touched by AI, to expand, accelerate, and democratize impactful AI science. (Programmers interested in the technical details can pip install nnsight to try NDIF today.) Users can remotely access and alter activations, gradients, and customize any step of large models like llama3-70b like having a 70b model on your own laptop. This transparency goes far behyond commercial AI inference services, and NSF funding will expand NDIF to support every open model and a broad range of research methods. Read about the positions NDIF is hiring to fill on LinkedIn, Twitter/X, and on the NDIF website. NSF funds groundbreaking research led by Northeastern to democratize artificial intelligence. Northeastern Global News, May 2, 2024. Selected Projects Concept Sliders. While GANs are famous for containing disenangled latents that can control a variety of interpretable image attributes, it has not been known whether similar controllable latents are present in diffusion models. In this work, we develop Concept Sliders, a way of finding LoRA adjustments to diffusion model weights that cleanly and smoothly control a single disentangled concept. With Concept Sliders, an artist can easily modulate a single attribute like \"age\" or \"smiling\" or even \"cooked food\" to smoothly adjust the visual characteristics of an image. Concept sliders are based on the guided-training technique underling our previous ESD work, but instead of erasing a concept, we develop the needed techniques to modulate or amplify a concept without changing the underlying layout of the image, and without entangling the concept with correlated concepts that we wish to remain unchanged. Concept sliders have been an open-source hit among the artistic community, and they also provide a promising window into the organization of visual concept information within the parameter space of diffusion models. The paper develops and evaluates over 50 different concept sliders including very interesting sliders that reduce visible distortions in diffusion model output, and examines their efficacy, specificity, and composability. R Gandikota, J Materzyńska, T Zhou, A Torralba, D Bau. Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models ECCV 2024. Linearity of Relation Decoding in Transformer LMs. What is the right level of abstraction to use when understanding a huge network? While it is natural to examine individual neurons, attention heads, modules, and representation vectors, we should also ask whether taking a holistic view of a larger part of the network can reveal any higher-level structure. In this work, we ask how relationships between entities and their attributes are represented, and we measure the power of the Jacobian—the matrix derivative—to capture the action of a range of transformer layers in applying a relation to an entity. When a representation vector passes through a range of transformer layers, it is subjected to a very nonlinear transformation. Yet in this paper we find that when the network resolves a specific relationship such as person X plays instrument Y, the action of the transformer from the vector for X to the vector for Y will often be essentially linear, suggesting that the information about Y is already present in X. Moreover the linear operator can be extracted by examining the Jacobian using as few as a single example of the relation. We analyze more than 40 different relations to determine which have a linear representation, and we introduce a tool, the attribute lens that exploits linearity to visualize the relational information carried in a state vector. E Hernandez, A Sen Sharma, T Haklay, K Meng, M Wattenberg, J Andreas, Y Belinkov, D Bau. Linearity of Relation Decoding in Transformer Language Models. ICLR 2024 (spotlight). Fine-Tuning Enhances Existing Mechanisms. When you fine-tune an LLM, are you teaching it something new or exposing what it already knows? In this work, we pin down the detailed structure of the mechanisms for an entity-tracking task using new patching techniques, revealing a pre-existing circuit when a capability emerges from fine-tuning. The paper applies path-patching causal mediation methods as used in Wang 2022 (IOI) to identify the components for a circuit for entity tracking that emerges after fine-tuning. Interestingly, we find that the components already existed in the model prior to fine-tuning. Furthermore we use our DCM patching method to deduce the type of information being transmitted at most of the steps before and after fine-tuning, and find that the role of the information is unchanged under fine-tuning. Finally, we introduce Cross-Model Activation Patching (CMAP) to test whether the encoding of information is changed after fine-tuning, and we find that the encodings are compatible, not only allowing interchange, but also revealing that improved task performance can be obtained by directly patching model activations between models. N Prakash, T R Shaham, T Haklay, Y Belinkov, D Bau. Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking. ICLR 2024. Function Vectors in Large Language Models. The idea of treating a function reference as data is one of the most powerful concepts in computer science, enabling complex computational forms. Do neural networks learn to represent functions as data? In this paper, we study in-context-learning inside large transformer language models and show evidence that vector representations of functions appear. Function vectors (FVs) emerge when a language model generalizes a list of demonstrations of input-output pairs (via in-context learning, ICL). To study how ICL works, we apply ",
  "content_length": 27061,
  "method": "requests",
  "crawl_time": "2025-12-01 12:58:38"
}