{
  "name": "Jie Tang 0001",
  "homepage": "http://keg.cs.tsinghua.edu.cn/jietang",
  "status": "success",
  "content": "Jie Tang (Tang, Jie) 's Homepage Jie Tang (Tang, Jie) 唐 杰 Homepage Research Publications Services Awards Students Jie Tang (Tang, Jie) 唐 杰 Professor ACM/AAAI/IEEE Fellow NSFC Distinguished Young Scholar Knowledge Engineering Group (KEG), Department of Computer Science, Tsinghua University E-Mail:  jietang at tsinghua . edu . cn Open Codes GLM-130B ChatGLM CogView&CogVideo CodeGeex More... Homepage Research Publications Services Awards Talks Students I am a Professor of Computer Science of Tsinghua\tUniversity. I am a Fellow of the ACM, a Fellow of AAAI, and a Fellow of the IEEE. My research interests include artificial general intelligence (AGI), data mining, social networks, machine learning and knowledge graph. Our research received the SIGKDD Test-of-Time Award (Ten-year Best Paper). I also received the SIGKDD Service Award. Recent Research: I put all my efforts into artificial general intelligence with a mission toward teaching machines to think like humans. Similar to Open AI's GPT serials, we, together with a big research team, have developed GLM-130B, ChatGLM, CogView&CogVideo, CodeGeex. Proudly, our pretrained base model has been downloaded by more than 1,000 organizations from 70+ countries, and our open ChatGLM-6b has been downloaded by nearly 10,000,000 times all over the world. For More: I also invented AMiner.org, which has attracted over 30,000,000 users from 220 countries/regions. I served as GC of WWW'23, PC of WWW'22, and EiC of IEEE T. on Big Data and AI Open J. I also received the 2nd National Award for Science&Technology, and NSFC for Distinguished Young Scholar. Hiring: I am looking for highly-motivated and fully-devoted students to closely work with me on the exciting area of artificial general intelligence. I also have open Postdoctoral Positions to investigate underlying theory and algorithms in artificial general intelligence. If you want me to write a recommendation letter for you, please first read this. *New CKDD'24 Keynote: GLM: 从大模型看AGI的发展 [PDF] [Slides] ICLR'24 Keynote (The ChatGLM's Road to AGI) Our ArnetMiner paper has been awarded the (SIGKDD Test-of-Time Award)! RESEARCH   Go Top I am recently working on artificial general intelligence, graph neural networks, social network mining, and academic knowledge graph. Large Pre-trained Models [ChatGLM: 从千亿模型到ChatGPT (in Chinese)] [Keynote@AIED'22/NetSci'22: Pre-Training the World (in English)] GLM-130B (ICLR'23) is an open bilingual (English & Chinese) bidirectional dense model with 130 billion parameters, pre-trained using the General Language Model (GLM) algorithm (ACL'22). GLM-130B has been trained on over 1 trillion text tokens and exhibits quite a few unique features. Based on GLM-130B, we have developed ChatGLM by applying techniques such as supervised fine-tune and RLHF. The open-sourced version ChatGLM-6B tops the trending of Huggingface for more than three weeks. Models and codes: [GLM-130B] [ChatGLM-6B] [GLM] [CogView] [CogVideo] [CodeGeex] Representation Learning on Networks [GNN&Self-supervised Learning] [人工智能下一个十年] The goal is to automatically encode network structure into low-dimensional space (embeddings), using techniques such as neural networks. We theoretically prove that recent models such as DeepWalk, LINE, PTE, and node2vec can be unified into the matrix factorization framework with closed forms. we present a new method NetMF, which significantly outperforms DeepWalk and LINE for conventional network mining tasks (Qiu et al., WSDM'18). Based on the learned representations, we further propose a multi-head attention network for predicting user behavior (Qiu et al., KDD'18) and NetSMF for large scale networks (Qiu et al., WWW'19). Further, we incorporate user feedback into the prediction and propose a bandit learning model (Qi et al., NeurIPS'18). Datasets and codes: [NetMF] [DeepInf] Social Network Mining [IC2S2'19 Tutorial] [KDD'18 Tutorial] [Book] [Survey] Online social networks already become a bridge to connect our physical daily life with the virtual information space, producing huge volume of networked data. We aim to understand the mechanism underlying the dynamics of social interaction between users and information diffusion in the network. We propose a new method Topical Affinity Propagation (TAP) to model the topic-level social influence (Tang et al., KDD'09), conformity influence analysis (Tang et al., KDD'13), structural influence (Zhang et al., AAAI'17), inferring social tie (Tang et al., WSDM'12, Tang et al., TOIS'16), and user demographics (Dong et al., KDD'14). At the macro-level, we focus on mining top-k structural hole spanners, who control the information diffusion across different communities (Lou and Tang, WWW'13) and following link diffusion (Zhang et al., TKDE'15). Datasets and codes: [Topic-Influence] [Structural hole] [Datasets for SNA] Academic Knowledge Graph [Tutorial] [System] [Career Trajectory] We focus on building large-scale knowledge graph, particularly for scholarly data. In this research, we work on various topics including Expert Finding ( Qian et al., IJCAI'18; Tang et al., Machine Learning J'18 ) Career Trajectory Mining ( Wu et al., IJCAI'18 ) Social Recommendation (Tang et al., KDD'12), Information/knowledge Integration (Zhong et al., SIGMOD'09, Wang et al., WWW'12, Wang et al., IJCAI'13), Name Disambiguation (Tang et al., TKDE'12, Zhang et al., KDD'18 ) Summarization (Yang et al., SIGIR'11), Content Alignment (Hou et al., IJCAI'13; Hou et al., TOIS'17 ), Similarity Search (Zhang et al., TOIS'17). Based on these research, we have developed a system AMiner (ArnetMiner) (Tang et al., KDD'08 (SIGKDD Test-of-Time Award); Tang et al., TKDD'10), for academic search and mining. The system has over 136 million researchers and 200 million papers. Since 2006, the system has attracted over 10 million independent IP accesses from more than 220 countries/regions. Datasets and codes: [AMiner Dataset]  [Open Academic Graph] SELECTED PUBLICATIONS (A COMPLETE LIST)   Go Top Summary: Research interest: Artificial Intelligence, Social Networks, Data Mining, Knowledge Graph. I have published 400+ articles in major computer science conferences, including IJCAI / AAAI (30+), NIPS/ICML (10+), KDD (40+), and ~100 articles in the core journals of computer science including TKDE/TKDD/TOIS/TAC (40+). Social Network / Data Mining / Machine Learning Zhongyang Zhu and Jie Tang . CogCartoon: Towards Practical Story Visualization. International Journal of Computer Vision (IJCV), Volume 133, pages 1808–1833, 2025 LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs. In Proceedings of the 12th International Conference on Learning Representations (ICLR'25). [PDF] [*Code&Model*] [*@ChatGLM*] Zhen Yang, Ming Ding, Tinglin Huang, Yukuo Cen, Junshuai Song, Bin Xu, Yuxiao Dong, and Jie Tang. Does Negative Sampling Matter? A Review with Insights into its Theory and Applications. IEEE Transaction on Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2024. Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. Understanding Emergent Abilities of Language Models from the Loss Perspective. In Proceedings of the Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS'24). [PDF] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, XiXuan Song, Jiazheng Xu, Keqin Chen, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. CogVLM: Visual Expert for Pretrained Language Models. In Proceedings of the Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS'24). [PDF] [*Code&Data&Model*] Xingyi Cheng, Bo Chen, Pan Li, Jing Gong, Jie Tang, and Le Song. Training Compute-Optimal Protein Language Models. In Proceedings of the Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS'24). [PDF] [*Code&Data&Model*] Bo Chen, Zhilei Bei, Xingyi Cheng, Pan Li, Jie Tang, and Le Song. MSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training. In Proceedings of the Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS'24). [PDF] [*Code&Data&Model*] Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search. In Proceedings of the Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS'24). [PDF] [*Code&Data&Model*] Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, and Jie Tang. SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models. In Proceedings of the Thirty-Eighth Annual Conference on Neural Information Processing Systems (DB Track) (NeurIPS'24). [PDF] [*Code&Data&Model*] Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxing Xu, Yiming Liu, Jie Tang, Hongning Wang, and Minlie Huang. Benchmarking Complex Instruction-Following with Multiple Constraints Composition. In Proceedings of the Thirty-Eighth Annual Conference on Neural Information Processing Systems (DB Track) (NeurIPS'24). [PDF] [*Code&Data&Model*] Zeyao Ma, Bohan Zhang, Jing Zhang, Jifan Yu, Xiaokang Zhang, Xiaohan Zhang, Sijia Luo, Xi Wang, and Jie Tang. SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation. In Proceedings of the Thirty-Eighth Annual Conference on Neural Information Processing Systems (DB Track) (NeurIPS'24). [PDF] [*Code&Data&Model*] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Yu Hao, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. AutoWebGLM: A Large Language Model-based Web Navigating Agent. In Proceedings of the Thirty ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'24). Huanjing Zhao, Beining Yang, Yukuo Cen, Junyu Ren, Chenhui Zhang, Yuxiao Dong, Evgeny Kharlamov, Shu Zhao, and Jie Tang. Pre",
  "content_length": 74722,
  "method": "requests",
  "crawl_time": "2025-12-01 13:30:26"
}