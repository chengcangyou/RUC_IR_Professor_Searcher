{
  "name": "Carolin E. Brandt",
  "homepage": "https://carolin-brandt.de",
  "status": "success",
  "content": "Carolin Brandt | Caro‚Äôs personal website Carolin Brandt Assistant Professor Researcher Software Developer Write me! Google Scholar CV About Me Hello! I‚Äôm Caro üê≥ I'm an Assistant Professor at the Software Engineering Research Group of TU Delft, where I lead the research line on Human- and Developer-centered Software Engineering, and manage the FUSE lab. My research focuses on embedding the future users of software and automation tools more closely and responsibly into the software development process. In my PhD with my advisor Andy Zaidman, I investigated how to let automatic test amplification tools collaborate with developers to generate test cases that they can use to improve their test suites. üòä News I'll present our TSE paper \"Shaken, Not Stirred. How Developers Like Their Amplified Tests\" at ICSE'25 in Ottawa. See you there! I'll be at ICST'25 in Napels and present \"Towards Refined Code Coverage: A New Predictive Problem in Software Testing\" together with Aurora Ram√≠rez ‚òÄÔ∏è. I've started as an Assistant Professor at TU Delft. üòä Our paper \"Shaken, Not Stirred. How Developers Like Their Amplified Tests\" was accepted at IEEE Transactions on Software Engineering üôåüèº Recent Publications All Peer-reviewed Presentations More Towards Refined Code Coverage: A New Predictive Problem in Software Testing ICST'25 Towards Refined Code Coverage: A New Predictive Problem in Software Testing ICST'25 Full Publication Presentation Slides Replication Package Permalink to here Carolin Brandt ¬∑ Aurora Ram√≠rez This paper introduces a novel way to look at code coverage in software testing: can we predict it based on the code? Our idea is to learn from existing code coverage in open source projects what characterizes code that developers find it worth to cover with tests. Abstract To measure and improve the strength of test suites, software projects and their developers commonly use code coverage and aim for a threshold of around 80%. But what is the 80% of the source code that should be covered? To prepare for the development of new, more refined code coverage criteria, we introduce a novel predictive problem in software testing: whether a code line is, or should be, covered by the test suite. In this short paper, we propose the collection of coverage information, source code metrics, and abstract syntax tree data and explore whether they are relevant to predict whether a code line is exercised by the test suite or not. We present a preliminary experiment using four machine learning (ML) algorithms and an open source Java project. We observe that ML classifiers can achieve high accuracy (up to 90%) on this novel predictive problem. We also apply an explainable method to better understand the characteristics of code lines that make them more ‚Äúappealing‚Äù to be covered. Our work opens a research line worth to investigate further, where the focus of the prediction is the code to be tested. Our innovative approach contrasts with most predictive problems in software testing, which aim to predict the test case failure probability. The Art of Generating Useful Tests Invited Talk The Art of Generating Useful Tests Invited Talk Slides Permalink to here In October 2024 I had the pleasure to give a talk for the Quality and Testing Community at Bol ‚òÄÔ∏è. During this talk I gave an overall introduction to test generation, which tools are available (mostly research ones‚Ä¶), and dive into two of our research projects: Shaken, Not Stirred. How Developers Like Their Amplified Tests, where we investigate what developers change in automatically generated tests before adding them to their test suite by submitting tests to open source projects and analyzing the maintainer‚Äôs feedback. And Using GitHub Copilot for Test Generation in Python: An Empirical Study where we studied the usablity of unmodified GitHub Copilot to generate tests for Python projects. Software Quality Assurance Analytics: Enabling Software Engineers to Reflect on QA Practices SCAM'24 Software Quality Assurance Analytics: Enabling Software Engineers to Reflect on QA Practices SCAM'24 Full Publication Presentation Recording Permalink to here Ali Khatami ¬∑ Carolin Brandt ¬∑ Andy Zaidman Software quality is crucial, but previous research showed that developers often lack awareness about QA practices in their projects. We aimed to address this gap by creating a dashboard that provides insights into various QA activities. In this work: We designed RepoInsights, a dashboard that gives an overview of QA practices like testing, code reviews, automated workflows, and project guidelines. We conducted interviews with 14 software engineers from open-source, industry, and academia to evaluate the tool. We asked participants to assess real projects using RepoInsights and provide feedback on its usefulness. Key Findings Most participants found value in having a centralized overview of QA metrics across their projects. The dashboard helped engineers reflect on QA practices by providing meaningful metrics and details about project activities. Participants expressed interest in using such a tool in the future, especially for periodic project health checks. Engineers wanted more granular, contextualized information tailored to each project‚Äôs specific needs and practices. There‚Äôs a need for actionable insights derived from the data, not just raw metrics. Recommendations for Improvement Based on participant feedback, future QA analytics tools should: Provide highly detailed, project-specific information Offer meaningful comparisons between similar projects Include more context about each project‚Äôs unique situation Generate actionable insights and improvement suggestions Allow customization of displayed information Integrate seamlessly into existing developer workflows Why It Matters This research lays the groundwork for better QA analytics tools. By improving developers‚Äô awareness and understanding of QA practices, we can potentially enhance software quality across the industry. Shaken, Not Stirred. How Developers Like Their Amplified Tests IEEE Transactions on Software Engineering Shaken, Not Stirred. How Developers Like Their Amplified Tests IEEE Transactions on Software Engineering Full Publication DOI / Publication in IEEE Xplore Replication Package Poster Permalink to here Carolin Brandt ¬∑ Ali Khatami ¬∑ Mairieli Wessel ¬∑ Andy Zaidman In this paper we conduct a large-scale open source contribution study, submitting pull requests with amplified tests to Java projects on GitHub. We analyze the feedback from the maintainers, as well as the preparations we needed to do to prepare the amplified tests for the pull requests. Based on this we collect guidelines on what developers can expect to change in amplified tests before including them in their test suite. Part of this are very helpful changes that are based on the developer‚Äôs understanding of the code base and their software project, leading us to call for more support for developers to understanding amplified tests instead of focusing on further automation. Abstract Test amplification makes systematic changes to existing, manually written tests to provide tests complementary to an automated test suite. We consider developer-centric test amplification, where the developer explores, judges and edits the amplified tests before adding them to their maintained test suite. However, it is as yet unclear which kind of selection and editing steps developers take before including an amplified test into the test suite. In this paper we conduct an open source contribution study, amplifying tests of open source Java projects from GitHub. We report which deficiencies we observe in the amplified tests while manually filtering and editing them to open 39 pull requests with amplified tests. We present a detailed analysis of the maintainer‚Äôs feedback regarding proposed changes, requested information, and expressed judgment. Our observations provide a basis for practitioners to take an informed decision on whether to adopt developer-centric test amplification. As several of the edits we observe are based on the developer‚Äôs understanding of the amplified test, we conjecture that developer-centric test amplification should invest in supporting the developer to understand the amplified tests. Keywords Software Testing ¬∑ Automatic Test Generation ¬∑ Developer-Centric Test Amplification Test Amplification For and With Developers PhD Thesis Test Amplification For and With Developers PhD Thesis Thesis Propositions Thesis without Cover (smaller file size) DOI Permalink to here This thesis summarizes four years of my work on test amplification, focussing on how to make it better for developers and collaborating with developers üòä In the Netherlands, a thesis come with a set of propositions about the broader research field and community. The proposition are also discussed and defended during the public defense. The cover was designed by me and is inspired by the work in the thesis, the environment I worked in during the PhD, and the interests I developed in this time. Summary Developer testing has become an established practice in large software projects. The developers working on the functionality of a project also write short, automated scripts that check the behavior of their code. While the benefits of developer testing are widely accepted, writing tests is still seen as tedious and time-consuming. Researchers are working towards alleviating developer effort by automatically generating tests. One approach to do this is test amplification, which modifies existing, manually written tests to create new tests that improve the strength of the existing test suite. When trying to fully automatically generate tests, test generation tools face the relevance problem and the oracle problem: Which behavior of the system is worth testing and what is the expected output to check for? The developer already needs to have an understanding of these two aspects to write the code under test. We propos",
  "content_length": 44281,
  "method": "requests",
  "crawl_time": "2025-12-01 12:49:22"
}