{
  "name": "Carl Vondrick",
  "homepage": "http://www.cs.columbia.edu/~vondrick",
  "status": "success",
  "content": "Carl Vondrick - Columbia UniversityBrief Bio I am a professor in the computer science department at Columbia, and I am also a researcher at Apple. My research studies computer vision, machine learning, and their applications. Previously, I have been a research scientist at Google and a visiting researcher at Cruise. I completed my PhD at MIT in 2017 advised by Antonio Torralba and my BS at UC Irvine in 2011, where I got my start working with Deva Ramanan. I received the 2024 PAMI Young Researcher Award and the 2021 NSF CAREER Award. I am also the Senior Program Chair for ICLR 2025, and General Chair for ICLR 2026. Research By training machines to observe and interact with their surroundings, our research aims to create robust and versatile models for perception. Our lab often investigates visual models that capitalize on large amounts of unlabeled data and transfer across tasks and modalities. Other interests include scene dynamics, sound and language and beyond, interpretable models, and perception for robotics. The lab recruits one or two PhD students each year. Prospective PhD students should apply to the PhD program. Due to the volume of email we receive, we unfortunately cannot respond to emails about applications. PhD Students and PostdocsArjun ManiEge OzgurogluJunbang LiangLennart SchulzeSachit MenonSreehari RammohanSruthi SudhakarGraduated PhD Students and Former PostdocsMia Chiquier (2025), Research Scientist at Mistral Utkarsh Mall (2025), Assistant Professor at MBZUAI Ruoshi Liu (2025), Assistant Professor at UMD Basile Van Hoorick (2024), Research Scientist at TRI Didac Suris (2024), Research Scientist at Meta Chengzhi Mao (2023), Assistant Professor at Rutgers PortfolioRepresentative Papers More Our research creates perception systems with diverse skills, including spatial, physical, logical, and reasoning abilities, for flexibly analyzing visual data. Our multimodal approach provides versatile representations for tasks like 3D reconstruction, visual question answering, and robot manipulation, while offering inherent explainability and excellent zero-shot generalization. The below papers highlight key examples of these capabilities. Machine perception is challenging because most knowledge about our world, such as physical commonsense, is not written down. Through large amounts of unlabeled video and interaction with the natural world, we create algorithms that learn perceptual skills without manual supervision.We create interpretable machine learning methods for perception that allow people to audit decisions and reprogram representations. Unlike black-box neural networks, we develop methods that are explainable by construction while still offering excellent performance.Central to our research is forming an integrative perspective on perception to build accurate and robust models. Our research exploits the natural synchronization between vision, sound, and other modalities to learn cross-modal representations for tasks like recognition, source localization, and artistic correspondence.We create new representations for spatial awareness, allowing vision systems to reconstruct scenes in 3D and anticipate object dynamics in the future. We often tightly integrate geometry, physics, and generative models in order to equip 3D vision systems with intuitive, and sometimes un-intuitive, physical skills.We develop multi-modal learning methods for robotics, integrating vision, sound, interaction, and other modalities together in order to learn representations for perception, design, and action.We leverage visual data to accelerate scientific discovery, developing methods that can identify patterns, make hypotheses, and generate insights across diverse scientific domains.Our research creates visual representations that learn the human behavior continuum, capturing the goals underlying human action. We aim to create computer vision systems that can assist people at their activities, thereby enabling new opportunities for human-computer interaction.We harness language to learn neuro-symbolic methods for computer vision, establishing methods that rapidly generalize to open world tasks while offering inherent explainability too.The ubiquity of machine perception creates exciting possibilities for applications, but simultaneously exposes significant potential risks. Our research is exploring methods that prevent computer vision methods from solving potentially harmful problems.Critical applications require systems that are trustworthy and reliable. Our research demonstrates that predictive models have intrinsic empirical and theoretical advantages for improving robustness and generalization.2025Loading...Video Generators are Robot Policies Junbang Liang, Pavel Tokmakov, Ruoshi Liu, Sruthi Sudhakar, Paarth Shah, Rares Ambrus, Carl Vondrick arXiv 2025 PaperProject PageBibTeXLoading...MINERVA: Evaluating Complex Video Reasoning Arsha Nagrani, Sachit Menon, Ahmet Iscen, Shyamal Buch, Ramin Mehran, Nilpa Jha, Anja Hauth, Yukun Zhu, Carl Vondrick, Mikhail Sirotenko, Cordelia Schmid, Tobias Weyand ICCV 2025 PaperDatasetBibTeXLoading...Towards LLM Agents for Earth Observation Chia Hsiang Kao, Wenting Zhao, Shreelekha Revankar, Samuel Speas, Snehal Bhagat, Rajeev Datta, Cheng Perng Phoo, Utkarsh Mall, Carl Vondrick, Kavita Bala, Bharath Hariharan arXiv 2025 PaperProject PageBibTeXLoading...Teaching Humans Subtle Differences with DIFF-usion Mia Chiquier, Orr Avrech, Yossi Gandelsman, Berthy Feng, Katherine Bouman, Carl Vondrick arXiv 2025 PaperProject PageBibTeXLoading...Generative Data Mining with Longtail-Guided Diffusion David S. Hayden, Mao Ye, Timur Garipov, Gregory P. Meyer, Carl Vondrick, Zhao Chen, Yuning Chai, Eric Wolff, Siddhartha S. Srinivasa ICML 2025 PaperBibTeXLoading...DiSciPLE: Learning Interpretable Programs for Scientific Visual Discovery Utkarsh Mall, Cheng Perng Phoo, Mia Chiquier, Bharath Hariharan, Kavita Bala, Carl Vondrick CVPR 2025 PaperProject PageBibTeXLoading...Self-Improving Autonomous Underwater Manipulation Ruoshi Liu, Huy Ha, Mengxue Hou, Shuran Song, Carl Vondrick ICRA 2025 PaperProject PageBibTeX2024Loading...Differentiable Robot Rendering Ruoshi Liu, Alper Canberk, Shuran Song, Carl Vondrick CoRL 2024 (Oral) PaperProject PageBibTeXLoading...Dreamitate: Real-World Visuomotor Policy Learning via Video Generation Junbang Liang, Ruoshi Liu, Ege Ozguroglu, Sruthi Sudhakar, Achal Dave, Pavel Tokmakov, Shuran Song, Carl Vondrick CoRL 2024 PaperProject PageBibTeXLoading...Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities Sachit Menon, Richard Zemel, Carl Vondrick EMNLP 2024 PaperProject PageBibTeXLoading...See It from My Perspective: Diagnosing the Western Cultural Bias of Large Vision-Language Models in Image Understanding Amith Ananthram, Elias Stengel-Eskin, Carl Vondrick, Mohit Bansal, Kathleen McKeown EMNLP 2024 PaperCodeBibTeXLoading...EraseDraw: Learning to Draw Step-by-Step via Erasing Objects from Images Alper Canberk, Maksym Bondarenko, Ege Ozguroglu, Ruoshi Liu, Carl Vondrick ECCV 2024 PaperProject PageBibTeXLoading...How Video Meetings Change Your Expression Sumit Sarin, Utkarsh Mall, Purva Tendulkar, Carl Vondrick ECCV 2024 PaperProject PageBibTeXLoading...Controlling the World by Sleight of Hand Sruthi Sudhakar, Ruoshi Liu, Basile Van Hoorick, Carl Vondrick, and Richard Zemel ECCV 2024 (Oral) PaperBibTeXLoading...Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, Carl Vondrick ECCV 2024 (Oral) PaperProject PageBibTeXLoading...Evolving Interpretable Visual Classifiers with Large Language Models Mia Chiquier, Utkarsh Mall, Carl Vondrick ECCV 2024 PaperProject PageBibTeXLoading...SelfIE: Self-Interpretation of Large Language Model Embeddings Haozhe Chen, Carl Vondrick, Chengzhi Mao ICML 2024 PaperProject PageBibTeXLoading...PaperBot: Learning to Design Real-World Tools Using Paper Ruoshi Liu, Junbang Liang, Sruthi Sudhakar, Huy Ha, Cheng Chi, Shuran Song, Carl Vondrick arXiv 2024 PaperProject PageBibTeXLoading...pix2gestalt: Amodal Segmentation by Synthesizing Wholes Ege Ozguroglu, Ruoshi Liu, DÃ­dac SurÃ­s, Dian Chen, Achal Dave, Pavel Tokmakov, Carl Vondrick CVPR 2024 PaperProject PageBibTeXLoading...Raidar: geneRative AI Detection viA Rewriting Chengzhi Mao, Carl Vondrick, Hao Wang, Junfeng Yang ICLR 2024 PaperBibTeXLoading...Interpreting and Controlling Vision Foundation Models via Text Explanations Haozhe Chen, Junfeng Yang, Carl Vondrick, Chengzhi Mao ICLR 2024 PaperBibTeXLoading...Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape Rundi Wu, Ruoshi Liu, Carl Vondrick, Changxi Zheng ICLR 2024 PaperProject PageBibTeXLoading...Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment Utkarsh Mall, Cheng Perng Phoo, Meilin Liu, Carl Vondrick, Bharath Hariharan, Kavita Bala ICLR 2024 PaperBibTeX2023Loading...Objaverse-XL: A Universe of 10M+ 3D Objects Matt Deitke, et al. NeurIPS 2023 PaperBibTeXLoading...ViperGPT: Visual Inference via Python Execution for Reasoning DÃ­dac SurÃ­s, Sachit Menon, Carl Vondrick ICCV 2023 (Oral) PaperProject PageCodeBibTeXLoading...Zero-1-to-3: Zero-shot One Image to 3D Object Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, Carl Vondrick ICCV 2023 (Oral) PaperProject PageCodeDemoBibTeXLoading...Muscles in Action Mia Chiquier, Carl Vondrick ICCV 2023 PaperProject PageBibTeXLoading...SurfsUp: Learning Fluid Simulation for Novel Surfaces Arjun Mani, Ishaan Preetam Chandratreya, Elliot Creager, Carl Vondrick, Richard Zemel ICCV 2023 PaperProject PageBibTeXLoading...Landscape Learning for Neural Network Inversion Ruoshi Liu, Chengzhi Mao, Purva Tendulkar, Hao Wang, Carl Vondrick ICCV 2023 PaperBlog PostBibTeXLoading...SHIFT3D: Synthesizing Hard Inputs For Tricking 3D Detectors Hongge Chen, Zhao Chen, Greg Meyer, Dennis Park, Carl Vond",
  "content_length": 20544,
  "method": "requests",
  "crawl_time": "2025-12-01 12:48:41"
}