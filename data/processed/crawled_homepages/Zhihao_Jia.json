{
  "name": "Zhihao Jia",
  "homepage": "https://www.cs.cmu.edu/~zhihaoj2",
  "status": "success",
  "content": "Zhihao Jia Projects Teaching Awards Publications Experience Zhihao Jia Assistant Professor Computer Science Department Carnegie Mellon University zhihao@cmu.edu I am an assistant professor in the Computer Science Department at Carnegie Mellon University, where I work on computer systems and machine learning as part of CMU Catalyst Group and Parallel Data Lab. I received my PhD from the Computer Science Department at Stanford University in 2020, where I was co-advised by Alex Aiken and Matei Zaharia. Before coming to Stanford, I received my bachelor degree in Computer Science from the Special Pilot CS Class supervised by Andrew Yao at Tsinghua University. Research interests: My research interests lie in the intersection of computer systems and machine learning (ML). In particular, my current research focuses on building efficient, scalable, and high-performance software systems for emerging ML applications, such as large language models and generative AI tasks. I am actively looking for strong and self-motivated students interested in building systems for machine learning and quantum computing to join my group. (1) Prospective students: if you are interested in working with me as a Ph.D. student, please apply through the CMU CS PhD program and mention me in your application. (2) Current CMU students: if you are already a graduate, masters, or undergraduate student at CMU, please send me an email and we can find a time to chat. (3) Prospective students not at CMU: if you are interested in joining my group for remote collaborations, please send me an email with your CV. News [2025] Honored to be named a Sloan Research Fellow. [2025] SpotServe received an IEEE Micro Top Picks Honorable Mention as one of the \"most significant research papers in computer architecture based on novelty and potential for long-term impact\". [2024] Received an NVIDIA Academic Award. [2024] Helix and GraphPipe were accepted at ASPLOS 2025. [2024] Received a Google Academic Research Award. [2024] Sequoia (spotlight), SpecExec and Communication Bounds were accepted at NeurIPS 2024. [2024] Quantized Side Tuning received an Outstanding Paper Award at ACL 2024! [2024] Two papers on quantum systems Atlas and Quarl were accepted at SC 2024 and OOPSLA 2024. [2024] We lanuched two tutorials on LLM serving systems at ICML 2024 and SIGMOD 2024. [2024] RalmSpec was accepted at ICML 2024. [2024] Received a Samsung GRO Research Award. [2024] SpotServe received a Distinguished Artifact Award at ASPLOS 2024! [2023] Parcae was accepted at NSDI 2024. [2023] Sia was accepted at SOSP 2023. [2023] SpecInfer, SpotServe, and Korch were accepted at ASPLOS 2024. [2023] TOD and SDPipe were accepted at VLDB 2023. [2023] Received a Cisco Research Award. [2023] Received an Amazon Research Award. [2023] Received an NSF CAREER Award. [2023] EinNet was accepted at OSDI 2023. [2023] Bamboo and TopoOpt were accepted at NSDI 2023. Teaching 15-779 Advanced Topics in Machine Learning Systems (LLM Edition): fall 2025 15-418/618 Parallel Computer Architecture: fall 2024 15-442/642 Machine Learning Systems: spring 2024 15-418/618 Parallel Computer Architecture: fall 2023 15-418/618 Parallel Computer Architecture: spring 2023 15-418/618 Parallel Computer Architecture: fall 2022 15-849 Machine Learning Systems: spring 2022 Projects Mirage is a superoptimizer that automatically generates highly-optimized GPU kernels for ML applications. Mirage can discover kernels up to 3.5x faster than the ones manually implemented by GPU experts. FlexFlow Serve is a compiler and distributed runtime for low-latency, high-performance LLM serving by leveraging tree-based speculative inference and verification. FlexFlow is a deep learning engine that accelerates distributed DNN training by automatically discovering fast parallelization strategies for a specific parallel machine. TASO is a Tensor Algebra SuperOptimizer for deep learning. It optimizes DNN computation graphs using automatically generated graph transformations, achieving up to 3x speedup over existing DNN frameworks. PET further extends TASO by leveraging partially equivalent transformations and automated corrections. Lux is a distributed multi-GPU system for high performance graph processing. Lux achieves fast graph processing by exploiting the aggregate memory bandwidth of multiple GPUs. Lux achieves up to 20x speedup over state-of-the-art graph processing systems. Quartz is a quantum circuit superoptimizer that automatically generates and verifies circuit transformations for arbitrary quantum gate sets. By using these auto-generated transformations, Quartz can outperform existing quantum circuit optimizers on a diversity of gate sets. Legion is a high performance programming system for heterogeneous, parallel machines with complex memory hierarchies. Awards Sloan Research Fellowship, 2025 IEEE Micro Top Picks Honorable Mention, 2025 ACL Outstanding Paper Award, 2024 ASPLOS Distinguished Artifact Award, 2024 NVIDIA Academic Award, 2024 Google Academic Research Award, 2024 Samsung GRO Research Award, 2023 Cisco Research Award, 2023 Amazon Research Award, 2023 NSF CAREER Award, 2023 Meta Research Award, 2022 Qualcomm Innovation Fellowship, 2022 Amazon Research Award, 2022 Google Faculty Research Award, 2022 Stanford Arthur Samuel Best Doctoral Thesis Award, 2020 Current Ph.D. and Postdoctoral Students Zhuoming Chen (with Beidi Chen) Xinhao Cheng Zikun Li Gabriele Oliaro Mengdi Wu Mingkuan Xu (with Umut Acar) Zhihao Zhang Alumni Xupeng Miao (Postdoc, 2024, now Assistant Professor at Purdue) Byungsoo Jeon (PhD, 2024, now Software Engineer at Octo AI) Alan Zhu (BS, 2024, now PhD at Berkeley) Zhengxin Zhang (MS, 2024, now PhD at Cornell) Yue Zhao (PhD, 2023, now Assistant Professor at USC) Shiyi Cao (MS, 2023, now PhD at Berkeley) Yixuan Mei (BS, 2023, now PhD at CMU) Muyan Hu (BS, 2023, now PhD at UIUC) Jinjun Peng (BS, 2023, now PhD at Columbia) Ying Yee (Rae) Wong (BS, 2023, now Masters at Stanford) Andrew Gu (MS, 2022, now Software Engineer at Meta) Yuxuan (Eric) Zheng (MS, 2022, now Software Engineer at Citadel) Publications 2026 AdaServe: Accelerating Multi-SLO LLM Serving with SLO-Customized Speculative Decoding Zikun Li*, Zhuofu Chen*, Remi Delacourt, Gabriele Oliaro, Zeyu Wang, Qinghan Chen, Shuhuai Lin, April Yang, Zhihao Zhang, Zhuoming Chen, Sean Lai, Xinhao Cheng, Xupeng Miao, Zhihao Jia In Proceedings of the European Conference on Computer Systems (EuroSys), April 2026. FlexLLM: Token-Level Co-Serving of LLM Inference and Fine-Tuning with SLO Guarantees Gabriele Oliaro*, Xupeng Miao*, Xinhao Cheng, Vineeth Kada, Ruohan Gao, Yingyi Huang, Remi Delacourt, April Yang, Yingcheng Wang, Mengdi Wu, Colin Unger, and Zhihao Jia In Proceedings of the Symposium on Networked Systems Design and Implementation (NSDI), April 2026. 2025 SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications Gabriele Oliaro, Zhihao Jia, Daniel F Campos, and Aurick Qiao In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), December 2025. Spotlight SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning Rui Pan, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, and Ravi Netravali In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), December 2025. Mirage: A Multi-Level Superoptimizer for Tensor Programs Mengdi Wu, Xinhao Cheng, Shengyu Liu, Chunan Shi, Jianan Ji, Kit Ao, Praveen Velliengiri, Xupeng Miao, Oded Padan, and Zhihao Jia In Proceedings of the Symposium on Operating Systems Design and Implementation (OSDI), July 2025. TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention Lijie Yang*, Zhihao Zhang*, Zhuofu Chen, Zikun Li, and Zhihao Jia In Proceedings of the International Conference on Learning Representations (ICLR), April 2025. MagicPIG: LSH Sampling for Efficient LLM Generation Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, and Beidi Chen In Proceedings of the International Conference on Learning Representations (ICLR), April 2025. Spotlight GraphPipe: Improving Performance and Scalability of DNN Training with Graph Pipeline Parallelism Byungsoo Jeon, Mengdi Wu, Shiyi Cao, Sunghyun Kim, Sunghyun Park, Neeraj Aggarwal, Colin Unger, Daiyaan Arfeen, Peiyuan Liao, Xupeng Miao, Mohammad Alizadeh, Gregory R. Ganger, Tianqi Chen, and Zhihao Jia In Proceedings of the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), April 2025. Helix: Serving Large Language Models over Heterogeneous GPUs and Network via Max-Flow Yixuan Mei, Yonghao Zhuang, Xupeng Miao, Juncheng Yang, Zhihao Jia, and Rashmi Vinayak In Proceedings of the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), April 2025. 2024 Sequoia: Scalable and Robust Speculative Decoding Zhuoming Chen, Avner May, Ruslan Svirschevski, Yu-Hsun Huang, Max Ryabinin, Zhihao Jia, and Beidi Chen In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), December 2024. Spotlight SpecExec: Massively Parallel Speculative Decoding For Interactive LLM Inference on Consumer Devices Ruslan Svirschevski, Avner May, Zhuoming Chen, Beidi Chen, Zhihao Jia, and Max Ryabinin In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), December 2024. Communication Bounds for the Distributed Experts Problem Zhihao Jia, Qi Pang, Trung Tran, David Woodruff, Zhihao Zhang, Wenting Zheng (alphabetically) In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), December 2024. Atlas: Hierarchical Partitioning for Quantum Circuit Simulation on GPUs Mingkuan Xu, Shiyi Cao, Xupeng Miao, Umut Acar, and Zhihao Jia In Proceedings of the International Conference on Supercomputing (SC), November 2024. Quarl: A Learning-Ba",
  "content_length": 21154,
  "method": "requests",
  "crawl_time": "2025-12-01 14:55:30"
}