{
  "name": "Abhijit Mahalanobis",
  "homepage": "https://www.crcv.ucf.edu/person/abhijit-mahalanobis",
  "status": "success",
  "content": "Abhijit Mahalanobis – Center for Research in Computer Vision Skip to main content Associate Professor CRCV | Center for Research in Computer Vision University of Central Florida 4328 Scorpius St. HEC 244 Orlando, FL 32816-2365 Office: HEC 244 E-mail: amahalan@crcv.ucf.edu Biography Dr. Mahalanobis joined UCF from Lockheed Martin, where he was a Senior Fellow of the Lockheed Martin Corporation. His primary research areas are in Systems for Information processing, Computational Sensing and Imaging, and Video/Image processing for information exploitation and ATR. He has over 170 journal and conference publications in this area. He also holds four patents, co-authored a book on pattern recognition, contributed several book chapters, and edited special issues of several journals. Abhijit completed his B.S. degree with Honors at the University of California, Santa Barbara in 1984. He then joined the Carnegie Mellon University and received the MS. and Ph.D. degrees in 1985 and 1987, respectively. Prior to joining Lockheed Martin, Abhijit worked at Raytheon in Tucson, and was a faculty at the University of Arizona and the University of Maryland. Abhijit was elected a Fellow of SPIE in 1997, and a Fellow of OSA 2004 for his work on optical pattern recognition and automatic target recognition. He was elected Fellow of IEEE in 2015 for his work on the theory of correlation filters. He served as an associate editor for Applied Optics from 2004-2009. He was as an associate editor for the journal of the Pattern Recognition Society from 1994-2003. He served on OSA’s Science and Engineering council in the capacity of Pattern Recognition Chair from 2001-2004, and as Technical Group Chair for Information Acquisition, Processing and Display on OSA’s Board of Meetings from 2012-2015. He also serves on the organizing committees for the SPIE conferences, and OSA’s annual and topical meetings. Abhijit received the Hughes Business unit Patent Award in 1998. He was recognized as the Innovator of the Year by the State of Arizona in 1999, and was elected to the Raytheon Honors program for distinguished technical contribution and leadership. At Lockheed Martin, he was elected to the rank of Distinguished Member of Technical Staff in 2000, and twice received the Lockheed Martin Technical Excellence award, the Author of the Year award in 2001, and the Inventor of the Year in 2005 for designing novel target recognition systems. In October 2005, he received the prestigious Lockheed Martin NOVA award, the Corporation’s highest honor, for putting together a National Team and a winning strategy in the FCS competition. Abhijit was also recognized as the 2006 Scientist of the Year by Science Spectrum Magazine, a publication of the Career Communication Group, Inc.. Research Interests Publications You can access the CRCV Publications Page for enhanced search capabilities. 2022 Arif, Maliha; Yong, Calvin; Mahalanobis, Abhijit; Rahnavard, NazaninBackground-Tolerant Object Classification with Embedded Segmentation Mask for Infrared and Color Imagery Conference IEEE International Conference on Image Processing, 2022.Abstract | BibTeX | Links: @conference{Arif2022, title = {Background-Tolerant Object Classification with Embedded Segmentation Mask for Infrared and Color Imagery}, author = {Maliha Arif and Calvin Yong and Abhijit Mahalanobis and Nazanin Rahnavard}, url = {https://www.crcv.ucf.edu/wp-content/uploads/2018/11/Final_ICIP2022_MA_submission.pdf https://www.crcv.ucf.edu/wp-content/uploads/2018/11/bg_poster_ICIP2022_Single.jpg}, year = {2022}, date = {2022-10-16}, urldate = {2022-10-16}, booktitle = {IEEE International Conference on Image Processing}, abstract = {Even though convolutional neural networks (CNNs) can classify objects in images very accurately, it is well known that the attention of the network may not always be on the semantically important regions of the scene. It has been observed that networks often learn background textures, which are not relevant to the object of interest. In turn this makes the networks susceptible to variations and changes in the background which may negatively affect their performance. We propose a new three-step training procedure called split training to reduce this bias in CNNs for object recognition using Infrared imagery and Color (RGB) data. Our split training procedure has three steps. First, a baseline model is trained to recognize objects in images without background, and the activations produced by the higher layers are observed. Next, a second network is trained using Mean Square Error (MSE) loss to produce the same activations, but in response to the objects embedded in background. This forces the second network to ignore the background while focusing on the object of interest. Finally, with layers producing the activations frozen, the rest of the second network is trained using cross-entropy loss to classify the objects in images with background. Our training method outperforms the traditional training procedure in both a simple CNN architecture, as well as for deep CNNs like VGG and DenseNet, and learns to mimic human vision which focuses more on shape and structure than background with higher accuracy. Index Terms— infrared imagery, background invariant learning, grad-CAM, split training, MS-COCO}, keywords = {}, pubstate = {published}, tppubtype = {conference} } CloseEven though convolutional neural networks (CNNs) can classify objects in images very accurately, it is well known that the attention of the network may not always be on the semantically important regions of the scene. It has been observed that networks often learn background textures, which are not relevant to the object of interest. In turn this makes the networks susceptible to variations and changes in the background which may negatively affect their performance. We propose a new three-step training procedure called split training to reduce this bias in CNNs for object recognition using Infrared imagery and Color (RGB) data. Our split training procedure has three steps. First, a baseline model is trained to recognize objects in images without background, and the activations produced by the higher layers are observed. Next, a second network is trained using Mean Square Error (MSE) loss to produce the same activations, but in response to the objects embedded in background. This forces the second network to ignore the background while focusing on the object of interest. Finally, with layers producing the activations frozen, the rest of the second network is trained using cross-entropy loss to classify the objects in images with background. Our training method outperforms the traditional training procedure in both a simple CNN architecture, as well as for deep CNNs like VGG and DenseNet, and learns to mimic human vision which focuses more on shape and structure than background with higher accuracy. Index Terms— infrared imagery, background invariant learning, grad-CAM, split training, MS-COCOClose Hassan, Shah; Jiban, MdJibanul Haque; Mahalanobis, AbhijitPerformance Evaluation of Boosted 2-stream TCRNet Conference International Congress on Information and Communication Technology, 2022.BibTeX | Links: @conference{nokey, title = {Performance Evaluation of Boosted 2-stream TCRNet}, author = {Shah Hassan and MdJibanul Haque Jiban and Abhijit Mahalanobis}, url = {https://www.crcv.ucf.edu/wp-content/uploads/2018/11/paper.pdf}, year = {2022}, date = {2022-03-02}, urldate = {2022-03-02}, publisher = {International Congress on Information and Communication Technology}, keywords = {}, pubstate = {published}, tppubtype = {conference} } Close2021 Jiban, Md Jibanul Haque; Hassan, Shah; Mahalanobis, AbhijitTwo-Stream Boosted TCRNET for Range-Tolerant Infra-Red Target Detection Conference IEEE Conference on Image Processing, 2021.Abstract | BibTeX | Links: @conference{jiban2021icip, title = {Two-Stream Boosted TCRNET for Range-Tolerant Infra-Red Target Detection}, author = {Md Jibanul Haque Jiban and Shah Hassan and Abhijit Mahalanobis}, url = {https://www.crcv.ucf.edu/wp-content/uploads/2018/11/Two-Stream_TCRNet__IEEE_ICIP2021.pdf}, year = {2021}, date = {2021-09-19}, publisher = {IEEE Conference on Image Processing}, abstract = {The detection of vehicular targets in infra-red imagery is a challenging task, both due to the relatively few pixels on target and the false alarms produced by the surrounding terrain clutter. It has been previously shown [1] that a relatively simple network (known as TCRNet) can outperform conventional deep CNNs for such applications by maximizing a target to clutter ratio (TCR) metric. In this paper, we introduce a new form of the network (referred to as TCRNet-2) that further improves the performance by first processing target and clutter information in two parallel channels and then combining them to optimize the TCR metric. We also show that the overall performance can be considerably improved by boosting the performance of a primary TCRNet-2 detector, with a secondary network that enhances discrimination between targets and clutter in the false alarm space of the primary network. We analyze the performance of the proposed networks using a publicly available data set of infra-red images of targets in natural terrain. It is shown that the TCRNet-2 and its boosted version yield considerably better performance than the original TCRNet over a wide range of distances, in both day and night conditions. Index Terms— TCRNet, Infrared, Target Detection, MWIR, Surveillance}, keywords = {}, pubstate = {published}, tppubtype = {conference} } CloseThe detection of vehicular targets in infra-red imagery is a challenging task, both due to the relatively few pixels on target and the false alarms produced by the surrounding terrain clutter. It has been previously shown [1] that a relatively simple network (known as TCRNet) can outperform conventional deep CNNs for such applications by maximizing a target to clutter ratio (TCR) ",
  "content_length": 50281,
  "method": "requests",
  "crawl_time": "2025-12-01 12:28:34"
}