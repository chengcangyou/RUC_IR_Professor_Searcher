{
  "name": "Jason D. Lee",
  "homepage": "https://jasondlee88.github.io",
  "status": "success",
  "content": "Jason D. Lee Jason D. Lee Home CV (PDF) Teaching Research Publications Talks/Slides Group Members Jason D. Lee Jason D. Lee jasonD+mylastname+88@gmail.com (research) or jasond+mylastname@berkeley.edu (teaching or Berkeley-related) Google scholar and Twitter and Talk Bio. About Me I am an associate professor of EECS and Statistics at UC Berkeley. Previously, I was a research scientist at Google Deepmind, member of the IAS, and an associate professor at Princeton. Before that, I was a postdoc at UC Berkeley working with Michael I. Jordan. I received my PhD at Stanford advised by Trevor Hastie and Jonathan Taylor. I received a BS in Mathematics from Duke University advised by Mauro Maggioni. I am a native of Cupertino, CA. My research interests are broadly in Foundations of AI and Deep Learning (slides) (video) Representation Learning Slides and Video. Foundations of Deep Reinforcement Learning (slides) (video 1) (video 2) Students, Visitors, and Postdocs Any email from prospective students, postdoc, or visitors, please include the string “filter_student” in the subject, else I will not receive your email; please use the gmail given above, not the Berkeley mail which I do not filter. I am recruiting PhD students and postdoctoral scholars starting in 2025 at Berkeley, please email me a CV apply. Berkeley PhD students interested in machine learning, statistics, or optimization research, please contact me. My current focus is on machine learning with a focus on foundations of AI, deep learning, representation learning, and deep reinforcement learning. I have lectured on the Foudations of Deep Learning at MIT Video and Slides; my tutorial at the Simons Institute: Slides and Video; and my tutorial at Machine Learning Summer School (MLSS 2021): Video and Slides. I have also given tutorials on Representation Learning at the Johns Hopkins Winter School and Beijing AI Institute; Slides and Video. I am also happy to host visitors. Summer visitors please contact me around February to schedule your visit. See a list of past visitors at here. Awards Samsung AI Researcher of the Year Award 2023 NSF Career Award 2022 ONR Young Investigator Award 2021 Sloan Research Fellow in Computer Science 2019 NIPS 2016 Best Student Paper Award for ‘‘Matrix Completion has no Spurious Local Minima\" Finalist for Best Paper Prize for Young Researchers in Continuous Optimization Princeton Commendation for Outstanding Teaching for ECE538B Selected The Generative Leap: Sharp Sample Complexity for Efficiently Learning Gaussian Multi-Index Models. Alex Damian, Jason D. Lee, Joan Bruna. NeurIPS 2025. Learning Compositional Functions with Transformers from Easy-to-Hard Data. Zixuan Wang, Eshaan Nichani, Alberto Bietti, Alex Damian, Daniel Hsu, Jason D. Lee, Denny Wu. COLT 2025. Emergence and scaling laws in SGD learning of shallow neural networks. Yunwei Ren, Eshaan Nichani, Denny Wu, Jason D. Lee. NeurIPS 2025. How Transformers Learn Causal Structure with Gradient Descent. Eshaan Nichani, Alex Damian, and Jason D. Lee. ICML 2024. Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. code and blog. ICML 2024. Fine-Tuning Language Models with Just Forward Passes. Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev Arora. NeurIPS 2023. Looped Transformers as Programmable Computers. Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D. Lee, and Dimitris Papailiopoulos. ICML 2023. Neural Networks can Learn Representations with Gradient Descent. Alex Damian, Jason D. Lee, and Mahdi Soltanolkotabi. COLT 2022. On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. JMLR. Video 1) and Video 2 and Slides by Sham Kakade. Gradient Descent Finds Global Minima of Deep Neural Networks Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. ICML 2019. Gradient Descent Converges to Minimizers. Jason D. Lee, Max Simchowitz, Michael I. Jordan, and Benjamin Recht. COLT 2016 Matrix Completion has No Spurious Local Minimum. Rong Ge, Jason D. Lee, and Tengyu Ma. Best Student Paper Award at NeurIPS 2016. Exact Post-Selection Inference with the Lasso. Jason D. Lee, Dennis L Sun, Yuekai Sun, and Jonathan Taylor. Annals of Statistics 2016. Page generated 2025-11-07 14:17:21 Pacific Standard Time, by jemdoc+MathJax.",
  "content_length": 4520,
  "method": "requests",
  "crawl_time": "2025-12-01 13:27:45"
}