{
  "name": "William J. Bowman",
  "homepage": "https://www.williamjbowman.com",
  "status": "success",
  "content": "William J. Bowman | Homeð William J. Bowman | Homeð William J. Bowman is an Assistant Professor of computer science in the Software Practices Lab at University of British Columbia. Broadly speaking, he is interested in making it easier for programmers to communicate their intent to machines, and preserving that intent through compilation. More specifically, their research interests include secure and verified compilation, dependently typed programming, verification, meta-programming, and interoperability. His recent work examines type-preserving compilation of dependently typed programming languages like Coq, a technique that can enable preserving security and correctness invariants of verified software through compilation and statically enforcing those invariants in the low-level (assembly-like) code generated by compilers. If you’re interesting in working with me, check my lab website to see open positions and for advice on applying: williamjbowman.com/arli/. Coordinatesð In Cyberspace: wjb@williamjbowman.com (pref), or wilbowma@cs.ubc.ca (unreliable). In Space: ICICS/CS Building Room 389 2366 Main Mall Vancouver, BC V6T 1Z4 Canada In Time: My Free/Busy Calendar (iCal) My Free/Busy Calendar (HTML) Papers (Chronological)ð Fast and Extensible Hybrid Embeddings with Micros Sean Bocirnea and William J. Bowman Proceedings of the Scheme and Functional Programming Workshop (SCHEME). 2025. Macro embedding is a popular approach to defining extensible shallow embeddings of object languages in Scheme-like host languages. While macro embedding has even been shown to enable implementing extensible typed languages in systems like Racket, it comes at a cost: compile-time performance. In this paper, we revisit micros-syntax to intermediate representation (IR) transformers, rather than source syntax to source syntax transformers (macros). Micro embedding enables stopping at an IR, producing a deep embedding and enabling high performance compile-time functions over an efficient IR, before shallowly embedding the IR back into source syntax. Combining micros with several design patterns to enable the IR and functions over it to be extensible, we achieve extensible hybrid embedding of statically typed languages with significantly improved compile-time compared to macro-embedding approaches. We describe our design patterns and propose new abstractions packaging these patterns.AbstractAbstract (Hide) | Preprint Type Universes as Kripke Worlds Paulette Koronkevich and William J. Bowman Proc. of the ACM on Programming Languages (PACMPL). ICFP. 2025. What are mutable references; what do they mean? The answers to these questions have spawned lots of important theoretical work and form the foundation of many impactful tools. However, existing semantics collapse a key distinction: which allocations does a reference depend on? In this paper, we deconstruct the space of mutable higher-order references. We formalize a novel distinction—splitting the design space of references not only into higher-order vs (full-)ground references, but also dependency of an allocation on past vs future allocations. This distinction is fundamental to a thorny issue that arises in constructing semantic models of mutable references—the type-world circularity. The issue disappears for what we call predicative references, those that only quantify over past, not future, allocations, and for non-higher-order impredicative references. We design a syntax and semantics for each point in our newly described space. The syntax relies on a type universe hierarchy, à la dependent type theory, to kind the types of allocated terms, and stratify allocations. Each type universe corresponds to a semantic Kripke world, giving a lightweight syntactic mechanism to design and restrict heap shapes. The semantics bear a resemblance to work on regions, and suggest some connection between universe systems and regions, which we describe in some detail.AbstractAbstract (Hide) | Open Access DOI | Preprint Type-Preserving Flat Closure Optimization Adam T. Geller, Sean Bocirnea, Chester J. F. Gould, and Paulette Koronkevich Proc. of the ACM on Programming Languages (PACMPL). OOPSLA1. 2025. Type-preserving compilation seeks to make intent as much as a part of compilation as computation. Specifications of intent in the form of types are preserved and exploited during compilation and linking, alongside the mere computation of a program. This provides lightweight guarantees for compilation, optimization, and linking. Unfortunately, type-preserving compilation typically interferes with important optimizations. In this paper, we study typed closure representation and optimization. We analyze limitations in prior typed closure conversion representations, and the requirements of many important closure optimizations. We design a new typed closure representation in our Flat-Closure Calculus (FCC) that admits all these optimizations, prove type safety and subject reduction of FCC, prove type preservation from an existing closure converted IR to FCC, and implement common closure optimizations for FCC.AbstractAbstract (Hide) | Open Access DOI | Artifact The Ethical Compiler: Addressing the Is-Ought Gap in Compilation (Invited Talk) William J. Bowman Proceedings of the Workshop on Partial Evaluation and Semantics-based Program Manipulation (PEPM). 2025. The is-ought gap is a problem in moral philosophy observing that ethical judgments (\"ought\") cannot be grounded purely in truth judgments (\"is\"): that an ought cannot be derived from an is. This gap renders the following argument invalid: \"It is true that type safe languages prevent bugs and that bugs cause harm, therefore you ought to write in type safe languages\". To validate ethical claims, we must bridge the gap between is and ought with some ethical axiom, such as \"I believe one ought not cause harm\". But what do ethics have to do with manipulating programs? A lot! Ethics are central to correctness! For example, suppose an algorithm infers the type of is Bool, and is in fact a Bool; the program type checks. Is the program correct-does it behave as it ought? We cannot answer this without some ethical axioms: what does the programmer believe ought to be? I believe one ought to design and implement languages ethically. We must give the programmer the ability to express their ethics-their values and beliefs about a program-in addition to mere computational content, and build tools that respect the distinction between is and ought. This paper is a guide to ethical language design and implementation possibilities.AbstractAbstract (Hide) | Open Access DOI A Low-Level Look at A-normal Form William J. Bowman Proc. of the ACM on Programming Languages (PACMPL). OOPSLA. 2024. A-normal form (ANF) is a widely studied intermediate form in which local control and data flow is made explicit in syntax, and a normal form in which many programs with equivalent control-flow graphs have a single normal syntactic representation. However, ANF is difficult to implement effectively and, as we formalize, difficult to extend with new lexically scoped constructs such as scoped region-based allocation. The problem, as has often been observed, is that normalization of commuting conversions is hard. This traditional view of ANF that normalizing commuting conversions is hard, found in formal models and informed by high-level calculi, is wrong. By studying the low-level intensional aspects of ANF, we can derive a normal form in which normalizing commuting conversion is easy, does not require join points, or code duplication, or renormalization after inlining, and is easily extended with new lexically scoped effects. We formalize the connection between ANF and monadic form and their intensional properties, derive an imperative ANF, and design a compiler pipeline from an untyped lambda-calculus with scoped regions, to monadic form, to a low-level imperative monadic form in which A-normalization is trivial and safe for regions. We prove that any such compiler preserves, or optimizes, stack and memory behaviour compared to ANF. Our formalization reconstructs and systematizes pragmatic choices found in practice, including current production-ready compilers. The main take-away from this work is that, in general, monadic form should be preferred over ANF, and A-normalization should only be done in a low-level imperative intermediate form. This maximizes the advantages of each form, and avoids all the standard problems with ANF.AbstractAbstract (Hide) | Preprint | Artifact Type Universes as Allocation Effects Paulette Koronkevich and William J. Bowman 2024. In this paper, we explore a connection between type universes and memory allocation. Type universe hierarchies are used in dependent type theories to ensure consistency, by forbidding a type from quantifying over all types. Instead, the types of types (universes) form a hierarchy, and a type can only quantify over types in other universes (with some exceptions), restricting cyclic reasoning in proofs. We present a perspective where universes also describe where values are allocated in the heap, and the choice of universe algebra imposes a structure on the heap overall. The resulting type system provides a simple declarative system for reasoning about and restricting memory allocation, without reasoning about reads or writes. We present a theoretical framework for equipping a type system with higher-order references restricted by a universe hierarchy, and conjecture that many existing universe algebras give rise to interesting systems for reasoning about allocation. We present 3 instantiations of this approach to enable reasoning about allocation in the simply typed λ-calculus: (1) the standard ramified universe hierarchy, which we prove guarantees termination of the language extended with higher-order references by restricting cycles in the heap; (2) an extension with an impredicative base universe, which we conjecture enabl",
  "content_length": 36171,
  "method": "requests",
  "crawl_time": "2025-12-01 14:46:53"
}