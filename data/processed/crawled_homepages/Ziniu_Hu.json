{
  "name": "Ziniu Hu",
  "homepage": "https://acbull.github.io",
  "status": "success",
  "content": "Ziniu Hu's Website Ziniu Hu About I was working at xAI, reinforcing Large Language Models (e.g. Grok Code Fast 1, Grok 3-mini Reasoning, Grok 3 and Grok 2) to solve real-world problems. I finished Postdoc at Caltech CMS hosted by Prof. Yisong Yue, during which I was also a visiting researcher at Google DeepMind. I received CS PhD degree at UCLA, where I had the fortune to be advised by Prof. Yizhou Sun and Prof. Kai-Wei Chang. I received my CS bachelor degree at Peking University, advised by Prof. Xuanzhe Liu. My research is generously supported by Amazon PhD Fellowship and Baidu Scholarship. My PhD thesis on Neural-Symbolic AI won the ACM KDD 2024 Dissertation Award - Runner Up . Education Ph.D. of Computer Science Sept. 2018 -- May 2023 University of Calofornia, Los Angeles Thesis: Make Knowledge Computable: Towards Differentiable Neural-Symbolic AI B.Sc. of Computer Science Sept. 2014 -- Jun. 2018 Peking University Academic Awards ACM SIGKDD 2024 Dissertation Award - Runner Up Best Paper Award, NeurIPS 2023 Workshop (DL + Differential Equation) Best Paper Award, SoCal NLP Symposium 2022 Best Student Paper Award, KDD 2020 Workshop (DL on Graphs) Best Full Paper Award, WWW 2019 Services Research Track Workflow Co-Chair: SIGKDD 2023 Area Chair of ICLR, NeurIPS, ACL 2025, ICLR, ICML 2026 NeurIPS 2022 Top Reviewer Award Workshop Co-Organizer of Tool-VLM @ CVPR'24 and SSL @ WWW'21 Fellowships Computing, Data, and Society Fellow at Caltech, 2023 Baidu Scholarship (10 PhD students worldwide), 2021 Amazon PhD Fellowship, 2021-2022 SenseTime Scholarship, 2018 May 4th Scholarship of Peking University, 2016 Selected Publications TreeRL: LLM Reinforcement Learning with On-Policy Tree Search Zhenyu Hou*, Ziniu Hu*, Yujiang Li*, Rui Lu*, Jie Tang, Yuxiao Dong PDF CODE Conference of the Association for Computational Linguistics (ACL 2025) We propose TreeRL, a reinforcement learning framework that directly incorporates on-policy tree search for LLM RL training, as well as a cost-effective tree search approach that strategically branch from high-entropy tokens. Strategist: Learning Strategic Skills by LLMs via Bi-Level Tree Search Jonathan Light, Min Cai, Weiqin Chen, Guanzhi Wang, Xiusi Chen, Wei Cheng, Yisong Yue, Ziniu Hu PDF CODE DEMO International Conference on Learning Representations (ICLR 2025) Covered by State of AI Report 2024 , published by Air Street Capital. We propose Strategist, a method allowing LLMs to learn new skills for multi-agent games. With bi-level tree search approach, combining high-level strategic learning with low-level simulated self-play for feedback. It outperformed RL and other LLM-based approaches on Game of Pure Strategy and The Resistance: Avalon at action planning and dialogue generation. Multi-Token Joint Speculative Decoding for Accelerating Large Language Model Inference Zongyue Qin, Ziniu Hu, Zifan He, Neha Prakriya, Jason Cong, Yizhou Sun PDF CODE International Conference on Learning Representations (ICLR 2025) We propose a novel decoding that improves perplexity and downstream performance with 1.4 times faster and 1.5 times less energy cost compared to speculative decoding by considering joint probability of multiple tokens. QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search Zongyu Lin, Yao Tang, Xingcheng Yao, Da Yin, Ziniu Hu, Yizhou Sun, Kai-Wei Chang PDF International Conference on Machine Learning (ICML 2025) QLASS (Q-guided Language Agent Stepwise Search), is a framework that supercharges language agents at inference time. We build a process reward model to guide open language agents on complex interactive tasks by estimating the Q-value of each step without any human annotation. ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang PDF CODE Conference on Neural Information Processing Systems (NeurIPS 2024) In this paper, we develop a reinforced self-training approach, called ReST-MCTS*, based on integrating process reward guidance with tree search MCTS* for collecting higher-quality reasoning traces as well as per-step value to train policy and reward models. ReST-MCTS* circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning: Given oracle final correct answers, ReST-MCTS* is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer. These inferred rewards serve dual purposes: they act as value targets for further refining the process reward model and also facilitate the selection of high-quality traces for policy model self-training. SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, Jie Tang PDF CODE Conference on Neural Information Processing Systems (NeurIPS 2024, Dataset Track) We use LLM to self-curated SciInstruct, a diverse and high-quality dataset of college-level mathematics, physics, chemistry, and formal proofs. Using SciInstruct to finetune the ChatGLM family of LLMs, we introduce SciGLM, a suite of scientific language models for college-level mathematical/scientific reasoning. Physics-Informed Regularization for Domain-Agnostic Dynamical System Modeling Zijie Huang*, Wanjia Zhao*, Jingdong Gao, Ziniu Hu, Xiao Luo, Yadi Cao, Yuanzhou Chen, Yizhou Sun, Wei Wang PDF CODE Conference on Neural Information Processing Systems (NeurIPS 2024), Best Paper Award at NeurIPS 2023, Deep Learning and Differential Equations (DLDE) workshop We propose a physical-law-guided regularization term corresponding to a soft constraint of time-reversal symmetry. The term is applied to GraphODE models for multi-agent dynamical systems and demonstrated as superior to several baselines on a variety of benchmarks, including the challenging pendulum problem. Enhancing Large Vision Language Models with Self-Training on Image Comprehension Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen, James Zou, Kai-Wei Chang, Wei Wang PDF CODE WEBSITE Conference on Neural Information Processing Systems (NeurIPS 2024) We introduce Self-Training on Image Comprehension (STIC), which self-constructs a preference dataset for image descriptions using unlabeled images. Preferred responses are generated through a step-by-step prompt, while dis-preferred responses are generated from either corrupted images or misleading prompts. Can Large Language Model Agents Simulate Human Trust Behavior? Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Shiyang Lai, Kai Shu, Jindong Gu, Adel Bibi, Ziniu Hu, David Jurgens, James Evans, Philip Torr, Bernard Ghanem, Guohao Li PDF CODE Conference on Neural Information Processing Systems (NeurIPS 2024) Under the framework of Trust Games, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, indicating the feasibility to simulate human trust behaviors with LLM agents SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A. Ross, Cordelia Schmid, Alireza Fathi PDF International Conference on Machine Learning (ICML 2024, Oral Presentation) We introduces SceneCraft, an LLM Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. SceneCraft can keep self-improving via Library Learning. Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng Zhang, Chandramouli S Sastry, Siddharth Gururani, Sageev Oore, Yisong Yue PDF CODE DEMO International Conference on Machine Learning (ICML 2024, Oral Presentation) We study the problem of symbolic music generation, with a technical focus on non-differentiable rule guidance by Musical Rules (e.g., note density or chord progression). We propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained diffusion models in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time. SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models Xiaoxuan Wang*, Ziniu Hu*, Pan Lu*, Yanqiao Zhu*, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, Wei Wang PDF CODE & Dataset International Conference on Machine Learning (ICML 2024) Covered by Nature News Feature (15 November 2023) We propose SciBench to systematically examine LLM's reasoning for complex scientific problem solving. SCIBENCH contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. AVIS: Autonomous Visual Information Seeking with Large Language Model Agent Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou Sun, David A. Ross, Cordelia Schmid, Alireza Fathi PDF Google AI Blog-Post Conference on Neural Information Processing Systems (NeurIPS 2023) we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Learning to Group Auxiliary Datasets for Molecule Tinglin Huang, Ziniu Hu, Rex Ying PDF CODE Conference on Neural Information Processing Systems (NeurIPS 2023) We propose MolGroup to address the limited data problem in molecule property prediction by leveraging auxiliary datasets to improve performance on target datasets, via a routing mechanism w/ bi-level optimization. T",
  "content_length": 20034,
  "method": "requests",
  "crawl_time": "2025-12-01 14:55:55"
}